<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>glai API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>glai</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from .ai import AutoAI, EasyAI
from .back_end import AIMessages, ModelDB, ModelData

__all__ = [&#39;AutoAI&#39;, &#39;EasyAI&#39;, &#39;ModelDB&#39;, &#39;AIMessages&#39;, &#39;ModelData&#39;]
# print(f&#34;&#34;&#34;
# glai
# GGUF LLAMA AI - Package for simplified text generation with Llama models quantized to GGUF format is loaded.
# Provides high level APIs for loading models and generating text completions.
# For more information please check README.md file or visit https://github.com/laelhalawani/glai 
# Detailed API documentation can be found here: https://laelhalawani.github.io/glai/
# &#34;&#34;&#34;)</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="glai.ai" href="ai/index.html">glai.ai</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="glai.back_end" href="back_end/index.html">glai.back_end</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="glai.AIMessages"><code class="flex name class">
<span>class <span class="ident">AIMessages</span></span>
<span>(</span><span>user_tags: Union[tuple[str], list[str], dict] = ('[INST]', '[/INST]'), ai_tags: Union[tuple[str], list[str], dict] = ('', ''))</span>
</code></dt>
<dd>
<div class="desc"><p>Represents a collection of messages in an AI system.</p>
<h2 id="properties">Properties</h2>
<p>user_tag_open (str): The opening tag for user messages.
user_tag_close (str): The closing tag for user messages.
ai_tag_open (str): The opening tag for AI messages.
ai_tag_close (str): The closing tag for AI messages.
messages (dict): The messages in the collection: {id: AIMessage}.
_message_id_generator (int): The id generator for the messages.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>messages</code></strong> :&ensp;<code>Union[<a title="glai.AIMessages" href="#glai.AIMessages">AIMessages</a>, AIMessage, str, list]</code></dt>
<dd>The messages to add to the collection.</dd>
<dt><strong><code>user_tags</code></strong> :&ensp;<code>tuple</code></dt>
<dd>The tags to use for user messages.</dd>
<dt><strong><code>ai_tags</code></strong> :&ensp;<code>tuple</code></dt>
<dd>The tags to use for AI messages.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AIMessages:
    &#34;&#34;&#34;
    Represents a collection of messages in an AI system.
    Properties:
        user_tag_open (str): The opening tag for user messages.
        user_tag_close (str): The closing tag for user messages.
        ai_tag_open (str): The opening tag for AI messages.
        ai_tag_close (str): The closing tag for AI messages.
        messages (dict): The messages in the collection: {id: AIMessage}.
        _message_id_generator (int): The id generator for the messages.

    Args:
        messages (Union[AIMessages, AIMessage, str, list]): The messages to add to the collection.
        user_tags (tuple): The tags to use for user messages.
        ai_tags (tuple): The tags to use for AI messages.
    &#34;&#34;&#34;

    def __init__(self,user_tags:Union[tuple[str], list[str], dict]=(&#34;[INST]&#34;, &#34;[/INST]&#34;), ai_tags:Union[tuple[str], list[str], dict]=(&#34;&#34;, &#34;&#34;)):
        if isinstance(user_tags, dict):
            if &#34;open&#34; in user_tags and &#34;close&#34; in user_tags:
                self.user_tag_open = user_tags[&#34;open&#34;]
                self.user_tag_close = user_tags[&#34;close&#34;]
            else:
                raise ValueError(f&#34;Invalid user tags: {user_tags}, for dict tags both &#39;open&#39; and &#39;close&#39; keys must be present.&#34;)
        elif isinstance(user_tags, set) or isinstance(user_tags, list) or isinstance(user_tags, tuple):
            self.user_tag_open = user_tags[0]
            self.user_tag_close = user_tags[1]
        else:
            raise TypeError(f&#34;Invalid type for user tags: {type(user_tags)}, must be dict, set or list.&#34;)
        
        if isinstance(ai_tags, dict):
            if &#34;open&#34; in ai_tags and &#34;close&#34; in ai_tags:
                self.ai_tag_open = ai_tags[&#34;open&#34;]
                self.ai_tag_close = ai_tags[&#34;close&#34;]
            else:
                raise ValueError(f&#34;Invalid user tags: {ai_tags}, for dict tags both &#39;open&#39; and &#39;close&#39; keys must be present.&#34;)
        elif isinstance(ai_tags, set) or isinstance(ai_tags, list) or isinstance(ai_tags, tuple):
            self.ai_tag_open = ai_tags[0]
            self.ai_tag_close = ai_tags[1]
        else:
            raise TypeError(f&#34;Invalid type for user tags: {type(ai_tags)}, must be dict, set or list.&#34;)
        self.messages = {}
        self._message_id_generator = 0
    
    def user_tags(self) -&gt; tuple[str]:
        &#34;&#34;&#34;
        Returns the user tags.

        Returns:
            tuple[str]: The user tags.
        &#34;&#34;&#34;
        return (self.user_tag_open, self.user_tag_close)
    
    def ai_tags(self) -&gt; tuple[str]:
        &#34;&#34;&#34;
        Returns the AI tags.

        Returns:
            tuple[str]: The AI tags.
        &#34;&#34;&#34;
        return (self.ai_tag_open, self.ai_tag_close)    
    
    def load_messages(self, messages:Union[Any, AIMessage, str, list[Union[dict, AIMessage]]]) -&gt; None:
        if messages is not None:
            if isinstance(messages, AIMessages):
                self.messages = messages.messages
            elif isinstance(messages, AIMessage):
                self.messages = [messages]
            elif isinstance(messages, str):
                self.messages = [AIMessage(messages, self.user_tag_open, self.user_tag_close)]
            elif isinstance(messages, list):
                if all([isinstance(message, AIMessage) for message in messages]):
                    self.messages = messages
                elif all(isinstance(message, str) for message in messages):
                    self.messages = [AIMessage(message, tag_open=self.user_tag_open, tag_close=self.user_tag_close) for message in messages]
                elif all(isinstance(message, dict) for message in messages):
                    self.messages = [AIMessage.from_dict(message_dict) for message_dict in messages]
                else:
                    raise TypeError(&#34;If passing list as messages it must be a list of AIMessage or str&#34;)
            else:
                raise TypeError(&#34;messages must be a list of AIMessage or str&#34;)
    
    def to_dict(self) -&gt; dict:
        &#34;&#34;&#34;
        Returns the messages as a dictionary.

        Returns:
            dict: The messages as a dictionary.
        &#34;&#34;&#34;
        return {
            &#34;user_tag_open&#34;: self.user_tag_open,
            &#34;user_tag_close&#34;: self.user_tag_close,
            &#34;ai_tag_open&#34;: self.ai_tag_open,
            &#34;ai_tag_close&#34;: self.ai_tag_close,
            &#34;messages&#34;: [message.to_dict() for message in self.messages]
        }
    
    @staticmethod
    def from_dict(messages_dict:dict) -&gt; &#34;AIMessages&#34;:
        &#34;&#34;&#34;
        Creates a new AIMessages from a dictionary.

        Args:
            messages_dict (dict): The dictionary to create the AIMessages from.

        Returns:
            AIMessages: The created AIMessages.
        &#34;&#34;&#34;
        ai_msgs = AIMessages()
        ai_msgs.user_tag_open = messages_dict[&#34;user_tag_open&#34;]
        ai_msgs.user_tag_close = messages_dict[&#34;user_tag_close&#34;]
        ai_msgs.ai_tag_open = messages_dict[&#34;ai_tag_open&#34;]
        ai_msgs.ai_tag_close = messages_dict[&#34;ai_tag_close&#34;]
        ai_msgs.load_messages(messages_dict[&#34;messages&#34;])
        return ai_msgs
    
    def _generate_message_id(self) -&gt; int:
        &#34;&#34;&#34;
        Generates a unique message ID.
        Iters the message ID generator.

        Returns:
            int: The generated message ID.
        &#34;&#34;&#34;
        self._message_id_generator += 1
        id = self._message_id_generator
        return id
    
    def add_message(self, message:str, tag_open:str, tag_close:str) -&gt; AIMessage:
        &#34;&#34;&#34;
        Adds a new message to the messages dictionary.
        Iters the message ID generator.

        Parameters:
        - message (str): The content of the message.
        - tag_open (str): The opening tag for the message.
        - tag_close (str): The closing tag for the message.

        Returns:
        AIMessage
        &#34;&#34;&#34;
        self.messages[self._generate_message_id()] = AIMessage(message, tag_open, tag_close)
        return self.messages[self._message_id_generator]

    def add_user_message(self, message: str) -&gt; AIMessage:
        &#34;&#34;&#34;
        Adds a user message to the message list.
        Uses add_message() with the user tags.
        Automatically iters the message ID generator.

        Parameters:
            message (str): The message to be added.

        Returns:
            AIMessage
        &#34;&#34;&#34;
        return self.add_message(message, self.user_tag_open, self.user_tag_close)

    def add_ai_message(self, message:str) -&gt; AIMessage:
        &#34;&#34;&#34;
        Adds a ai message to the message list.
        Uses add_message() with the ai tags.
        Automatically iters the message ID generator.

        Parameters:
            message (str): The message to be added.

        Returns:
            AIMessage
        &#34;&#34;&#34;
        return self.add_message(message, self.ai_tag_open, self.ai_tag_close)

    def reset_messages(self) -&gt; None:
        self.messages = {}
        self._message_id_generator = 0

    def __str__(self) -&gt; str:
        return &#34;&#34;.join([str(message) for message in self.messages.values()])

    def __repr__(self) -&gt; str:
        return self.__str__()
    
    def text(self):
        return self.__str__()
    
    def get_last_message(self) -&gt; AIMessage:
        &#34;&#34;&#34;
        Returns the last message in the collection.

        Returns:
            AIMessage: The last message in the collection.
        &#34;&#34;&#34;
        return self.messages[self._message_id_generator]
    
    def edit_last_message(self, new_content:str, tag_open:str=None, tag_close:str=None) -&gt; None:
        &#34;&#34;&#34;
        Updates the content of the last message in the collection.

        Parameters:
            new_content (str): The new content for the message.
            tag_open (str): Optional. The new opening tag for the message.
            tag_close (str): Optional. The new closing tag for the message.

        Returns:
            None
        &#34;&#34;&#34;
        self.get_last_message().edit(new_content, tag_open, tag_close)
    
    def edit_message(self, message_id:int, new_content:str, tag_open:str=None, tag_close:str=None) -&gt; None:
        &#34;&#34;&#34;
        Updates the content of a message in the collection.

        Parameters:
            message_id (int): The id of the message to edit.
            new_content (str): The new content for the message.
            tag_open (str): The new opening tag for the message.
            tag_close (str): The new closing tag for the message.

        Returns:
            None
        &#34;&#34;&#34;
        self.messages[message_id].edit(new_content, tag_open, tag_close)
    
    def save_json(self, file_path:str) -&gt; None:
        &#34;&#34;&#34;
        Saves the messages as a json file.

        Parameters:
            file_path (str): The path to save the json file to.

        Returns:
            None
        &#34;&#34;&#34;
        save_json_file(self.to_dict(), file_path)
    
    @staticmethod
    def from_json(file_path:str) -&gt; &#34;AIMessages&#34;:
        &#34;&#34;&#34;
        Creates a new AIMessages from a json file.

        Args:
            file_path (str): The path to the json file.

        Returns:
            AIMessages: The created AIMessages.
        &#34;&#34;&#34;
        return AIMessages.from_dict(load_json_file(file_path))</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="glai.AIMessages.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>messages_dict: dict) ‑> <a title="glai.back_end.messages.AIMessages" href="back_end/messages.html#glai.back_end.messages.AIMessages">AIMessages</a></span>
</code></dt>
<dd>
<div class="desc"><p>Creates a new AIMessages from a dictionary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>messages_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>The dictionary to create the AIMessages from.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="glai.AIMessages" href="#glai.AIMessages">AIMessages</a></code></dt>
<dd>The created AIMessages.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_dict(messages_dict:dict) -&gt; &#34;AIMessages&#34;:
    &#34;&#34;&#34;
    Creates a new AIMessages from a dictionary.

    Args:
        messages_dict (dict): The dictionary to create the AIMessages from.

    Returns:
        AIMessages: The created AIMessages.
    &#34;&#34;&#34;
    ai_msgs = AIMessages()
    ai_msgs.user_tag_open = messages_dict[&#34;user_tag_open&#34;]
    ai_msgs.user_tag_close = messages_dict[&#34;user_tag_close&#34;]
    ai_msgs.ai_tag_open = messages_dict[&#34;ai_tag_open&#34;]
    ai_msgs.ai_tag_close = messages_dict[&#34;ai_tag_close&#34;]
    ai_msgs.load_messages(messages_dict[&#34;messages&#34;])
    return ai_msgs</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>file_path: str) ‑> <a title="glai.back_end.messages.AIMessages" href="back_end/messages.html#glai.back_end.messages.AIMessages">AIMessages</a></span>
</code></dt>
<dd>
<div class="desc"><p>Creates a new AIMessages from a json file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the json file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="glai.AIMessages" href="#glai.AIMessages">AIMessages</a></code></dt>
<dd>The created AIMessages.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(file_path:str) -&gt; &#34;AIMessages&#34;:
    &#34;&#34;&#34;
    Creates a new AIMessages from a json file.

    Args:
        file_path (str): The path to the json file.

    Returns:
        AIMessages: The created AIMessages.
    &#34;&#34;&#34;
    return AIMessages.from_dict(load_json_file(file_path))</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="glai.AIMessages.add_ai_message"><code class="name flex">
<span>def <span class="ident">add_ai_message</span></span>(<span>self, message: str) ‑> <a title="glai.back_end.messages.AIMessage" href="back_end/messages.html#glai.back_end.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Adds a ai message to the message list.
Uses add_message() with the ai tags.
Automatically iters the message ID generator.</p>
<h2 id="parameters">Parameters</h2>
<p>message (str): The message to be added.</p>
<h2 id="returns">Returns</h2>
<p>AIMessage</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_ai_message(self, message:str) -&gt; AIMessage:
    &#34;&#34;&#34;
    Adds a ai message to the message list.
    Uses add_message() with the ai tags.
    Automatically iters the message ID generator.

    Parameters:
        message (str): The message to be added.

    Returns:
        AIMessage
    &#34;&#34;&#34;
    return self.add_message(message, self.ai_tag_open, self.ai_tag_close)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.add_message"><code class="name flex">
<span>def <span class="ident">add_message</span></span>(<span>self, message: str, tag_open: str, tag_close: str) ‑> <a title="glai.back_end.messages.AIMessage" href="back_end/messages.html#glai.back_end.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Adds a new message to the messages dictionary.
Iters the message ID generator.</p>
<p>Parameters:
- message (str): The content of the message.
- tag_open (str): The opening tag for the message.
- tag_close (str): The closing tag for the message.</p>
<p>Returns:
AIMessage</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_message(self, message:str, tag_open:str, tag_close:str) -&gt; AIMessage:
    &#34;&#34;&#34;
    Adds a new message to the messages dictionary.
    Iters the message ID generator.

    Parameters:
    - message (str): The content of the message.
    - tag_open (str): The opening tag for the message.
    - tag_close (str): The closing tag for the message.

    Returns:
    AIMessage
    &#34;&#34;&#34;
    self.messages[self._generate_message_id()] = AIMessage(message, tag_open, tag_close)
    return self.messages[self._message_id_generator]</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.add_user_message"><code class="name flex">
<span>def <span class="ident">add_user_message</span></span>(<span>self, message: str) ‑> <a title="glai.back_end.messages.AIMessage" href="back_end/messages.html#glai.back_end.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Adds a user message to the message list.
Uses add_message() with the user tags.
Automatically iters the message ID generator.</p>
<h2 id="parameters">Parameters</h2>
<p>message (str): The message to be added.</p>
<h2 id="returns">Returns</h2>
<p>AIMessage</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_user_message(self, message: str) -&gt; AIMessage:
    &#34;&#34;&#34;
    Adds a user message to the message list.
    Uses add_message() with the user tags.
    Automatically iters the message ID generator.

    Parameters:
        message (str): The message to be added.

    Returns:
        AIMessage
    &#34;&#34;&#34;
    return self.add_message(message, self.user_tag_open, self.user_tag_close)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.ai_tags"><code class="name flex">
<span>def <span class="ident">ai_tags</span></span>(<span>self) ‑> tuple[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the AI tags.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[str]</code></dt>
<dd>The AI tags.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ai_tags(self) -&gt; tuple[str]:
    &#34;&#34;&#34;
    Returns the AI tags.

    Returns:
        tuple[str]: The AI tags.
    &#34;&#34;&#34;
    return (self.ai_tag_open, self.ai_tag_close)    </code></pre>
</details>
</dd>
<dt id="glai.AIMessages.edit_last_message"><code class="name flex">
<span>def <span class="ident">edit_last_message</span></span>(<span>self, new_content: str, tag_open: str = None, tag_close: str = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Updates the content of the last message in the collection.</p>
<h2 id="parameters">Parameters</h2>
<p>new_content (str): The new content for the message.
tag_open (str): Optional. The new opening tag for the message.
tag_close (str): Optional. The new closing tag for the message.</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit_last_message(self, new_content:str, tag_open:str=None, tag_close:str=None) -&gt; None:
    &#34;&#34;&#34;
    Updates the content of the last message in the collection.

    Parameters:
        new_content (str): The new content for the message.
        tag_open (str): Optional. The new opening tag for the message.
        tag_close (str): Optional. The new closing tag for the message.

    Returns:
        None
    &#34;&#34;&#34;
    self.get_last_message().edit(new_content, tag_open, tag_close)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.edit_message"><code class="name flex">
<span>def <span class="ident">edit_message</span></span>(<span>self, message_id: int, new_content: str, tag_open: str = None, tag_close: str = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Updates the content of a message in the collection.</p>
<h2 id="parameters">Parameters</h2>
<p>message_id (int): The id of the message to edit.
new_content (str): The new content for the message.
tag_open (str): The new opening tag for the message.
tag_close (str): The new closing tag for the message.</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit_message(self, message_id:int, new_content:str, tag_open:str=None, tag_close:str=None) -&gt; None:
    &#34;&#34;&#34;
    Updates the content of a message in the collection.

    Parameters:
        message_id (int): The id of the message to edit.
        new_content (str): The new content for the message.
        tag_open (str): The new opening tag for the message.
        tag_close (str): The new closing tag for the message.

    Returns:
        None
    &#34;&#34;&#34;
    self.messages[message_id].edit(new_content, tag_open, tag_close)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.get_last_message"><code class="name flex">
<span>def <span class="ident">get_last_message</span></span>(<span>self) ‑> <a title="glai.back_end.messages.AIMessage" href="back_end/messages.html#glai.back_end.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Returns the last message in the collection.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>AIMessage</code></dt>
<dd>The last message in the collection.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_last_message(self) -&gt; AIMessage:
    &#34;&#34;&#34;
    Returns the last message in the collection.

    Returns:
        AIMessage: The last message in the collection.
    &#34;&#34;&#34;
    return self.messages[self._message_id_generator]</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.load_messages"><code class="name flex">
<span>def <span class="ident">load_messages</span></span>(<span>self, messages: Union[Any, <a title="glai.back_end.messages.AIMessage" href="back_end/messages.html#glai.back_end.messages.AIMessage">AIMessage</a>, str, list[Union[dict, <a title="glai.back_end.messages.AIMessage" href="back_end/messages.html#glai.back_end.messages.AIMessage">AIMessage</a>]]]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_messages(self, messages:Union[Any, AIMessage, str, list[Union[dict, AIMessage]]]) -&gt; None:
    if messages is not None:
        if isinstance(messages, AIMessages):
            self.messages = messages.messages
        elif isinstance(messages, AIMessage):
            self.messages = [messages]
        elif isinstance(messages, str):
            self.messages = [AIMessage(messages, self.user_tag_open, self.user_tag_close)]
        elif isinstance(messages, list):
            if all([isinstance(message, AIMessage) for message in messages]):
                self.messages = messages
            elif all(isinstance(message, str) for message in messages):
                self.messages = [AIMessage(message, tag_open=self.user_tag_open, tag_close=self.user_tag_close) for message in messages]
            elif all(isinstance(message, dict) for message in messages):
                self.messages = [AIMessage.from_dict(message_dict) for message_dict in messages]
            else:
                raise TypeError(&#34;If passing list as messages it must be a list of AIMessage or str&#34;)
        else:
            raise TypeError(&#34;messages must be a list of AIMessage or str&#34;)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.reset_messages"><code class="name flex">
<span>def <span class="ident">reset_messages</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_messages(self) -&gt; None:
    self.messages = {}
    self._message_id_generator = 0</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.save_json"><code class="name flex">
<span>def <span class="ident">save_json</span></span>(<span>self, file_path: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the messages as a json file.</p>
<h2 id="parameters">Parameters</h2>
<p>file_path (str): The path to save the json file to.</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_json(self, file_path:str) -&gt; None:
    &#34;&#34;&#34;
    Saves the messages as a json file.

    Parameters:
        file_path (str): The path to save the json file to.

    Returns:
        None
    &#34;&#34;&#34;
    save_json_file(self.to_dict(), file_path)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.text"><code class="name flex">
<span>def <span class="ident">text</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def text(self):
    return self.__str__()</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the messages as a dictionary.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>The messages as a dictionary.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict:
    &#34;&#34;&#34;
    Returns the messages as a dictionary.

    Returns:
        dict: The messages as a dictionary.
    &#34;&#34;&#34;
    return {
        &#34;user_tag_open&#34;: self.user_tag_open,
        &#34;user_tag_close&#34;: self.user_tag_close,
        &#34;ai_tag_open&#34;: self.ai_tag_open,
        &#34;ai_tag_close&#34;: self.ai_tag_close,
        &#34;messages&#34;: [message.to_dict() for message in self.messages]
    }</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.user_tags"><code class="name flex">
<span>def <span class="ident">user_tags</span></span>(<span>self) ‑> tuple[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the user tags.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[str]</code></dt>
<dd>The user tags.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def user_tags(self) -&gt; tuple[str]:
    &#34;&#34;&#34;
    Returns the user tags.

    Returns:
        tuple[str]: The user tags.
    &#34;&#34;&#34;
    return (self.user_tag_open, self.user_tag_close)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="glai.AutoAI"><code class="flex name class">
<span>class <span class="ident">AutoAI</span></span>
<span>(</span><span>name_search: Optional[str] = None, quantization_search: Optional[str] = None, keyword_search: Optional[str] = None, new_tokens: int = 1500, max_input_tokens: int = 900, model_db_dir: str = './gguf_db')</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the AutoAI class for super easy LLM AI generation.</p>
<p>Searches for a model based on name/quantization/keyword.
Downloads model and sets up LlamaAI.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name_search</code></strong></dt>
<dd>Name of model to search for. Optional. Default None.</dd>
<dt><strong><code>quantization_search</code></strong></dt>
<dd>Quantization of model to search for. Optional. Default None.</dd>
<dt><strong><code>keyword_search</code></strong></dt>
<dd>Keyword of model to search for. Optional. Default None.</dd>
<dt><strong><code>new_tokens</code></strong></dt>
<dd>New token length for LlamaAI model. Default 1500.</dd>
<dt><strong><code>max_input_tokens</code></strong></dt>
<dd>Max input tokens for LlamaAI model. Default 900.</dd>
<dt><strong><code>model_db_dir</code></strong></dt>
<dd>Directory to store model data in. Default DEFAULT_LOCAL_GGUF_DIR.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>ai_db</code></strong></dt>
<dd>ModelDB object. - represents the database of models, has useful functions for searching and importing models.</dd>
<dt><strong><code>model_data</code></strong></dt>
<dd>ModelData object. - represents the data of the model, has useful functions for creating, downloading and loading the model data and gguf.</dd>
<dt><strong><code>ai</code></strong></dt>
<dd>LlamaAI object. - represents the LlamaAI model, a wrapper for llama llm and tokenizer models quantized to gguf format. Has methods for adjusting generation and for generating.</dd>
<dt><strong><code>msgs</code></strong></dt>
<dd>AIMessages object. - represents the AIMessages a collection of AIMessage objects, has useful functions for adding and editing messages and can be printed to string.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoAI:
    &#34;&#34;&#34;
    Initialize the AutoAI class for super easy LLM AI generation.

    Searches for a model based on name/quantization/keyword. 
    Downloads model and sets up LlamaAI.

    Args:
        name_search: Name of model to search for. Optional. Default None.
        quantization_search: Quantization of model to search for. Optional. Default None.
        keyword_search: Keyword of model to search for. Optional. Default None.
        new_tokens: New token length for LlamaAI model. Default 1500.
        max_input_tokens: Max input tokens for LlamaAI model. Default 900.
        model_db_dir: Directory to store model data in. Default DEFAULT_LOCAL_GGUF_DIR.

    Attributes:
        ai_db: ModelDB object. - represents the database of models, has useful functions for searching and importing models.
        model_data: ModelData object. - represents the data of the model, has useful functions for creating, downloading and loading the model data and gguf.
        ai: LlamaAI object. - represents the LlamaAI model, a wrapper for llama llm and tokenizer models quantized to gguf format. Has methods for adjusting generation and for generating.
        msgs: AIMessages object. - represents the AIMessages a collection of AIMessage objects, has useful functions for adding and editing messages and can be printed to string.
        
    &#34;&#34;&#34;
    def __init__(self, 
                 name_search: Optional[str] = None,
                 quantization_search: Optional[str] = None,
                 keyword_search: Optional[str] = None,
                 new_tokens: int = 1500,
                 max_input_tokens: int = 900,
                 model_db_dir:str = DEFAULT_LOCAL_GGUF_DIR) -&gt; None:

        self.ai_db = ModelDB(model_db_dir=model_db_dir, copy_examples=True)
        self.model_data: ModelData = self.ai_db.find_model(
            name_search, quantization_search, keyword_search
        )
        self.model_data.download_gguf()
        self.ai = LlamaAI(
            self.model_data.gguf_file_path, new_tokens, max_input_tokens
        )
        print(f&#34;Using model: {self.model_data}&#34;)
        self.msgs: AIMessages = AIMessages(
            self.model_data.user_tags, self.model_data.ai_tags
        )

    def generate_from_prompt(
        self, 
        prompt: str,
        max_tokens_if_needed: int = 2000,
        stop_at:str = None,
        include_stop_str:bool = True
    ) -&gt; str:
        &#34;&#34;&#34;
        Generate text from a prompt using the LlamaAI model.

        Args:
            prompt: Prompt text to generate from.
            max_tokens_if_needed: Max tokens to allow.

        Returns:
            Generated text string.
        &#34;&#34;&#34;
        return self.ai.infer(
            prompt, only_string=True, max_tokens_if_needed=max_tokens_if_needed, stop_at_str=stop_at, include_stop_str=include_stop_str
        )

    def generate(
        self,
        user_message_text: str,
        ai_message_to_be_continued: Optional[str] = None,
        stop_at:Optional[str] = None,
        include_stop_str:bool = True
    ) -&gt; AIMessage:
        &#34;&#34;&#34;
        Generate an AI response to a user message.

        Args:
            user_message_text: User message text.
            ai_message_to_be_continued: Optional text to prepend.
            stop_at: Optional string to stop generation at.
            include_stop_str: Whether to include the stop string in the generated message.

        Returns:
            Generated AIMessage object.
        &#34;&#34;&#34;
        generation_messages = AIMessages(user_tags=self.model_data.user_tags, ai_tags=self.model_data.ai_tags)
        generation_messages.reset_messages()
        generation_messages.add_user_message(user_message_text)


        if ai_message_to_be_continued is not None:
            generation_messages.add_message(
                ai_message_to_be_continued, 
                self.msgs.ai_tag_open, 
                &#34;&#34;
            )
        print(f&#34;Promt: {generation_messages.text()}&#34;)
        
        generated = self.generate_from_prompt(generation_messages.text(), stop_at=stop_at, include_stop_str=include_stop_str)

        if ai_message_to_be_continued is not None:
            generation_messages.edit_last_message(
                ai_message_to_be_continued + generated,
                self.msgs.ai_tag_open,
                self.msgs.ai_tag_close
            )
        else:
            generation_messages.add_ai_message(generated)

        print(f&#34;Generated: {generation_messages.get_last_message().text()}&#34;)
        output = generation_messages.get_last_message()
        return output

    def count_tokens(
        self,
        user_message_text: str,
        ai_message_to_be_continued: Optional[str] = None
    ) -&gt; int:
        &#34;&#34;&#34;
        Count the number of tokens in a generated message.

        Args:
            user_message_text: User message text.
            ai_message_to_be_continued: Optional text to prepend.

        Returns:
            Number of tokens in generated message.
        &#34;&#34;&#34;
        generation_messages = AIMessages()
        generation_messages.reset_messages()
        generation_messages.add_user_message(user_message_text)

        if ai_message_to_be_continued is not None:
            generation_messages.add_message(
                ai_message_to_be_continued, 
                self.msgs.ai_tag_open, 
                &#34;&#34;
            )
        return self.ai.count_tokens(generation_messages.text())
    
    def is_within_input_limit(
        self,
        user_message_text: str,
        ai_message_to_be_continued: Optional[str] = None
        ) -&gt; bool:
        &#34;&#34;&#34;
        Check if the generated message is within the input limit.

        Args:
            user_message_text: User message text.
            ai_message_to_be_continued: Optional text to prepend.

        Returns:
            True if within input limit, False otherwise.
        &#34;&#34;&#34;
        return self.ai.is_within_input_limit(self.count_tokens(user_message_text, ai_message_to_be_continued))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="glai.AutoAI.count_tokens"><code class="name flex">
<span>def <span class="ident">count_tokens</span></span>(<span>self, user_message_text: str, ai_message_to_be_continued: Optional[str] = None) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Count the number of tokens in a generated message.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user_message_text</code></strong></dt>
<dd>User message text.</dd>
<dt><strong><code>ai_message_to_be_continued</code></strong></dt>
<dd>Optional text to prepend.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Number of tokens in generated message.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def count_tokens(
    self,
    user_message_text: str,
    ai_message_to_be_continued: Optional[str] = None
) -&gt; int:
    &#34;&#34;&#34;
    Count the number of tokens in a generated message.

    Args:
        user_message_text: User message text.
        ai_message_to_be_continued: Optional text to prepend.

    Returns:
        Number of tokens in generated message.
    &#34;&#34;&#34;
    generation_messages = AIMessages()
    generation_messages.reset_messages()
    generation_messages.add_user_message(user_message_text)

    if ai_message_to_be_continued is not None:
        generation_messages.add_message(
            ai_message_to_be_continued, 
            self.msgs.ai_tag_open, 
            &#34;&#34;
        )
    return self.ai.count_tokens(generation_messages.text())</code></pre>
</details>
</dd>
<dt id="glai.AutoAI.generate"><code class="name flex">
<span>def <span class="ident">generate</span></span>(<span>self, user_message_text: str, ai_message_to_be_continued: Optional[str] = None, stop_at: Optional[str] = None, include_stop_str: bool = True) ‑> <a title="glai.back_end.messages.AIMessage" href="back_end/messages.html#glai.back_end.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Generate an AI response to a user message.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user_message_text</code></strong></dt>
<dd>User message text.</dd>
<dt><strong><code>ai_message_to_be_continued</code></strong></dt>
<dd>Optional text to prepend.</dd>
<dt><strong><code>stop_at</code></strong></dt>
<dd>Optional string to stop generation at.</dd>
<dt><strong><code>include_stop_str</code></strong></dt>
<dd>Whether to include the stop string in the generated message.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Generated AIMessage object.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate(
    self,
    user_message_text: str,
    ai_message_to_be_continued: Optional[str] = None,
    stop_at:Optional[str] = None,
    include_stop_str:bool = True
) -&gt; AIMessage:
    &#34;&#34;&#34;
    Generate an AI response to a user message.

    Args:
        user_message_text: User message text.
        ai_message_to_be_continued: Optional text to prepend.
        stop_at: Optional string to stop generation at.
        include_stop_str: Whether to include the stop string in the generated message.

    Returns:
        Generated AIMessage object.
    &#34;&#34;&#34;
    generation_messages = AIMessages(user_tags=self.model_data.user_tags, ai_tags=self.model_data.ai_tags)
    generation_messages.reset_messages()
    generation_messages.add_user_message(user_message_text)


    if ai_message_to_be_continued is not None:
        generation_messages.add_message(
            ai_message_to_be_continued, 
            self.msgs.ai_tag_open, 
            &#34;&#34;
        )
    print(f&#34;Promt: {generation_messages.text()}&#34;)
    
    generated = self.generate_from_prompt(generation_messages.text(), stop_at=stop_at, include_stop_str=include_stop_str)

    if ai_message_to_be_continued is not None:
        generation_messages.edit_last_message(
            ai_message_to_be_continued + generated,
            self.msgs.ai_tag_open,
            self.msgs.ai_tag_close
        )
    else:
        generation_messages.add_ai_message(generated)

    print(f&#34;Generated: {generation_messages.get_last_message().text()}&#34;)
    output = generation_messages.get_last_message()
    return output</code></pre>
</details>
</dd>
<dt id="glai.AutoAI.generate_from_prompt"><code class="name flex">
<span>def <span class="ident">generate_from_prompt</span></span>(<span>self, prompt: str, max_tokens_if_needed: int = 2000, stop_at: str = None, include_stop_str: bool = True) ‑> str</span>
</code></dt>
<dd>
<div class="desc"><p>Generate text from a prompt using the LlamaAI model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>prompt</code></strong></dt>
<dd>Prompt text to generate from.</dd>
<dt><strong><code>max_tokens_if_needed</code></strong></dt>
<dd>Max tokens to allow.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Generated text string.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_from_prompt(
    self, 
    prompt: str,
    max_tokens_if_needed: int = 2000,
    stop_at:str = None,
    include_stop_str:bool = True
) -&gt; str:
    &#34;&#34;&#34;
    Generate text from a prompt using the LlamaAI model.

    Args:
        prompt: Prompt text to generate from.
        max_tokens_if_needed: Max tokens to allow.

    Returns:
        Generated text string.
    &#34;&#34;&#34;
    return self.ai.infer(
        prompt, only_string=True, max_tokens_if_needed=max_tokens_if_needed, stop_at_str=stop_at, include_stop_str=include_stop_str
    )</code></pre>
</details>
</dd>
<dt id="glai.AutoAI.is_within_input_limit"><code class="name flex">
<span>def <span class="ident">is_within_input_limit</span></span>(<span>self, user_message_text: str, ai_message_to_be_continued: Optional[str] = None) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Check if the generated message is within the input limit.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user_message_text</code></strong></dt>
<dd>User message text.</dd>
<dt><strong><code>ai_message_to_be_continued</code></strong></dt>
<dd>Optional text to prepend.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>True if within input limit, False otherwise.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_within_input_limit(
    self,
    user_message_text: str,
    ai_message_to_be_continued: Optional[str] = None
    ) -&gt; bool:
    &#34;&#34;&#34;
    Check if the generated message is within the input limit.

    Args:
        user_message_text: User message text.
        ai_message_to_be_continued: Optional text to prepend.

    Returns:
        True if within input limit, False otherwise.
    &#34;&#34;&#34;
    return self.ai.is_within_input_limit(self.count_tokens(user_message_text, ai_message_to_be_continued))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="glai.EasyAI"><code class="flex name class">
<span>class <span class="ident">EasyAI</span></span>
<span>(</span><span>**kwds)</span>
</code></dt>
<dd>
<div class="desc"><p>EasyAI provides a simple interface for LlamaAI based AI using quantized GGUF models
for inference on CPU.</p>
<p>Initialization:
Can be intialized with no arguments, followed by following configuration methods, step by step, all in one or using a dict:</p>
<p>Step by step call these with appropriate arguments:</p>
<pre><code>1. `self.load_model_db(model_db_dir: str = DEFAULT_LOCAL_GGUF_DIR)`
2. One of the following with necessary args: &lt;code&gt;self.model\_data\_from\_url()&lt;/code&gt; or &lt;code&gt;self.model\_data\_from\_file()&lt;/code&gt; or &lt;code&gt;self.find\_model\_data()&lt;/code&gt;
3. `self.load_ai(max_tokens: int = 200, max_input_tokens: int = 100)`
</code></pre>
<p>All in one:</p>
<pre><code>```python
self.configure(
    model_db_dir:str = DEFAULT_LOCAL_GGUF_DIR, 
    model_url: Optional[str] = None, 
    model_gguf_path: Optional[str] = None, 
    name_search: Optional[str] = None, 
    quantization_search: Optional[str] = None, 
    keyword_search: Optional[str] = None, 
    new_tokens: int = 200, 
    max_input_tokens: int = 100
    )
```

Or with a dictionary with the following keys:
- model_db_dir: Directory to store model data in. Defaults to DEFAULT_LOCAL_GGUF_DIR.
- model_url: URL of model to configure with. Automatically downloads and builds as needed. (Optional)
- name_search: Name of model to search for in the model db dir.(Optional)
- quantization_search: Quantization of model to search for in the model db dir..(Optional)
- keyword_search: Keyword of model to search for in the model db dir..(Optional)
- model_gguf_path: Path to GGUF file of model to configure with.(Optional, not a recommended method, doesn't preserve download url)
- new_tokens: Max tokens to be generated by LlamaAI model. (Defaults to 200, set to around 500-1k for regular use)
- max_input_tokens: Max input tokens to be generated by LlamaAI model. (Defaults to 100, set to around 200-300 for regular use)
</code></pre>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model_db</code></strong></dt>
<dd>ModelDB for searching/loading models</dd>
<dt><strong><code>messages</code></strong></dt>
<dd>AIMessages for tracking conversation </dd>
<dt><strong><code>model_data</code></strong></dt>
<dd>ModelData of selected model</dd>
<dt><strong><code>lai</code></strong></dt>
<dd>LlamaAI instance for generating text</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>DB:
load_model_db: Load ModelDB from directory
ModelData:
find_model_data: Search model DB for ModelData
model_data_from_url: Get ModelData from URL
model_data_from_file: Load ModelData from file
Load to memory:
load_ai: Create LlamaAI instance from ModelData
Inference:
infer: Generate AI response to user message</p>
<p>EasyAI handles loading models, setting up messages/LLamaAI,
and generating responses. It provides a simple interface to using
LLama</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EasyAI:
    &#34;&#34;&#34;
    EasyAI provides a simple interface for LlamaAI based AI using quantized GGUF models
    for inference on CPU.
    
    Initialization:
    Can be intialized with no arguments, followed by following configuration methods, step by step, all in one or using a dict:\n
    Step by step call these with appropriate arguments:\n
        1. `self.load_model_db(model_db_dir: str = DEFAULT_LOCAL_GGUF_DIR)`
        2. One of the following with necessary args: `self.model_data_from_url()` or `self.model_data_from_file()` or `self.find_model_data()`
        3. `self.load_ai(max_tokens: int = 200, max_input_tokens: int = 100)`
    All in one:\n
        ```python
        self.configure(
            model_db_dir:str = DEFAULT_LOCAL_GGUF_DIR, 
            model_url: Optional[str] = None, 
            model_gguf_path: Optional[str] = None, 
            name_search: Optional[str] = None, 
            quantization_search: Optional[str] = None, 
            keyword_search: Optional[str] = None, 
            new_tokens: int = 200, 
            max_input_tokens: int = 100
            )
        ```
        
        Or with a dictionary with the following keys:
        - model_db_dir: Directory to store model data in. Defaults to DEFAULT_LOCAL_GGUF_DIR.
        - model_url: URL of model to configure with. Automatically downloads and builds as needed. (Optional)
        - name_search: Name of model to search for in the model db dir.(Optional)
        - quantization_search: Quantization of model to search for in the model db dir..(Optional)
        - keyword_search: Keyword of model to search for in the model db dir..(Optional)
        - model_gguf_path: Path to GGUF file of model to configure with.(Optional, not a recommended method, doesn&#39;t preserve download url)
        - new_tokens: Max tokens to be generated by LlamaAI model. (Defaults to 200, set to around 500-1k for regular use)
        - max_input_tokens: Max input tokens to be generated by LlamaAI model. (Defaults to 100, set to around 200-300 for regular use)

    Attributes:
        model_db: ModelDB for searching/loading models
        messages: AIMessages for tracking conversation 
        model_data: ModelData of selected model
        lai: LlamaAI instance for generating text

    Methods:
        DB:
            load_model_db: Load ModelDB from directory
        ModelData:
            find_model_data: Search model DB for ModelData
            model_data_from_url: Get ModelData from URL
            model_data_from_file: Load ModelData from file
        Load to memory:
            load_ai: Create LlamaAI instance from ModelData
        Inference:
            infer: Generate AI response to user message

    EasyAI handles loading models, setting up messages/LLamaAI,
    and generating responses. It provides a simple interface to using
    LLama
    &#34;&#34;&#34;
    def __init__(self, **kwds) -&gt; None:
        self.model_db: ModelDB = None
        self.messages: Optional[AIMessages] = None
        self.model_data: Optional[ModelData] = None
        self.ai: Optional[LlamaAI] = None
        if kwds:
            self.configure(**kwds)

    def configure(self,
                  model_db_dir: str = DEFAULT_LOCAL_GGUF_DIR,
                  model_url: Optional[str] = None,
                  model_gguf_path: Optional[str] = None,
                  name_search: Optional[str] = None,
                  quantization_search: Optional[str] = None,
                  keyword_search: Optional[str] = None,
                  new_tokens: int = 200,
                  max_input_tokens: int = 100) -&gt; None:
        &#34;&#34;&#34;
        Configure EasyAI with model data.

        Configures EasyAI with model data from given URL, GGUF file path, or model name/quantization/keyword.
        Sets model data attribute.

        Args:
            model_db_dir: Directory to store model data in. Defaults to DEFAULT_LOCAL_GGUF_DIR.
            max_tokens: Max tokens to be generated by LlamaAI model. (Defaults to 200, set to around 500-1k for regular use)
            max_input_tokens: Max input tokens to be generated by LlamaAI model. (Defaults to 100, set to around 200-300 for regular use)

            Provide at least one of these args to fetch ModelData: 
            ---
            model_url: URL of model to configure with. Automatically downloads and builds as needed. (Optional)
            name_search: Name of model to search for in the model db dir.(Optional)
            quantization_search: Quantization of model to search for in the model db dir..(Optional)
            keyword_search: Keyword of model to search for in the model db dir..(Optional)
            model_gguf_path: Path to GGUF file of model to configure with.(Optional, not a recommended method, doesn&#39;t preserve download url)
            ---

        Raises:
            Exception: If no model DB loaded.
            Exception: If no model data found.

        &#34;&#34;&#34;
        if model_db_dir is None:
            print(f&#34;Using default model DB dir: {DEFAULT_LOCAL_GGUF_DIR}&#34;)
        self.load_model_db(model_db_dir)
        if model_url is not None:
            self.model_data_from_url(model_url)
        elif model_gguf_path is not None:
            self.model_data_from_file(model_gguf_path)
        elif name_search is not None or quantization_search is not None or keyword_search is not None:
            self.find_model_data(name_search, quantization_search, keyword_search)
        else:
            raise Exception(&#34;Can&#39;t find model data. Please provide a model URL, GGUF file path, or model name/quantization/keyword.&#34;)
        
        self.load_ai(new_tokens, max_input_tokens)
    


            
            


    def load_model_db(self, db_dir: str = DEFAULT_LOCAL_GGUF_DIR, copy_examples=True) -&gt; None:
        &#34;&#34;&#34;
        Load ModelDB from given directory.

        Args:
            db_dir: Directory to load ModelDB from.
            copy_examples: Whether to copy example GGUF files to db_dir if db_dir is empty.
        &#34;&#34;&#34;
        self.model_db = ModelDB(model_db_dir=db_dir, copy_examples=copy_examples)

    def find_model_data(self,
                        model_name: Optional[str] = None,  
                        quantization: Optional[str] = None,
                        keyword: Optional[str] = None) -&gt; ModelData:
        &#34;&#34;&#34;
        Find model data in database that matches given criteria.

        Searches model database for model data matching the given model name, 
        quantization, and/or keyword. Any parameters left as None are not used  
        in the search.

        Args:
            model_name: Name of model to search for.
            quantization: Quantization of model to search for.
            keyword: Keyword of model to search for.

        Returns:
            ModelData object if a match is found, else None.
        &#34;&#34;&#34;
        if self.model_db is None:
            raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
        model_data = self.model_db.find_model(model_name, quantization, keyword)
        self.model_data = model_data
        return model_data

    def model_data_from_url(self,
                            url: str,
                            user_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                            ai_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                            description: Optional[str] = None,
                            keywords: Optional[str] = None,
                            save: bool = True) -&gt; None:
        &#34;&#34;&#34;
        Get model data for URL, downloading model if needed.

        Checks if model data already exists for the given URL. If not, downloads
        the model from the URL and creates new model data. Sets model data attribute.

        Args:
            url: URL of model to get data for.
            user_tags: User tags to assign if creating new model data. 
            ai_tags: AI tags to assign if creating new model data.
            description: Optional description for new model data.
            keyword: Optional keyword for new model data.
            save: Whether to save new model data JSON file.
        &#34;&#34;&#34;
        if self.model_db is None:
            raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
        print(f&#34;Trying to get model data from url: {url}&#34;)
        print(f&#34;Checking if model data already exists...&#34;)
        model_data = self.model_db.get_model_by_url(url)
        if model_data is None:
            print(f&#34;Model data not found. Creating new model data...&#34;)
            model_data = ModelData(url, self.model_db.gguf_db_dir, user_tags, ai_tags, description, keywords)
            print(f&#34;Created model data: {model_data}&#34;)
        else:
            print(f&#34;Found model data: {model_data}&#34;)
        if save:
            model_data.save_json()
        self.model_data = model_data

    def model_data_from_file(self,
                             gguf_file_path: str,
                             user_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                             ai_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                             description: Optional[str] = None,
                             keyword: Optional[str] = None,
                             save: bool = False) -&gt; None:
        &#34;&#34;&#34;
        Get model data from local GGUF file.

        Loads model data from the given local GGUF file path. Sets model data attribute.

        Args:
            gguf_file_path: Path to GGUF file.
            user_tags: User tags to assign to model data.
            ai_tags: AI tags to assign to model data.
            description: Optional description for model data.
            keyword: Optional keyword for model data.
            save: Whether to save model data JSON file.
        &#34;&#34;&#34;
        if self.model_db is None:
            raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
        model_data = ModelData.from_file(gguf_file_path, self.model_db.gguf_db_dir, user_tags, ai_tags, description, keyword)
        if save:
            model_data.save_json()
        self.model_data = model_data

    def _load_messages(self) -&gt; None:
        &#34;&#34;&#34;
        Load AIMessages using tags from model data.

        Uses user_tags and ai_tags from loaded model data to initialize AIMessages object. 
        Sets messages attribute.

        Raises:
            Exception: If no model data loaded yet.
        &#34;&#34;&#34;
        if self.model_data is None:
            raise Exception(&#34;No model data loaded. Use find_model_data(), get_model_data_from_url(), or get_model_data_from_file() first.&#34;)
        self.messages = AIMessages(user_tags=self.model_data.user_tags, ai_tags=self.model_data.ai_tags)

    def load_ai(self,
                max_tokens: int = 200,
                max_input_tokens: int = 100) -&gt; None:
        &#34;&#34;&#34;
        Load LlamaAI model from model data.

        Downloads model file from model data URL if needed. Initializes LlamaAI with model and sets lai attribute.

        Args:
            max_tokens: Max tokens for LlamaAI model.
            max_input_tokens: Max input tokens for LlamaAI model.

        Raises:
            Exception: If no model data or messages loaded yet.
        &#34;&#34;&#34;
        self._load_messages()
        if self.messages is None:
            raise Exception(&#34;No messages loaded. Use load_messages() first.&#34;)
        if self.model_data is None:
            raise Exception(&#34;No model data loaded. Use find_model_data(), get_model_data_from_url(), or get_model_data_from_file() first.&#34;)
        self.model_data.download_gguf()
        self.ai = LlamaAI(self.model_data.model_path(), max_tokens=max_tokens, max_input_tokens=max_input_tokens)
        print(f&#34;Loaded: {self.model_data}&#34;)

    def generate(self,
              user_message: str,
              ai_response_content_tbc: Optional[str] = None,
              stop_at:Optional[str]=None,
              include_stop_str:bool=True
              ) -&gt; AIMessage:
        &#34;&#34;&#34;
        Generate AI response to user message.

        Runs user message through loaded LlamaAI to generate response. Allows prepending optional 
        content to AI response. Adds messages and returns generated AIMessage.

        Args:
            user_message: User message text.
            ai_response_content_tbc: Optional text to prepend to AI response.
            stop_at: Optional string to stop generation at.
            include_stop_str: Whether to include stop string in generated message.

        Returns:
            Generated AIMessage object.

        Raises:
            Exception: If no AI or messages loaded yet.
        &#34;&#34;&#34;
        if self.ai is None:
            raise Exception(&#34;No AI loaded. Use load_ai() first.&#34;)
        if self.messages is None:
            raise Exception(&#34;No messages loaded. Use load_ai() first.&#34;)
        self.messages.reset_messages()
        self.messages.add_user_message(user_message)
        print(f&#34;User message: \n{self.messages.get_last_message()}&#34;)
        generated: str = &#34;&#34;
        if not (ai_response_content_tbc == &#34;&#34; or ai_response_content_tbc is None):
            generated += ai_response_content_tbc
            self.messages.add_message(ai_response_content_tbc, self.model_data.get_ai_tag_open(), &#34;&#34;)
        if stop_at is None:
            stop_at = self.messages.ai_tag_close if any([self.messages.ai_tag_close is None, self.messages.ai_tag_close == &#34;&#34;, self.messages.ai_tag_close != &#34; &#34;]) else None
        generated += self.ai.infer(self.messages.text(), only_string=True, stop_at_str=stop_at, include_stop_str=include_stop_str)
        if not (ai_response_content_tbc == &#34;&#34; or ai_response_content_tbc is None):
            self.messages.edit_last_message(generated,
                                            self.model_data.get_ai_tag_open(),
                                            self.model_data.get_ai_tag_close())
        else:
            self.messages.add_ai_message(generated)
        print(f&#34;AI message: \n{self.messages.get_last_message()}&#34;)
        return self.messages.get_last_message()

    



    def count_tokens(
        self,
        user_message_text: str,
        ai_message_to_be_continued: Optional[str] = None
    ) -&gt; int:
        &#34;&#34;&#34;
        Count the number of tokens in a generated message.

        Args:
            user_message_text: User message text.
            ai_message_to_be_continued: Optional text to prepend.

        Returns:
            Number of tokens in generated message.
        &#34;&#34;&#34;
        generation_messages = AIMessages()
        generation_messages.reset_messages()
        generation_messages.add_user_message(user_message_text)

        if ai_message_to_be_continued is not None:
            generation_messages.add_message(
                ai_message_to_be_continued, 
                self.messages.ai_tag_open, 
                &#34;&#34;
            )

        return self.ai.count_tokens(generation_messages.text())
    
    def is_within_input_limit(
        self,
        user_message_text: str,
        ai_message_to_be_continued: Optional[str] = None
        ) -&gt; bool:
        &#34;&#34;&#34;
        Check if the generated message is within the input limit.

        Args:
            user_message_text: User message text.
            ai_message_to_be_continued: Optional text to prepend.

        Returns:
            True if within input limit, False otherwise.
        &#34;&#34;&#34;
        return self.ai.is_within_input_limit(self.count_tokens(user_message_text, ai_message_to_be_continued))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="glai.EasyAI.configure"><code class="name flex">
<span>def <span class="ident">configure</span></span>(<span>self, model_db_dir: str = './gguf_db', model_url: Optional[str] = None, model_gguf_path: Optional[str] = None, name_search: Optional[str] = None, quantization_search: Optional[str] = None, keyword_search: Optional[str] = None, new_tokens: int = 200, max_input_tokens: int = 100) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Configure EasyAI with model data.</p>
<p>Configures EasyAI with model data from given URL, GGUF file path, or model name/quantization/keyword.
Sets model data attribute.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_db_dir</code></strong></dt>
<dd>Directory to store model data in. Defaults to DEFAULT_LOCAL_GGUF_DIR.</dd>
<dt><strong><code>max_tokens</code></strong></dt>
<dd>Max tokens to be generated by LlamaAI model. (Defaults to 200, set to around 500-1k for regular use)</dd>
<dt><strong><code>max_input_tokens</code></strong></dt>
<dd>Max input tokens to be generated by LlamaAI model. (Defaults to 100, set to around 200-300 for regular use)</dd>
</dl>
<h2 id="provide-at-least-one-of-these-args-to-fetch-modeldata">Provide at least one of these args to fetch ModelData:</h2>
<dl>
<dt><strong><code>model_url</code></strong></dt>
<dd>URL of model to configure with. Automatically downloads and builds as needed. (Optional)</dd>
<dt><strong><code>name_search</code></strong></dt>
<dd>Name of model to search for in the model db dir.(Optional)</dd>
<dt><strong><code>quantization_search</code></strong></dt>
<dd>Quantization of model to search for in the model db dir..(Optional)</dd>
<dt><strong><code>keyword_search</code></strong></dt>
<dd>Keyword of model to search for in the model db dir..(Optional)</dd>
<dt><strong><code>model_gguf_path</code></strong></dt>
<dd>Path to GGUF file of model to configure with.(Optional, not a recommended method, doesn't preserve download url)</dd>
</dl>
<hr>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If no model DB loaded.</dd>
<dt><code>Exception</code></dt>
<dd>If no model data found.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure(self,
              model_db_dir: str = DEFAULT_LOCAL_GGUF_DIR,
              model_url: Optional[str] = None,
              model_gguf_path: Optional[str] = None,
              name_search: Optional[str] = None,
              quantization_search: Optional[str] = None,
              keyword_search: Optional[str] = None,
              new_tokens: int = 200,
              max_input_tokens: int = 100) -&gt; None:
    &#34;&#34;&#34;
    Configure EasyAI with model data.

    Configures EasyAI with model data from given URL, GGUF file path, or model name/quantization/keyword.
    Sets model data attribute.

    Args:
        model_db_dir: Directory to store model data in. Defaults to DEFAULT_LOCAL_GGUF_DIR.
        max_tokens: Max tokens to be generated by LlamaAI model. (Defaults to 200, set to around 500-1k for regular use)
        max_input_tokens: Max input tokens to be generated by LlamaAI model. (Defaults to 100, set to around 200-300 for regular use)

        Provide at least one of these args to fetch ModelData: 
        ---
        model_url: URL of model to configure with. Automatically downloads and builds as needed. (Optional)
        name_search: Name of model to search for in the model db dir.(Optional)
        quantization_search: Quantization of model to search for in the model db dir..(Optional)
        keyword_search: Keyword of model to search for in the model db dir..(Optional)
        model_gguf_path: Path to GGUF file of model to configure with.(Optional, not a recommended method, doesn&#39;t preserve download url)
        ---

    Raises:
        Exception: If no model DB loaded.
        Exception: If no model data found.

    &#34;&#34;&#34;
    if model_db_dir is None:
        print(f&#34;Using default model DB dir: {DEFAULT_LOCAL_GGUF_DIR}&#34;)
    self.load_model_db(model_db_dir)
    if model_url is not None:
        self.model_data_from_url(model_url)
    elif model_gguf_path is not None:
        self.model_data_from_file(model_gguf_path)
    elif name_search is not None or quantization_search is not None or keyword_search is not None:
        self.find_model_data(name_search, quantization_search, keyword_search)
    else:
        raise Exception(&#34;Can&#39;t find model data. Please provide a model URL, GGUF file path, or model name/quantization/keyword.&#34;)
    
    self.load_ai(new_tokens, max_input_tokens)</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.count_tokens"><code class="name flex">
<span>def <span class="ident">count_tokens</span></span>(<span>self, user_message_text: str, ai_message_to_be_continued: Optional[str] = None) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Count the number of tokens in a generated message.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user_message_text</code></strong></dt>
<dd>User message text.</dd>
<dt><strong><code>ai_message_to_be_continued</code></strong></dt>
<dd>Optional text to prepend.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Number of tokens in generated message.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def count_tokens(
    self,
    user_message_text: str,
    ai_message_to_be_continued: Optional[str] = None
) -&gt; int:
    &#34;&#34;&#34;
    Count the number of tokens in a generated message.

    Args:
        user_message_text: User message text.
        ai_message_to_be_continued: Optional text to prepend.

    Returns:
        Number of tokens in generated message.
    &#34;&#34;&#34;
    generation_messages = AIMessages()
    generation_messages.reset_messages()
    generation_messages.add_user_message(user_message_text)

    if ai_message_to_be_continued is not None:
        generation_messages.add_message(
            ai_message_to_be_continued, 
            self.messages.ai_tag_open, 
            &#34;&#34;
        )

    return self.ai.count_tokens(generation_messages.text())</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.find_model_data"><code class="name flex">
<span>def <span class="ident">find_model_data</span></span>(<span>self, model_name: Optional[str] = None, quantization: Optional[str] = None, keyword: Optional[str] = None) ‑> <a title="glai.back_end.model_db.model_data.ModelData" href="back_end/model_db/model_data.html#glai.back_end.model_db.model_data.ModelData">ModelData</a></span>
</code></dt>
<dd>
<div class="desc"><p>Find model data in database that matches given criteria.</p>
<p>Searches model database for model data matching the given model name,
quantization, and/or keyword. Any parameters left as None are not used<br>
in the search.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_name</code></strong></dt>
<dd>Name of model to search for.</dd>
<dt><strong><code>quantization</code></strong></dt>
<dd>Quantization of model to search for.</dd>
<dt><strong><code>keyword</code></strong></dt>
<dd>Keyword of model to search for.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ModelData object if a match is found, else None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_model_data(self,
                    model_name: Optional[str] = None,  
                    quantization: Optional[str] = None,
                    keyword: Optional[str] = None) -&gt; ModelData:
    &#34;&#34;&#34;
    Find model data in database that matches given criteria.

    Searches model database for model data matching the given model name, 
    quantization, and/or keyword. Any parameters left as None are not used  
    in the search.

    Args:
        model_name: Name of model to search for.
        quantization: Quantization of model to search for.
        keyword: Keyword of model to search for.

    Returns:
        ModelData object if a match is found, else None.
    &#34;&#34;&#34;
    if self.model_db is None:
        raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
    model_data = self.model_db.find_model(model_name, quantization, keyword)
    self.model_data = model_data
    return model_data</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.generate"><code class="name flex">
<span>def <span class="ident">generate</span></span>(<span>self, user_message: str, ai_response_content_tbc: Optional[str] = None, stop_at: Optional[str] = None, include_stop_str: bool = True) ‑> <a title="glai.back_end.messages.AIMessage" href="back_end/messages.html#glai.back_end.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Generate AI response to user message.</p>
<p>Runs user message through loaded LlamaAI to generate response. Allows prepending optional
content to AI response. Adds messages and returns generated AIMessage.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user_message</code></strong></dt>
<dd>User message text.</dd>
<dt><strong><code>ai_response_content_tbc</code></strong></dt>
<dd>Optional text to prepend to AI response.</dd>
<dt><strong><code>stop_at</code></strong></dt>
<dd>Optional string to stop generation at.</dd>
<dt><strong><code>include_stop_str</code></strong></dt>
<dd>Whether to include stop string in generated message.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Generated AIMessage object.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If no AI or messages loaded yet.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate(self,
          user_message: str,
          ai_response_content_tbc: Optional[str] = None,
          stop_at:Optional[str]=None,
          include_stop_str:bool=True
          ) -&gt; AIMessage:
    &#34;&#34;&#34;
    Generate AI response to user message.

    Runs user message through loaded LlamaAI to generate response. Allows prepending optional 
    content to AI response. Adds messages and returns generated AIMessage.

    Args:
        user_message: User message text.
        ai_response_content_tbc: Optional text to prepend to AI response.
        stop_at: Optional string to stop generation at.
        include_stop_str: Whether to include stop string in generated message.

    Returns:
        Generated AIMessage object.

    Raises:
        Exception: If no AI or messages loaded yet.
    &#34;&#34;&#34;
    if self.ai is None:
        raise Exception(&#34;No AI loaded. Use load_ai() first.&#34;)
    if self.messages is None:
        raise Exception(&#34;No messages loaded. Use load_ai() first.&#34;)
    self.messages.reset_messages()
    self.messages.add_user_message(user_message)
    print(f&#34;User message: \n{self.messages.get_last_message()}&#34;)
    generated: str = &#34;&#34;
    if not (ai_response_content_tbc == &#34;&#34; or ai_response_content_tbc is None):
        generated += ai_response_content_tbc
        self.messages.add_message(ai_response_content_tbc, self.model_data.get_ai_tag_open(), &#34;&#34;)
    if stop_at is None:
        stop_at = self.messages.ai_tag_close if any([self.messages.ai_tag_close is None, self.messages.ai_tag_close == &#34;&#34;, self.messages.ai_tag_close != &#34; &#34;]) else None
    generated += self.ai.infer(self.messages.text(), only_string=True, stop_at_str=stop_at, include_stop_str=include_stop_str)
    if not (ai_response_content_tbc == &#34;&#34; or ai_response_content_tbc is None):
        self.messages.edit_last_message(generated,
                                        self.model_data.get_ai_tag_open(),
                                        self.model_data.get_ai_tag_close())
    else:
        self.messages.add_ai_message(generated)
    print(f&#34;AI message: \n{self.messages.get_last_message()}&#34;)
    return self.messages.get_last_message()</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.is_within_input_limit"><code class="name flex">
<span>def <span class="ident">is_within_input_limit</span></span>(<span>self, user_message_text: str, ai_message_to_be_continued: Optional[str] = None) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Check if the generated message is within the input limit.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user_message_text</code></strong></dt>
<dd>User message text.</dd>
<dt><strong><code>ai_message_to_be_continued</code></strong></dt>
<dd>Optional text to prepend.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>True if within input limit, False otherwise.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_within_input_limit(
    self,
    user_message_text: str,
    ai_message_to_be_continued: Optional[str] = None
    ) -&gt; bool:
    &#34;&#34;&#34;
    Check if the generated message is within the input limit.

    Args:
        user_message_text: User message text.
        ai_message_to_be_continued: Optional text to prepend.

    Returns:
        True if within input limit, False otherwise.
    &#34;&#34;&#34;
    return self.ai.is_within_input_limit(self.count_tokens(user_message_text, ai_message_to_be_continued))</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.load_ai"><code class="name flex">
<span>def <span class="ident">load_ai</span></span>(<span>self, max_tokens: int = 200, max_input_tokens: int = 100) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Load LlamaAI model from model data.</p>
<p>Downloads model file from model data URL if needed. Initializes LlamaAI with model and sets lai attribute.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>max_tokens</code></strong></dt>
<dd>Max tokens for LlamaAI model.</dd>
<dt><strong><code>max_input_tokens</code></strong></dt>
<dd>Max input tokens for LlamaAI model.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If no model data or messages loaded yet.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_ai(self,
            max_tokens: int = 200,
            max_input_tokens: int = 100) -&gt; None:
    &#34;&#34;&#34;
    Load LlamaAI model from model data.

    Downloads model file from model data URL if needed. Initializes LlamaAI with model and sets lai attribute.

    Args:
        max_tokens: Max tokens for LlamaAI model.
        max_input_tokens: Max input tokens for LlamaAI model.

    Raises:
        Exception: If no model data or messages loaded yet.
    &#34;&#34;&#34;
    self._load_messages()
    if self.messages is None:
        raise Exception(&#34;No messages loaded. Use load_messages() first.&#34;)
    if self.model_data is None:
        raise Exception(&#34;No model data loaded. Use find_model_data(), get_model_data_from_url(), or get_model_data_from_file() first.&#34;)
    self.model_data.download_gguf()
    self.ai = LlamaAI(self.model_data.model_path(), max_tokens=max_tokens, max_input_tokens=max_input_tokens)
    print(f&#34;Loaded: {self.model_data}&#34;)</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.load_model_db"><code class="name flex">
<span>def <span class="ident">load_model_db</span></span>(<span>self, db_dir: str = './gguf_db', copy_examples=True) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Load ModelDB from given directory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>db_dir</code></strong></dt>
<dd>Directory to load ModelDB from.</dd>
<dt><strong><code>copy_examples</code></strong></dt>
<dd>Whether to copy example GGUF files to db_dir if db_dir is empty.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model_db(self, db_dir: str = DEFAULT_LOCAL_GGUF_DIR, copy_examples=True) -&gt; None:
    &#34;&#34;&#34;
    Load ModelDB from given directory.

    Args:
        db_dir: Directory to load ModelDB from.
        copy_examples: Whether to copy example GGUF files to db_dir if db_dir is empty.
    &#34;&#34;&#34;
    self.model_db = ModelDB(model_db_dir=db_dir, copy_examples=copy_examples)</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.model_data_from_file"><code class="name flex">
<span>def <span class="ident">model_data_from_file</span></span>(<span>self, gguf_file_path: str, user_tags: Tuple[str, str] = ('', ''), ai_tags: Tuple[str, str] = ('', ''), description: Optional[str] = None, keyword: Optional[str] = None, save: bool = False) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Get model data from local GGUF file.</p>
<p>Loads model data from the given local GGUF file path. Sets model data attribute.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gguf_file_path</code></strong></dt>
<dd>Path to GGUF file.</dd>
<dt><strong><code>user_tags</code></strong></dt>
<dd>User tags to assign to model data.</dd>
<dt><strong><code>ai_tags</code></strong></dt>
<dd>AI tags to assign to model data.</dd>
<dt><strong><code>description</code></strong></dt>
<dd>Optional description for model data.</dd>
<dt><strong><code>keyword</code></strong></dt>
<dd>Optional keyword for model data.</dd>
<dt><strong><code>save</code></strong></dt>
<dd>Whether to save model data JSON file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_data_from_file(self,
                         gguf_file_path: str,
                         user_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                         ai_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                         description: Optional[str] = None,
                         keyword: Optional[str] = None,
                         save: bool = False) -&gt; None:
    &#34;&#34;&#34;
    Get model data from local GGUF file.

    Loads model data from the given local GGUF file path. Sets model data attribute.

    Args:
        gguf_file_path: Path to GGUF file.
        user_tags: User tags to assign to model data.
        ai_tags: AI tags to assign to model data.
        description: Optional description for model data.
        keyword: Optional keyword for model data.
        save: Whether to save model data JSON file.
    &#34;&#34;&#34;
    if self.model_db is None:
        raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
    model_data = ModelData.from_file(gguf_file_path, self.model_db.gguf_db_dir, user_tags, ai_tags, description, keyword)
    if save:
        model_data.save_json()
    self.model_data = model_data</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.model_data_from_url"><code class="name flex">
<span>def <span class="ident">model_data_from_url</span></span>(<span>self, url: str, user_tags: Tuple[str, str] = ('', ''), ai_tags: Tuple[str, str] = ('', ''), description: Optional[str] = None, keywords: Optional[str] = None, save: bool = True) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Get model data for URL, downloading model if needed.</p>
<p>Checks if model data already exists for the given URL. If not, downloads
the model from the URL and creates new model data. Sets model data attribute.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>url</code></strong></dt>
<dd>URL of model to get data for.</dd>
<dt><strong><code>user_tags</code></strong></dt>
<dd>User tags to assign if creating new model data. </dd>
<dt><strong><code>ai_tags</code></strong></dt>
<dd>AI tags to assign if creating new model data.</dd>
<dt><strong><code>description</code></strong></dt>
<dd>Optional description for new model data.</dd>
<dt><strong><code>keyword</code></strong></dt>
<dd>Optional keyword for new model data.</dd>
<dt><strong><code>save</code></strong></dt>
<dd>Whether to save new model data JSON file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_data_from_url(self,
                        url: str,
                        user_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                        ai_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                        description: Optional[str] = None,
                        keywords: Optional[str] = None,
                        save: bool = True) -&gt; None:
    &#34;&#34;&#34;
    Get model data for URL, downloading model if needed.

    Checks if model data already exists for the given URL. If not, downloads
    the model from the URL and creates new model data. Sets model data attribute.

    Args:
        url: URL of model to get data for.
        user_tags: User tags to assign if creating new model data. 
        ai_tags: AI tags to assign if creating new model data.
        description: Optional description for new model data.
        keyword: Optional keyword for new model data.
        save: Whether to save new model data JSON file.
    &#34;&#34;&#34;
    if self.model_db is None:
        raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
    print(f&#34;Trying to get model data from url: {url}&#34;)
    print(f&#34;Checking if model data already exists...&#34;)
    model_data = self.model_db.get_model_by_url(url)
    if model_data is None:
        print(f&#34;Model data not found. Creating new model data...&#34;)
        model_data = ModelData(url, self.model_db.gguf_db_dir, user_tags, ai_tags, description, keywords)
        print(f&#34;Created model data: {model_data}&#34;)
    else:
        print(f&#34;Found model data: {model_data}&#34;)
    if save:
        model_data.save_json()
    self.model_data = model_data</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="glai.ModelDB"><code class="flex name class">
<span>class <span class="ident">ModelDB</span></span>
<span>(</span><span>model_db_dir: Optional[str] = None, copy_examples=True)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelDB:
    def __init__(self, model_db_dir:Optional[str]=None, copy_examples=True):
        self.gguf_db_dir = None
        self.models = []

        if model_db_dir is None:
            model_db_dir = MODEL_EXAMPLES_DB_DIR
        self.set_model_db_dir(model_db_dir)
        if model_db_dir != MODEL_EXAMPLES_DB_DIR:
            print(f&#34;Copying examples to {model_db_dir}...&#34;)
            if copy_examples:
                for file in list_files_in_dir(MODEL_EXAMPLES_DB_DIR, show_directories=False, only_with_extensions=[&#34;.json&#34;], absolute=False):
                    f_mdt = ModelData.from_json(join_paths(MODEL_EXAMPLES_DB_DIR, file))
                    f_mdt.set_save_dir(model_db_dir)
                    f_mdt.save_json()
                    print(f&#34;Saved a copy of {file} to {model_db_dir}.&#34;)
        else:
            print(f&#34;Using default model db dir: {model_db_dir}, changes here will be visible and accessible globaly. If you would like to work with a specific db_dir provide it as an argument to the ModelDB constructor.&#34;)
        self.load_models()
    
    def set_model_db_dir(self, model_db_dir:str) -&gt; None:
        print(f&#34;ModelDB dir set to {model_db_dir}.&#34;)
        self.gguf_db_dir = create_dir(model_db_dir)
    
    def load_models(self) -&gt; None:
        self.models = []
        files = list_files_in_dir(self.gguf_db_dir, show_directories=False, show_files=True, only_with_extensions=[&#34;.json&#34;])
        for file in files:
            try:
                file_path = join_paths(self.gguf_db_dir, file)
                model_data = ModelData.from_json(file_path)
                self.models.append(model_data)
            except Exception as e:
                print(f&#34;Error trying to load from {file_path}: \t\n{e}, \nskipping...&#34;)
                continue
        print(f&#34;Loaded {len(self.models)} models from {self.gguf_db_dir}.&#34;)

    def find_models(self, name_query:Optional[str]=None, 
                   quantization_query:Optional[str]=None, 
                   keywords_query:Optional[str]=None,
                   treshold:float=0.5) -&gt; Union[None, list]:
        if name_query is None and quantization_query is None and keywords_query is None:
            return None
        scoring_models_dict = {}
        for i, model in enumerate(self.models):
            scoring_models_dict[i] = {&#34;model&#34;:model, &#34;score&#34;:0}
        for id in scoring_models_dict.keys():
            model = scoring_models_dict[id][&#34;model&#34;]
            model:ModelData = model
            model_name = model.name
            model_quantization = model.model_quantization
            model_keywords = model.keywords
            if name_query is not None:
                #print(f&#34;Searching for name: {name_query}&#34;)
                top_name_score = 0
                for model_subname in model_name.split(&#34;-&#34;):
                    name_score = compare_two_strings(name_query, model_subname)
                    if name_score &gt; top_name_score:
                        top_name_score = name_score
                if top_name_score &gt; treshold:
                    scoring_models_dict[id][&#34;score&#34;] += top_name_score
                #print(f&#34;Model {model_name} {model_quantization} top score: {top_name_score} treshold: {treshold}&#34;)
            if quantization_query is not None:
                #print(f&#34;Searching for quantization: {quantization_query}&#34;)
                quantization_score = compare_two_strings(quantization_query, model_quantization)
                if quantization_score &gt; treshold:
                    scoring_models_dict[id][&#34;score&#34;] += quantization_score
                #print(f&#34;Model {model_name} {model_quantization} score: {quantization_score} treshold: {treshold}&#34;)
            if keywords_query is not None:
                #print(f&#34;Searching for keyword: {keywords_query}&#34;)
                best_keyword_score = 0
                for keyword in model_keywords:
                    keyword_score = compare_two_strings(keywords_query, keyword)
                    if keyword_score &gt; best_keyword_score:
                        best_keyword_score = keyword_score
                if best_keyword_score &gt; treshold:
                    scoring_models_dict[id][&#34;score&#34;] += best_keyword_score
                #print(f&#34;Model {model_name} {model_quantization} score: {best_keyword_score} treshold: {treshold}&#34;)
            #print(f&#34;Model {model_name} {model_quantization} score: {scoring_models_dict[id][&#39;score&#39;]}&#34;)
        sorted_models = sorted(scoring_models_dict.items(), key=lambda x: x[1][&#34;score&#34;], reverse=True)
        #keep just the list of model data
        sorted_models = [x[1][&#34;model&#34;] for x in sorted_models]
        #print(f&#34;Found {len(sorted_models)} models.&#34;)
        #print(sorted_models)
        return sorted_models
    
    def find_model(self, name_query:Optional[str]=None, 
                   quantization_query:Optional[str]=None, 
                   keywords_query:Optional[str]=None,
                   ) -&gt; Optional[ModelData]:
        sorted_models = self.find_models(name_query, quantization_query, keywords_query)
        if sorted_models is None:
            return None
        else:
            #print(f&#34;Found {len(sorted_models)} models.&#34;)
            #print(sorted_models)
            return sorted_models[0]
    def get_model_by_url(self, url:str) -&gt; Optional[ModelData]:
        for model in self.models:
            model:ModelData = model
            if model.gguf_url == url:
                return model
        return None    
    def add_model_data(self, model_data:ModelData, save_model=True) -&gt; None:
        self.models.append(model_data)
        if save_model:
            model_data.save_json()
    
    def add_model_by_url(self, url:str, ) -&gt; None:
        model_data = ModelData(url, db_dir=self.gguf_db_dir)
        self.add_model_data(model_data)

    def add_model_by_json(self, json_file_path:str) -&gt; None:
        model_data = ModelData.from_json(json_file_path)
        self.add_model_data(model_data)

    def save_all_models(self) -&gt; None:
        for model in self.models:
            model:ModelData = model
            model.save_json()
                
    @staticmethod
    def _model_links_from_repo(hf_repo_url:str):
        #extract models from hf 
        response = requests.get(hf_repo_url)
        html = response.text
        soup = bs4.BeautifulSoup(html, &#39;html.parser&#39;)
        #find all links that end with .gguf
        print(f&#34;Looking for {hf_repo_url} gguf files...&#34;)
        model_links = []
        for link in soup.find_all(&#39;a&#39;):
            href = link.get(&#39;href&#39;)
            if href.endswith(&#34;.gguf&#34;):
                print(f&#34;Found model: {href}&#34;)
                model_links.append(href)
        return model_links
    def load_models_data_from_repo(self, hf_repo_url:str, 
                        user_tags:Optional[list[str]]=None,
                        ai_tags:Optional[list[str]]=None,
                        keywords:Optional[list[str]]=None, 
                        description:Optional[str]=None):  
        #create model data from hf repo
        model_links = ModelDB._model_links_from_repo(hf_repo_url)
        model_datas = []
        for model_link in model_links:
            model_data = ModelData(gguf_url=model_link, db_dir=self.gguf_db_dir, user_tags=user_tags, ai_tags=ai_tags, description=description, keywords=keywords)
            model_datas.append(model_data)
            model_data.save_json()
        self.models.extend(model_datas)
        return model_datas

    def import_models_from_repo(self, hf_repo_url:str,
                        user_tags:Optional[list[str]]=None,
                        ai_tags:Optional[list[str]]=None,
                        keywords:Optional[list[str]]=None, 
                        description:Optional[str]=None,
                        replace_existing:bool=False,
                        ):  
        #create model data from hf repo
        model_links = ModelDB._model_links_from_repo(hf_repo_url)
        for model_link in model_links:
            model_data = ModelData(gguf_url=model_link, db_dir=self.gguf_db_dir, user_tags=user_tags, ai_tags=ai_tags, description=description, keywords=keywords)
            model_data.save_json(replace_existing=replace_existing)
        self.load_models()
    
    def list_available_models(self) -&gt; list[str]:
        print(f&#34;Available models in {self.gguf_db_dir}:&#34;)
        models = []
        for model in self.models:
            model:ModelData = model
            if model.name not in models:
                models.append(model.name)
        return models
    
    def list_models_quantizations(self, model_name:str) -&gt; list[str]:
        quantizations = []
        for model in self.models:
            model:ModelData = model
            if model.name == model_name:
                quantizations.append(model.model_quantization)
        return quantizations

    def show_db_info(self) -&gt; None:
        print(f&#34;ModelDB summary:&#34;)
        print(f&#34;ModelDB dir: {self.gguf_db_dir}&#34;)
        print(f&#34;Number of models: {len(self.models)}&#34;)
        print(f&#34;Available models:&#34;)
        models_info = {}
        for model in self.models:
            model:ModelData = model
            if model.name not in models_info.keys():
                models_info[model.name] = {}
                models_info[model.name][&#34;quantizations&#34;] = []
                models_info[model.name][&#34;description&#34;] = model.description
                models_info[model.name][&#34;keywords&#34;] = model.keywords
            if model.model_quantization not in models_info[model.name][&#34;quantizations&#34;]:
                models_info[model.name][&#34;quantizations&#34;].append(model.model_quantization)
        
        for model_name, models_info in models_info.items():
            print(f&#34;\t{model_name}:&#34;)
            print(f&#34;\t\tQuantizations: {models_info[&#39;quantizations&#39;]}&#34;)
            print(f&#34;\t\tKeywords: {models_info[&#39;keywords&#39;]}&#34;)
            print(f&#34;\t\tDescription: {models_info[&#39;description&#39;]}&#34;)
            print(f&#34;\t-------------------------------&#34;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="glai.ModelDB.add_model_by_json"><code class="name flex">
<span>def <span class="ident">add_model_by_json</span></span>(<span>self, json_file_path: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_model_by_json(self, json_file_path:str) -&gt; None:
    model_data = ModelData.from_json(json_file_path)
    self.add_model_data(model_data)</code></pre>
</details>
</dd>
<dt id="glai.ModelDB.add_model_by_url"><code class="name flex">
<span>def <span class="ident">add_model_by_url</span></span>(<span>self, url: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_model_by_url(self, url:str, ) -&gt; None:
    model_data = ModelData(url, db_dir=self.gguf_db_dir)
    self.add_model_data(model_data)</code></pre>
</details>
</dd>
<dt id="glai.ModelDB.add_model_data"><code class="name flex">
<span>def <span class="ident">add_model_data</span></span>(<span>self, model_data: <a title="glai.back_end.model_db.model_data.ModelData" href="back_end/model_db/model_data.html#glai.back_end.model_db.model_data.ModelData">ModelData</a>, save_model=True) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_model_data(self, model_data:ModelData, save_model=True) -&gt; None:
    self.models.append(model_data)
    if save_model:
        model_data.save_json()</code></pre>
</details>
</dd>
<dt id="glai.ModelDB.find_model"><code class="name flex">
<span>def <span class="ident">find_model</span></span>(<span>self, name_query: Optional[str] = None, quantization_query: Optional[str] = None, keywords_query: Optional[str] = None) ‑> Optional[<a title="glai.back_end.model_db.model_data.ModelData" href="back_end/model_db/model_data.html#glai.back_end.model_db.model_data.ModelData">ModelData</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_model(self, name_query:Optional[str]=None, 
               quantization_query:Optional[str]=None, 
               keywords_query:Optional[str]=None,
               ) -&gt; Optional[ModelData]:
    sorted_models = self.find_models(name_query, quantization_query, keywords_query)
    if sorted_models is None:
        return None
    else:
        #print(f&#34;Found {len(sorted_models)} models.&#34;)
        #print(sorted_models)
        return sorted_models[0]</code></pre>
</details>
</dd>
<dt id="glai.ModelDB.find_models"><code class="name flex">
<span>def <span class="ident">find_models</span></span>(<span>self, name_query: Optional[str] = None, quantization_query: Optional[str] = None, keywords_query: Optional[str] = None, treshold: float = 0.5) ‑> Optional[None]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_models(self, name_query:Optional[str]=None, 
               quantization_query:Optional[str]=None, 
               keywords_query:Optional[str]=None,
               treshold:float=0.5) -&gt; Union[None, list]:
    if name_query is None and quantization_query is None and keywords_query is None:
        return None
    scoring_models_dict = {}
    for i, model in enumerate(self.models):
        scoring_models_dict[i] = {&#34;model&#34;:model, &#34;score&#34;:0}
    for id in scoring_models_dict.keys():
        model = scoring_models_dict[id][&#34;model&#34;]
        model:ModelData = model
        model_name = model.name
        model_quantization = model.model_quantization
        model_keywords = model.keywords
        if name_query is not None:
            #print(f&#34;Searching for name: {name_query}&#34;)
            top_name_score = 0
            for model_subname in model_name.split(&#34;-&#34;):
                name_score = compare_two_strings(name_query, model_subname)
                if name_score &gt; top_name_score:
                    top_name_score = name_score
            if top_name_score &gt; treshold:
                scoring_models_dict[id][&#34;score&#34;] += top_name_score
            #print(f&#34;Model {model_name} {model_quantization} top score: {top_name_score} treshold: {treshold}&#34;)
        if quantization_query is not None:
            #print(f&#34;Searching for quantization: {quantization_query}&#34;)
            quantization_score = compare_two_strings(quantization_query, model_quantization)
            if quantization_score &gt; treshold:
                scoring_models_dict[id][&#34;score&#34;] += quantization_score
            #print(f&#34;Model {model_name} {model_quantization} score: {quantization_score} treshold: {treshold}&#34;)
        if keywords_query is not None:
            #print(f&#34;Searching for keyword: {keywords_query}&#34;)
            best_keyword_score = 0
            for keyword in model_keywords:
                keyword_score = compare_two_strings(keywords_query, keyword)
                if keyword_score &gt; best_keyword_score:
                    best_keyword_score = keyword_score
            if best_keyword_score &gt; treshold:
                scoring_models_dict[id][&#34;score&#34;] += best_keyword_score
            #print(f&#34;Model {model_name} {model_quantization} score: {best_keyword_score} treshold: {treshold}&#34;)
        #print(f&#34;Model {model_name} {model_quantization} score: {scoring_models_dict[id][&#39;score&#39;]}&#34;)
    sorted_models = sorted(scoring_models_dict.items(), key=lambda x: x[1][&#34;score&#34;], reverse=True)
    #keep just the list of model data
    sorted_models = [x[1][&#34;model&#34;] for x in sorted_models]
    #print(f&#34;Found {len(sorted_models)} models.&#34;)
    #print(sorted_models)
    return sorted_models</code></pre>
</details>
</dd>
<dt id="glai.ModelDB.get_model_by_url"><code class="name flex">
<span>def <span class="ident">get_model_by_url</span></span>(<span>self, url: str) ‑> Optional[<a title="glai.back_end.model_db.model_data.ModelData" href="back_end/model_db/model_data.html#glai.back_end.model_db.model_data.ModelData">ModelData</a>]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_model_by_url(self, url:str) -&gt; Optional[ModelData]:
    for model in self.models:
        model:ModelData = model
        if model.gguf_url == url:
            return model
    return None    </code></pre>
</details>
</dd>
<dt id="glai.ModelDB.import_models_from_repo"><code class="name flex">
<span>def <span class="ident">import_models_from_repo</span></span>(<span>self, hf_repo_url: str, user_tags: Optional[list[str]] = None, ai_tags: Optional[list[str]] = None, keywords: Optional[list[str]] = None, description: Optional[str] = None, replace_existing: bool = False)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def import_models_from_repo(self, hf_repo_url:str,
                    user_tags:Optional[list[str]]=None,
                    ai_tags:Optional[list[str]]=None,
                    keywords:Optional[list[str]]=None, 
                    description:Optional[str]=None,
                    replace_existing:bool=False,
                    ):  
    #create model data from hf repo
    model_links = ModelDB._model_links_from_repo(hf_repo_url)
    for model_link in model_links:
        model_data = ModelData(gguf_url=model_link, db_dir=self.gguf_db_dir, user_tags=user_tags, ai_tags=ai_tags, description=description, keywords=keywords)
        model_data.save_json(replace_existing=replace_existing)
    self.load_models()</code></pre>
</details>
</dd>
<dt id="glai.ModelDB.list_available_models"><code class="name flex">
<span>def <span class="ident">list_available_models</span></span>(<span>self) ‑> list[str]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_available_models(self) -&gt; list[str]:
    print(f&#34;Available models in {self.gguf_db_dir}:&#34;)
    models = []
    for model in self.models:
        model:ModelData = model
        if model.name not in models:
            models.append(model.name)
    return models</code></pre>
</details>
</dd>
<dt id="glai.ModelDB.list_models_quantizations"><code class="name flex">
<span>def <span class="ident">list_models_quantizations</span></span>(<span>self, model_name: str) ‑> list[str]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def list_models_quantizations(self, model_name:str) -&gt; list[str]:
    quantizations = []
    for model in self.models:
        model:ModelData = model
        if model.name == model_name:
            quantizations.append(model.model_quantization)
    return quantizations</code></pre>
</details>
</dd>
<dt id="glai.ModelDB.load_models"><code class="name flex">
<span>def <span class="ident">load_models</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_models(self) -&gt; None:
    self.models = []
    files = list_files_in_dir(self.gguf_db_dir, show_directories=False, show_files=True, only_with_extensions=[&#34;.json&#34;])
    for file in files:
        try:
            file_path = join_paths(self.gguf_db_dir, file)
            model_data = ModelData.from_json(file_path)
            self.models.append(model_data)
        except Exception as e:
            print(f&#34;Error trying to load from {file_path}: \t\n{e}, \nskipping...&#34;)
            continue
    print(f&#34;Loaded {len(self.models)} models from {self.gguf_db_dir}.&#34;)</code></pre>
</details>
</dd>
<dt id="glai.ModelDB.load_models_data_from_repo"><code class="name flex">
<span>def <span class="ident">load_models_data_from_repo</span></span>(<span>self, hf_repo_url: str, user_tags: Optional[list[str]] = None, ai_tags: Optional[list[str]] = None, keywords: Optional[list[str]] = None, description: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_models_data_from_repo(self, hf_repo_url:str, 
                    user_tags:Optional[list[str]]=None,
                    ai_tags:Optional[list[str]]=None,
                    keywords:Optional[list[str]]=None, 
                    description:Optional[str]=None):  
    #create model data from hf repo
    model_links = ModelDB._model_links_from_repo(hf_repo_url)
    model_datas = []
    for model_link in model_links:
        model_data = ModelData(gguf_url=model_link, db_dir=self.gguf_db_dir, user_tags=user_tags, ai_tags=ai_tags, description=description, keywords=keywords)
        model_datas.append(model_data)
        model_data.save_json()
    self.models.extend(model_datas)
    return model_datas</code></pre>
</details>
</dd>
<dt id="glai.ModelDB.save_all_models"><code class="name flex">
<span>def <span class="ident">save_all_models</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_all_models(self) -&gt; None:
    for model in self.models:
        model:ModelData = model
        model.save_json()</code></pre>
</details>
</dd>
<dt id="glai.ModelDB.set_model_db_dir"><code class="name flex">
<span>def <span class="ident">set_model_db_dir</span></span>(<span>self, model_db_dir: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_model_db_dir(self, model_db_dir:str) -&gt; None:
    print(f&#34;ModelDB dir set to {model_db_dir}.&#34;)
    self.gguf_db_dir = create_dir(model_db_dir)</code></pre>
</details>
</dd>
<dt id="glai.ModelDB.show_db_info"><code class="name flex">
<span>def <span class="ident">show_db_info</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def show_db_info(self) -&gt; None:
    print(f&#34;ModelDB summary:&#34;)
    print(f&#34;ModelDB dir: {self.gguf_db_dir}&#34;)
    print(f&#34;Number of models: {len(self.models)}&#34;)
    print(f&#34;Available models:&#34;)
    models_info = {}
    for model in self.models:
        model:ModelData = model
        if model.name not in models_info.keys():
            models_info[model.name] = {}
            models_info[model.name][&#34;quantizations&#34;] = []
            models_info[model.name][&#34;description&#34;] = model.description
            models_info[model.name][&#34;keywords&#34;] = model.keywords
        if model.model_quantization not in models_info[model.name][&#34;quantizations&#34;]:
            models_info[model.name][&#34;quantizations&#34;].append(model.model_quantization)
    
    for model_name, models_info in models_info.items():
        print(f&#34;\t{model_name}:&#34;)
        print(f&#34;\t\tQuantizations: {models_info[&#39;quantizations&#39;]}&#34;)
        print(f&#34;\t\tKeywords: {models_info[&#39;keywords&#39;]}&#34;)
        print(f&#34;\t\tDescription: {models_info[&#39;description&#39;]}&#34;)
        print(f&#34;\t-------------------------------&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="glai.ModelData"><code class="flex name class">
<span>class <span class="ident">ModelData</span></span>
<span>(</span><span>gguf_url: str, db_dir: str, user_tags: Union[dict, list, set] = ('', ''), ai_tags: Union[dict, list, set] = ('', ''), description: Optional[str] = None, keywords: Optional[None] = None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ModelData:
    def __init__(self, 
        gguf_url:str,
        db_dir:str,
        user_tags:Union[dict, list, set] = (&#34;&#34;, &#34;&#34;), 
        ai_tags:Union[dict, list, set] = (&#34;&#34;, &#34;&#34;),
        description:Optional[str] = None, 
        keywords:Optional[list] = None,
        ):

        #init all as None
        self.gguf_url = None
        self.gguf_file_path = None
        self.name = None
        self.model_quantization = None
        self.description = None
        self.keywords = None
        self.user_tags = None
        self.ai_tags = None
        self.save_dir = None

        #set values
        self.gguf_url = gguf_url
        self.set_save_dir(db_dir)
        self.gguf_file_path = self._url_to_file_path(db_dir, gguf_url) 
        self.name = self._url_extract_model_name(gguf_url)
        self.model_quantization = self._url_extract_quantization(gguf_url)
        self.description = description if description is not None else &#34;&#34;
        self.keywords = keywords if keywords is not None else []
        self.set_tags(ai_tags, user_tags)

    def __str__(self) -&gt; str:
        t = f&#34;&#34;&#34;ModelData(
            ---required---
            gguf_url: {self.gguf_url},
            ---required with defaults--- 
            save_dir: {self.save_dir},
            user_tags: {self.user_tags},
            ai_tags: {self.ai_tags},
            ---optionally provided, no defaults---
            description: {self.description},
            keywords: {self.keywords},
            ---automatically generated---
            gguf_file_path: {self.gguf_file_path},
            model_name: {self.name},
            model_quantization: {self.model_quantization}
        )&#34;&#34;&#34;
        return t
    
    def __repr__(self) -&gt; str:
        return self.__str__()
    
    def __dict__(self) -&gt; dict:
        return self.to_dict()
    
    @staticmethod
    def _hf_url_to_download_url(url) -&gt; str:
        #to download replace blob with resolve and add download=true
        if not &#34;huggingface.co&#34; in url:
            raise ValueError(f&#34;Invalid url: {url}, must be a huggingface.co url, other sources aren&#39;t implemented yet.&#34;)
        url = url.replace(&#34;blob&#34;, &#34;resolve&#34;)
        if url.endswith(&#34;/&#34;):
            url = url[:-1]
        if not url.endswith(&#34;?download=true&#34;):
            url = url + &#34;?download=true&#34;
        return url
    
    @staticmethod    
    def _url_to_file_path(save_dir:str, url:str)-&gt;str:
        #create_dirs_for(save_dir)
        file_path = join_paths(save_dir, ModelData._url_extract_file_name(url))
        return file_path 
    
    @staticmethod
    def _url_extract_file_name(url:str) -&gt; str:
        f_name =  url.split(&#34;/&#34;)[-1]
        if is_file_format(f_name, &#34;.gguf&#34;):
            return f_name
        else:
            raise ValueError(f&#34;File {f_name} is not a gguf file.&#34;)
    
    @staticmethod
    def _url_extract_quantization(url:str) -&gt; str:
        quantization = ModelData._url_extract_file_name(url).split(&#34;.&#34;)[-2]
        return quantization
    
    @staticmethod
    def _url_extract_model_name(url:str) -&gt; str:
        model_name = ModelData._url_extract_file_name(url).split(&#34;.&#34;)[0:-2]
        return &#34;.&#34;.join(model_name)

    def set_ai_tags(self, ai_tags:Union[dict, set[str], list[str], tuple[str]]) -&gt; None:
        if isinstance(ai_tags, dict):
            if &#34;open&#34; in ai_tags and &#34;close&#34; in ai_tags:
                self.ai_tags = ai_tags
            else:
                raise ValueError(f&#34;Invalid user tags: {ai_tags}, for dict tags both &#39;open&#39; and &#39;close&#39; keys must be present.&#34;)
        elif isinstance(ai_tags, set) or isinstance(ai_tags, list) or isinstance(ai_tags, tuple):
            self.ai_tags = {
                &#34;open&#34;: ai_tags[0],
                &#34;close&#34;: ai_tags[1]
            }
        else:
            raise TypeError(f&#34;Invalid type for user tags: {type(ai_tags)}, must be dict, set or list.&#34;)
        
    def set_user_tags(self, user_tags:Union[dict, set[str], list[str], tuple[str]]) -&gt; None:
        if isinstance(user_tags, dict):
            if &#34;open&#34; in user_tags and &#34;close&#34; in user_tags:
                self.user_tags = user_tags
            else:
                raise ValueError(f&#34;Invalid user tags: {user_tags}, for dict tags both &#39;open&#39; and &#39;close&#39; keys must be present.&#34;)
        elif isinstance(user_tags, set) or isinstance(user_tags, list) or isinstance(user_tags, tuple):
            self.user_tags = {
                &#34;open&#34;: user_tags[0],
                &#34;close&#34;: user_tags[1]
            }
        else:
            raise TypeError(f&#34;Invalid type for user tags: {type(user_tags)}, must be dict, set or list.&#34;)

    def set_tags(self, 
                 ai_tags:Optional[Union[dict, set[str], list[str], tuple[str]]],
                 user_tags:Optional[Union[dict, set[str], list[str], tuple[str]]]) -&gt; None:
        if ai_tags is not None:
            self.set_ai_tags(ai_tags)
        if user_tags is not None:
            self.set_user_tags(user_tags)

    def set_save_dir(self, save_dir:str) -&gt; None:
        self.save_dir = save_dir
        self.gguf_file_path = self._url_to_file_path(save_dir, self.gguf_url)

    def to_dict(self):
        model_data = {
            &#34;url&#34;: self.gguf_url,
            &#34;gguf_file_path&#34;: self.gguf_file_path,
            &#34;model_name&#34;: self.name,
            &#34;model_quantization&#34;: self.model_quantization, 
            &#34;description&#34;: self.description,
            &#34;keywords&#34;: self.keywords,
            &#34;user_tags&#34;: self.user_tags,
            &#34;ai_tags&#34;: self.ai_tags,
            &#34;save_dir&#34;: self.save_dir,
        }
        return model_data
    @staticmethod
    def from_dict(model_data:dict) -&gt; &#34;ModelData&#34;:
        url = model_data[&#34;url&#34;]
        save_dir = model_data[&#34;save_dir&#34;]
        description = model_data[&#34;description&#34;] if &#34;description&#34; in model_data else None
        keywords = model_data[&#34;keywords&#34;] if &#34;keywords&#34; in model_data else None
        user_tags = model_data[&#34;user_tags&#34;]
        ai_tags = model_data[&#34;ai_tags&#34;]
        new_model_data = ModelData(url, save_dir, user_tags, ai_tags, description, keywords)
        return new_model_data

    def is_downloaded(self) -&gt; bool:
        &#34;&#34;&#34;Checks if the model file is downloaded.&#34;&#34;&#34;
        return does_file_exist(self.gguf_file_path)
    
    def has_json(self) -&gt; bool:
        &#34;&#34;&#34;Checks if the model is in the db.&#34;&#34;&#34;
        return does_file_exist(self.json_path())
    
    def download_gguf(self, force_redownload:bool=False) -&gt; str:
        print(f&#34;Preparing {self.gguf_file_path}\n for {self.name} : {self.model_quantization}...&#34;)
        if not does_file_exist(self.gguf_file_path) or force_redownload:
            print(f&#34;Downloading {self.name} : {self.model_quantization}...&#34;)
            gguf_download_url = self._hf_url_to_download_url(self.gguf_url)
            response = requests.get(gguf_download_url, stream=True)
            total_size = int(response.headers.get(&#39;content-length&#39;, 0))
            block_size = 1024000  # 100 KB
            progress_bar = f&#34;Please wait, downloading {self.name} : {self.model_quantization}: {{0:0.2f}}% | {{1:0.3f}}/{{2:0.3f}} GB) | {{3:0.3f}} MB/s&#34;
            unfinished_save_path = self.gguf_file_path + &#34;.unfinished&#34;
            with open(unfinished_save_path, &#34;wb&#34;) as f:
                downloaded_size = 0
                start_time = time.time()
                elapsed_time = 0
                downloaded_since_last = 0
                for data in response.iter_content(block_size):
                    downloaded_size += len(data)
                    downloaded_since_last += len(data)
                    f.write(data)
                    elapsed_time = time.time() - start_time
                    download_speed = (downloaded_since_last*10/(1024**3)) / elapsed_time if elapsed_time &gt; 0 else 0
                    progress = downloaded_size / total_size * 100
                    gb_downloaded = downloaded_size/(1024**3)
                    gb_total = total_size/(1024**3)
                    if elapsed_time &gt;= 1:
                        print(progress_bar.format(progress, gb_downloaded, gb_total, download_speed), end=&#39;\r&#39;)
                        downloaded_since_last = 0
                        start_time = time.time()
            print(progress_bar.format(100, gb_downloaded, gb_total, download_speed))
            rename_file(unfinished_save_path, self.gguf_file_path)
        else:
            print(f&#34;File {self.gguf_file_path} already exists. Skipping download.&#34;)
        return self.gguf_file_path
    
    def json_path(self) -&gt; str:
        return change_extension(self.gguf_file_path, &#34;.json&#34;)
    
    def save_json(self, replace_existing:bool=True) -&gt; str:
        if replace_existing or not self.has_json():
            save_json_file(self.json_path(), self.to_dict())
        else:
            print(f&#34;File {self.json_path()} already exists and replace_existing={replace_existing}. Skipping save.&#34;)
        return self.json_path()
    
    @staticmethod
    def from_json(json_file_path:str) -&gt; &#34;ModelData&#34;:
        model_data = load_json_file(json_file_path)
        return ModelData.from_dict(model_data)

    @staticmethod
    def from_url(url:str, save_dir:str, user_tags:Union[dict, list, set] = (&#34;[INST]&#34;, &#34;[/INST]&#34;), ai_tags:Union[dict, list, set] = (&#34;&#34;, &#34;&#34;), description:Optional[str] = None, keywords:Optional[list] = None) -&gt; &#34;ModelData&#34;:
        return ModelData(url, save_dir, user_tags, ai_tags, description, keywords)

    def model_path(self) -&gt; str:
        return self.gguf_file_path

    @staticmethod
    def from_file(gguf_file_path:str, save_dir:Optional[str]=None, user_tags:Union[dict, list, set] = (&#34;[INST]&#34;, &#34;[/INST]&#34;), ai_tags:Union[dict, list, set] = (&#34;&#34;, &#34;&#34;), description:Optional[str] = None, keywords:Optional[list] = None) -&gt; &#34;ModelData&#34;:
        #creates a model where url is also the file path
        save_dir = get_directory(gguf_file_path) if save_dir is None else save_dir
        url = gguf_file_path
        return ModelData(url, save_dir, user_tags, ai_tags, description, keywords)

    def get_ai_tag_open(self) -&gt; str:
        return self.ai_tags[&#34;open&#34;]
    
    def get_ai_tag_close(self) -&gt; str:
        return self.ai_tags[&#34;close&#34;]
    
    def get_user_tag_open(self) -&gt; str:
        return self.user_tags[&#34;open&#34;]
    
    def get_user_tag_close(self) -&gt; str:
        return self.user_tags[&#34;close&#34;]
    
    def get_ai_tags(self) -&gt; list[str]:
        return [self.get_ai_tag_open(), self.get_ai_tag_close()]
    
    def get_user_tags(self) -&gt; list[str]:
        return [self.get_user_tag_open(), self.get_user_tag_close()]</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="glai.ModelData.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>model_data: dict) ‑> <a title="glai.back_end.model_db.model_data.ModelData" href="back_end/model_db/model_data.html#glai.back_end.model_db.model_data.ModelData">ModelData</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_dict(model_data:dict) -&gt; &#34;ModelData&#34;:
    url = model_data[&#34;url&#34;]
    save_dir = model_data[&#34;save_dir&#34;]
    description = model_data[&#34;description&#34;] if &#34;description&#34; in model_data else None
    keywords = model_data[&#34;keywords&#34;] if &#34;keywords&#34; in model_data else None
    user_tags = model_data[&#34;user_tags&#34;]
    ai_tags = model_data[&#34;ai_tags&#34;]
    new_model_data = ModelData(url, save_dir, user_tags, ai_tags, description, keywords)
    return new_model_data</code></pre>
</details>
</dd>
<dt id="glai.ModelData.from_file"><code class="name flex">
<span>def <span class="ident">from_file</span></span>(<span>gguf_file_path: str, save_dir: Optional[str] = None, user_tags: Union[dict, list, set] = ('[INST]', '[/INST]'), ai_tags: Union[dict, list, set] = ('', ''), description: Optional[str] = None, keywords: Optional[None] = None) ‑> <a title="glai.back_end.model_db.model_data.ModelData" href="back_end/model_db/model_data.html#glai.back_end.model_db.model_data.ModelData">ModelData</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_file(gguf_file_path:str, save_dir:Optional[str]=None, user_tags:Union[dict, list, set] = (&#34;[INST]&#34;, &#34;[/INST]&#34;), ai_tags:Union[dict, list, set] = (&#34;&#34;, &#34;&#34;), description:Optional[str] = None, keywords:Optional[list] = None) -&gt; &#34;ModelData&#34;:
    #creates a model where url is also the file path
    save_dir = get_directory(gguf_file_path) if save_dir is None else save_dir
    url = gguf_file_path
    return ModelData(url, save_dir, user_tags, ai_tags, description, keywords)</code></pre>
</details>
</dd>
<dt id="glai.ModelData.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>json_file_path: str) ‑> <a title="glai.back_end.model_db.model_data.ModelData" href="back_end/model_db/model_data.html#glai.back_end.model_db.model_data.ModelData">ModelData</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(json_file_path:str) -&gt; &#34;ModelData&#34;:
    model_data = load_json_file(json_file_path)
    return ModelData.from_dict(model_data)</code></pre>
</details>
</dd>
<dt id="glai.ModelData.from_url"><code class="name flex">
<span>def <span class="ident">from_url</span></span>(<span>url: str, save_dir: str, user_tags: Union[dict, list, set] = ('[INST]', '[/INST]'), ai_tags: Union[dict, list, set] = ('', ''), description: Optional[str] = None, keywords: Optional[None] = None) ‑> <a title="glai.back_end.model_db.model_data.ModelData" href="back_end/model_db/model_data.html#glai.back_end.model_db.model_data.ModelData">ModelData</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_url(url:str, save_dir:str, user_tags:Union[dict, list, set] = (&#34;[INST]&#34;, &#34;[/INST]&#34;), ai_tags:Union[dict, list, set] = (&#34;&#34;, &#34;&#34;), description:Optional[str] = None, keywords:Optional[list] = None) -&gt; &#34;ModelData&#34;:
    return ModelData(url, save_dir, user_tags, ai_tags, description, keywords)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="glai.ModelData.download_gguf"><code class="name flex">
<span>def <span class="ident">download_gguf</span></span>(<span>self, force_redownload: bool = False) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def download_gguf(self, force_redownload:bool=False) -&gt; str:
    print(f&#34;Preparing {self.gguf_file_path}\n for {self.name} : {self.model_quantization}...&#34;)
    if not does_file_exist(self.gguf_file_path) or force_redownload:
        print(f&#34;Downloading {self.name} : {self.model_quantization}...&#34;)
        gguf_download_url = self._hf_url_to_download_url(self.gguf_url)
        response = requests.get(gguf_download_url, stream=True)
        total_size = int(response.headers.get(&#39;content-length&#39;, 0))
        block_size = 1024000  # 100 KB
        progress_bar = f&#34;Please wait, downloading {self.name} : {self.model_quantization}: {{0:0.2f}}% | {{1:0.3f}}/{{2:0.3f}} GB) | {{3:0.3f}} MB/s&#34;
        unfinished_save_path = self.gguf_file_path + &#34;.unfinished&#34;
        with open(unfinished_save_path, &#34;wb&#34;) as f:
            downloaded_size = 0
            start_time = time.time()
            elapsed_time = 0
            downloaded_since_last = 0
            for data in response.iter_content(block_size):
                downloaded_size += len(data)
                downloaded_since_last += len(data)
                f.write(data)
                elapsed_time = time.time() - start_time
                download_speed = (downloaded_since_last*10/(1024**3)) / elapsed_time if elapsed_time &gt; 0 else 0
                progress = downloaded_size / total_size * 100
                gb_downloaded = downloaded_size/(1024**3)
                gb_total = total_size/(1024**3)
                if elapsed_time &gt;= 1:
                    print(progress_bar.format(progress, gb_downloaded, gb_total, download_speed), end=&#39;\r&#39;)
                    downloaded_since_last = 0
                    start_time = time.time()
        print(progress_bar.format(100, gb_downloaded, gb_total, download_speed))
        rename_file(unfinished_save_path, self.gguf_file_path)
    else:
        print(f&#34;File {self.gguf_file_path} already exists. Skipping download.&#34;)
    return self.gguf_file_path</code></pre>
</details>
</dd>
<dt id="glai.ModelData.get_ai_tag_close"><code class="name flex">
<span>def <span class="ident">get_ai_tag_close</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_ai_tag_close(self) -&gt; str:
    return self.ai_tags[&#34;close&#34;]</code></pre>
</details>
</dd>
<dt id="glai.ModelData.get_ai_tag_open"><code class="name flex">
<span>def <span class="ident">get_ai_tag_open</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_ai_tag_open(self) -&gt; str:
    return self.ai_tags[&#34;open&#34;]</code></pre>
</details>
</dd>
<dt id="glai.ModelData.get_ai_tags"><code class="name flex">
<span>def <span class="ident">get_ai_tags</span></span>(<span>self) ‑> list[str]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_ai_tags(self) -&gt; list[str]:
    return [self.get_ai_tag_open(), self.get_ai_tag_close()]</code></pre>
</details>
</dd>
<dt id="glai.ModelData.get_user_tag_close"><code class="name flex">
<span>def <span class="ident">get_user_tag_close</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_user_tag_close(self) -&gt; str:
    return self.user_tags[&#34;close&#34;]</code></pre>
</details>
</dd>
<dt id="glai.ModelData.get_user_tag_open"><code class="name flex">
<span>def <span class="ident">get_user_tag_open</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_user_tag_open(self) -&gt; str:
    return self.user_tags[&#34;open&#34;]</code></pre>
</details>
</dd>
<dt id="glai.ModelData.get_user_tags"><code class="name flex">
<span>def <span class="ident">get_user_tags</span></span>(<span>self) ‑> list[str]</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_user_tags(self) -&gt; list[str]:
    return [self.get_user_tag_open(), self.get_user_tag_close()]</code></pre>
</details>
</dd>
<dt id="glai.ModelData.has_json"><code class="name flex">
<span>def <span class="ident">has_json</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if the model is in the db.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def has_json(self) -&gt; bool:
    &#34;&#34;&#34;Checks if the model is in the db.&#34;&#34;&#34;
    return does_file_exist(self.json_path())</code></pre>
</details>
</dd>
<dt id="glai.ModelData.is_downloaded"><code class="name flex">
<span>def <span class="ident">is_downloaded</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Checks if the model file is downloaded.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_downloaded(self) -&gt; bool:
    &#34;&#34;&#34;Checks if the model file is downloaded.&#34;&#34;&#34;
    return does_file_exist(self.gguf_file_path)</code></pre>
</details>
</dd>
<dt id="glai.ModelData.json_path"><code class="name flex">
<span>def <span class="ident">json_path</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def json_path(self) -&gt; str:
    return change_extension(self.gguf_file_path, &#34;.json&#34;)</code></pre>
</details>
</dd>
<dt id="glai.ModelData.model_path"><code class="name flex">
<span>def <span class="ident">model_path</span></span>(<span>self) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_path(self) -&gt; str:
    return self.gguf_file_path</code></pre>
</details>
</dd>
<dt id="glai.ModelData.save_json"><code class="name flex">
<span>def <span class="ident">save_json</span></span>(<span>self, replace_existing: bool = True) ‑> str</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_json(self, replace_existing:bool=True) -&gt; str:
    if replace_existing or not self.has_json():
        save_json_file(self.json_path(), self.to_dict())
    else:
        print(f&#34;File {self.json_path()} already exists and replace_existing={replace_existing}. Skipping save.&#34;)
    return self.json_path()</code></pre>
</details>
</dd>
<dt id="glai.ModelData.set_ai_tags"><code class="name flex">
<span>def <span class="ident">set_ai_tags</span></span>(<span>self, ai_tags: Union[dict, set[str], list[str], tuple[str]]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_ai_tags(self, ai_tags:Union[dict, set[str], list[str], tuple[str]]) -&gt; None:
    if isinstance(ai_tags, dict):
        if &#34;open&#34; in ai_tags and &#34;close&#34; in ai_tags:
            self.ai_tags = ai_tags
        else:
            raise ValueError(f&#34;Invalid user tags: {ai_tags}, for dict tags both &#39;open&#39; and &#39;close&#39; keys must be present.&#34;)
    elif isinstance(ai_tags, set) or isinstance(ai_tags, list) or isinstance(ai_tags, tuple):
        self.ai_tags = {
            &#34;open&#34;: ai_tags[0],
            &#34;close&#34;: ai_tags[1]
        }
    else:
        raise TypeError(f&#34;Invalid type for user tags: {type(ai_tags)}, must be dict, set or list.&#34;)</code></pre>
</details>
</dd>
<dt id="glai.ModelData.set_save_dir"><code class="name flex">
<span>def <span class="ident">set_save_dir</span></span>(<span>self, save_dir: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_save_dir(self, save_dir:str) -&gt; None:
    self.save_dir = save_dir
    self.gguf_file_path = self._url_to_file_path(save_dir, self.gguf_url)</code></pre>
</details>
</dd>
<dt id="glai.ModelData.set_tags"><code class="name flex">
<span>def <span class="ident">set_tags</span></span>(<span>self, ai_tags: Union[dict, set[str], list[str], tuple[str], ForwardRef(None)], user_tags: Union[dict, set[str], list[str], tuple[str], ForwardRef(None)]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_tags(self, 
             ai_tags:Optional[Union[dict, set[str], list[str], tuple[str]]],
             user_tags:Optional[Union[dict, set[str], list[str], tuple[str]]]) -&gt; None:
    if ai_tags is not None:
        self.set_ai_tags(ai_tags)
    if user_tags is not None:
        self.set_user_tags(user_tags)</code></pre>
</details>
</dd>
<dt id="glai.ModelData.set_user_tags"><code class="name flex">
<span>def <span class="ident">set_user_tags</span></span>(<span>self, user_tags: Union[dict, set[str], list[str], tuple[str]]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_user_tags(self, user_tags:Union[dict, set[str], list[str], tuple[str]]) -&gt; None:
    if isinstance(user_tags, dict):
        if &#34;open&#34; in user_tags and &#34;close&#34; in user_tags:
            self.user_tags = user_tags
        else:
            raise ValueError(f&#34;Invalid user tags: {user_tags}, for dict tags both &#39;open&#39; and &#39;close&#39; keys must be present.&#34;)
    elif isinstance(user_tags, set) or isinstance(user_tags, list) or isinstance(user_tags, tuple):
        self.user_tags = {
            &#34;open&#34;: user_tags[0],
            &#34;close&#34;: user_tags[1]
        }
    else:
        raise TypeError(f&#34;Invalid type for user tags: {type(user_tags)}, must be dict, set or list.&#34;)</code></pre>
</details>
</dd>
<dt id="glai.ModelData.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self):
    model_data = {
        &#34;url&#34;: self.gguf_url,
        &#34;gguf_file_path&#34;: self.gguf_file_path,
        &#34;model_name&#34;: self.name,
        &#34;model_quantization&#34;: self.model_quantization, 
        &#34;description&#34;: self.description,
        &#34;keywords&#34;: self.keywords,
        &#34;user_tags&#34;: self.user_tags,
        &#34;ai_tags&#34;: self.ai_tags,
        &#34;save_dir&#34;: self.save_dir,
    }
    return model_data</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="glai.ai" href="ai/index.html">glai.ai</a></code></li>
<li><code><a title="glai.back_end" href="back_end/index.html">glai.back_end</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="glai.AIMessages" href="#glai.AIMessages">AIMessages</a></code></h4>
<ul class="two-column">
<li><code><a title="glai.AIMessages.add_ai_message" href="#glai.AIMessages.add_ai_message">add_ai_message</a></code></li>
<li><code><a title="glai.AIMessages.add_message" href="#glai.AIMessages.add_message">add_message</a></code></li>
<li><code><a title="glai.AIMessages.add_user_message" href="#glai.AIMessages.add_user_message">add_user_message</a></code></li>
<li><code><a title="glai.AIMessages.ai_tags" href="#glai.AIMessages.ai_tags">ai_tags</a></code></li>
<li><code><a title="glai.AIMessages.edit_last_message" href="#glai.AIMessages.edit_last_message">edit_last_message</a></code></li>
<li><code><a title="glai.AIMessages.edit_message" href="#glai.AIMessages.edit_message">edit_message</a></code></li>
<li><code><a title="glai.AIMessages.from_dict" href="#glai.AIMessages.from_dict">from_dict</a></code></li>
<li><code><a title="glai.AIMessages.from_json" href="#glai.AIMessages.from_json">from_json</a></code></li>
<li><code><a title="glai.AIMessages.get_last_message" href="#glai.AIMessages.get_last_message">get_last_message</a></code></li>
<li><code><a title="glai.AIMessages.load_messages" href="#glai.AIMessages.load_messages">load_messages</a></code></li>
<li><code><a title="glai.AIMessages.reset_messages" href="#glai.AIMessages.reset_messages">reset_messages</a></code></li>
<li><code><a title="glai.AIMessages.save_json" href="#glai.AIMessages.save_json">save_json</a></code></li>
<li><code><a title="glai.AIMessages.text" href="#glai.AIMessages.text">text</a></code></li>
<li><code><a title="glai.AIMessages.to_dict" href="#glai.AIMessages.to_dict">to_dict</a></code></li>
<li><code><a title="glai.AIMessages.user_tags" href="#glai.AIMessages.user_tags">user_tags</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="glai.AutoAI" href="#glai.AutoAI">AutoAI</a></code></h4>
<ul class="">
<li><code><a title="glai.AutoAI.count_tokens" href="#glai.AutoAI.count_tokens">count_tokens</a></code></li>
<li><code><a title="glai.AutoAI.generate" href="#glai.AutoAI.generate">generate</a></code></li>
<li><code><a title="glai.AutoAI.generate_from_prompt" href="#glai.AutoAI.generate_from_prompt">generate_from_prompt</a></code></li>
<li><code><a title="glai.AutoAI.is_within_input_limit" href="#glai.AutoAI.is_within_input_limit">is_within_input_limit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="glai.EasyAI" href="#glai.EasyAI">EasyAI</a></code></h4>
<ul class="">
<li><code><a title="glai.EasyAI.configure" href="#glai.EasyAI.configure">configure</a></code></li>
<li><code><a title="glai.EasyAI.count_tokens" href="#glai.EasyAI.count_tokens">count_tokens</a></code></li>
<li><code><a title="glai.EasyAI.find_model_data" href="#glai.EasyAI.find_model_data">find_model_data</a></code></li>
<li><code><a title="glai.EasyAI.generate" href="#glai.EasyAI.generate">generate</a></code></li>
<li><code><a title="glai.EasyAI.is_within_input_limit" href="#glai.EasyAI.is_within_input_limit">is_within_input_limit</a></code></li>
<li><code><a title="glai.EasyAI.load_ai" href="#glai.EasyAI.load_ai">load_ai</a></code></li>
<li><code><a title="glai.EasyAI.load_model_db" href="#glai.EasyAI.load_model_db">load_model_db</a></code></li>
<li><code><a title="glai.EasyAI.model_data_from_file" href="#glai.EasyAI.model_data_from_file">model_data_from_file</a></code></li>
<li><code><a title="glai.EasyAI.model_data_from_url" href="#glai.EasyAI.model_data_from_url">model_data_from_url</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="glai.ModelDB" href="#glai.ModelDB">ModelDB</a></code></h4>
<ul class="">
<li><code><a title="glai.ModelDB.add_model_by_json" href="#glai.ModelDB.add_model_by_json">add_model_by_json</a></code></li>
<li><code><a title="glai.ModelDB.add_model_by_url" href="#glai.ModelDB.add_model_by_url">add_model_by_url</a></code></li>
<li><code><a title="glai.ModelDB.add_model_data" href="#glai.ModelDB.add_model_data">add_model_data</a></code></li>
<li><code><a title="glai.ModelDB.find_model" href="#glai.ModelDB.find_model">find_model</a></code></li>
<li><code><a title="glai.ModelDB.find_models" href="#glai.ModelDB.find_models">find_models</a></code></li>
<li><code><a title="glai.ModelDB.get_model_by_url" href="#glai.ModelDB.get_model_by_url">get_model_by_url</a></code></li>
<li><code><a title="glai.ModelDB.import_models_from_repo" href="#glai.ModelDB.import_models_from_repo">import_models_from_repo</a></code></li>
<li><code><a title="glai.ModelDB.list_available_models" href="#glai.ModelDB.list_available_models">list_available_models</a></code></li>
<li><code><a title="glai.ModelDB.list_models_quantizations" href="#glai.ModelDB.list_models_quantizations">list_models_quantizations</a></code></li>
<li><code><a title="glai.ModelDB.load_models" href="#glai.ModelDB.load_models">load_models</a></code></li>
<li><code><a title="glai.ModelDB.load_models_data_from_repo" href="#glai.ModelDB.load_models_data_from_repo">load_models_data_from_repo</a></code></li>
<li><code><a title="glai.ModelDB.save_all_models" href="#glai.ModelDB.save_all_models">save_all_models</a></code></li>
<li><code><a title="glai.ModelDB.set_model_db_dir" href="#glai.ModelDB.set_model_db_dir">set_model_db_dir</a></code></li>
<li><code><a title="glai.ModelDB.show_db_info" href="#glai.ModelDB.show_db_info">show_db_info</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="glai.ModelData" href="#glai.ModelData">ModelData</a></code></h4>
<ul class="two-column">
<li><code><a title="glai.ModelData.download_gguf" href="#glai.ModelData.download_gguf">download_gguf</a></code></li>
<li><code><a title="glai.ModelData.from_dict" href="#glai.ModelData.from_dict">from_dict</a></code></li>
<li><code><a title="glai.ModelData.from_file" href="#glai.ModelData.from_file">from_file</a></code></li>
<li><code><a title="glai.ModelData.from_json" href="#glai.ModelData.from_json">from_json</a></code></li>
<li><code><a title="glai.ModelData.from_url" href="#glai.ModelData.from_url">from_url</a></code></li>
<li><code><a title="glai.ModelData.get_ai_tag_close" href="#glai.ModelData.get_ai_tag_close">get_ai_tag_close</a></code></li>
<li><code><a title="glai.ModelData.get_ai_tag_open" href="#glai.ModelData.get_ai_tag_open">get_ai_tag_open</a></code></li>
<li><code><a title="glai.ModelData.get_ai_tags" href="#glai.ModelData.get_ai_tags">get_ai_tags</a></code></li>
<li><code><a title="glai.ModelData.get_user_tag_close" href="#glai.ModelData.get_user_tag_close">get_user_tag_close</a></code></li>
<li><code><a title="glai.ModelData.get_user_tag_open" href="#glai.ModelData.get_user_tag_open">get_user_tag_open</a></code></li>
<li><code><a title="glai.ModelData.get_user_tags" href="#glai.ModelData.get_user_tags">get_user_tags</a></code></li>
<li><code><a title="glai.ModelData.has_json" href="#glai.ModelData.has_json">has_json</a></code></li>
<li><code><a title="glai.ModelData.is_downloaded" href="#glai.ModelData.is_downloaded">is_downloaded</a></code></li>
<li><code><a title="glai.ModelData.json_path" href="#glai.ModelData.json_path">json_path</a></code></li>
<li><code><a title="glai.ModelData.model_path" href="#glai.ModelData.model_path">model_path</a></code></li>
<li><code><a title="glai.ModelData.save_json" href="#glai.ModelData.save_json">save_json</a></code></li>
<li><code><a title="glai.ModelData.set_ai_tags" href="#glai.ModelData.set_ai_tags">set_ai_tags</a></code></li>
<li><code><a title="glai.ModelData.set_save_dir" href="#glai.ModelData.set_save_dir">set_save_dir</a></code></li>
<li><code><a title="glai.ModelData.set_tags" href="#glai.ModelData.set_tags">set_tags</a></code></li>
<li><code><a title="glai.ModelData.set_user_tags" href="#glai.ModelData.set_user_tags">set_user_tags</a></code></li>
<li><code><a title="glai.ModelData.to_dict" href="#glai.ModelData.to_dict">to_dict</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>