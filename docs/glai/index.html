<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>glai API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Package <code>glai</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from .ai import AutoAI, EasyAI
from .messages import AIMessages, AIMessage

__all__ = [&#39;AutoAI&#39;, &#39;EasyAI&#39;, &#39;AIMessages&#39;, &#39;AIMessage&#39;]
# print(f&#34;&#34;&#34;
# glai
# GGUF LLAMA AI - Package for simplified text generation with Llama models quantized to GGUF format is loaded.
# Provides high level APIs for loading models and generating text completions.
# For more information please check README.md file or visit https://github.com/laelhalawani/glai 
# Detailed API documentation can be found here: https://laelhalawani.github.io/glai/
# &#34;&#34;&#34;)</code></pre>
</details>
</section>
<section>
<h2 class="section-title" id="header-submodules">Sub-modules</h2>
<dl>
<dt><code class="name"><a title="glai.ai" href="ai/index.html">glai.ai</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt><code class="name"><a title="glai.messages" href="messages/index.html">glai.messages</a></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="glai.AIMessage"><code class="flex name class">
<span>class <span class="ident">AIMessage</span></span>
<span>(</span><span>content: str, tag_open: str, tag_close: str)</span>
</code></dt>
<dd>
<div class="desc"><p>Represents a message in an AI system.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>content</code></strong> :&ensp;<code>str</code></dt>
<dd>The content of the message.</dd>
<dt><strong><code>tag_open</code></strong> :&ensp;<code>str</code></dt>
<dd>The opening tag for the message.</dd>
<dt><strong><code>tag_close</code></strong> :&ensp;<code>str</code></dt>
<dd>The closing tag for the message.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AIMessage:
    &#34;&#34;&#34;
    Represents a message in an AI system.

    Attributes:
        content (str): The content of the message.
        tag_open (str): The opening tag for the message.
        tag_close (str): The closing tag for the message.
    &#34;&#34;&#34;

    def __init__(self, content:str, tag_open:str, tag_close:str):
        self.content = content
        self.tag_open = tag_open
        self.tag_close = tag_close

    def __str__(self) -&gt; str:
        return f&#34;{self.tag_open}{self.content}{self.tag_close}&#34;
    
    def __repr__(self) -&gt; str:
        return self.__str__()
    
    def edit(self, new_content, new_tag_open=None, new_tag_close=None):
        &#34;&#34;&#34;
        Edits the content of the message.

        Args:
            new_content (str): The new content for the message.
        &#34;&#34;&#34;
        self.content = new_content
        if new_tag_open is not None:
            self.tag_open = new_tag_open
        if new_tag_close is not None:
            self.tag_close = new_tag_close
    
    def text(self):
        &#34;&#34;&#34;
        Returns the text representation of the message.

        Returns:
            str: The text representation of the message.
        &#34;&#34;&#34;
        return self.__str__()
    
    def get_tags(self) -&gt; tuple:
        &#34;&#34;&#34;
        Returns the tags of the message.

        Returns:
            tuple: The tags of the message.
        &#34;&#34;&#34;
        return (self.tag_open, self.tag_close)
    
    def to_dict(self) -&gt; dict:
        &#34;&#34;&#34;
        Returns the message as a dictionary.

        Returns:
            dict: The message as a dictionary.
        &#34;&#34;&#34;
        return {
            &#34;content&#34;: self.content,
            &#34;tag_open&#34;: self.tag_open,
            &#34;tag_close&#34;: self.tag_close,
        }
    
    @staticmethod 
    def from_dict(message_dict:dict) -&gt; &#34;AIMessage&#34;:
        &#34;&#34;&#34;
        Creates a new AIMessage from a dictionary.

        Args:
            message_dict (dict): The dictionary to create the AIMessage from.

        Returns:
            AIMessage: The created AIMessage.
        &#34;&#34;&#34;
        return AIMessage(message_dict[&#34;content&#34;], message_dict[&#34;tag_open&#34;], message_dict[&#34;tag_close&#34;])</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="glai.AIMessage.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>message_dict: dict) ‑> <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Creates a new AIMessage from a dictionary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>message_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>The dictionary to create the AIMessage from.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="glai.AIMessage" href="#glai.AIMessage">AIMessage</a></code></dt>
<dd>The created AIMessage.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod 
def from_dict(message_dict:dict) -&gt; &#34;AIMessage&#34;:
    &#34;&#34;&#34;
    Creates a new AIMessage from a dictionary.

    Args:
        message_dict (dict): The dictionary to create the AIMessage from.

    Returns:
        AIMessage: The created AIMessage.
    &#34;&#34;&#34;
    return AIMessage(message_dict[&#34;content&#34;], message_dict[&#34;tag_open&#34;], message_dict[&#34;tag_close&#34;])</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="glai.AIMessage.edit"><code class="name flex">
<span>def <span class="ident">edit</span></span>(<span>self, new_content, new_tag_open=None, new_tag_close=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Edits the content of the message.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>new_content</code></strong> :&ensp;<code>str</code></dt>
<dd>The new content for the message.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit(self, new_content, new_tag_open=None, new_tag_close=None):
    &#34;&#34;&#34;
    Edits the content of the message.

    Args:
        new_content (str): The new content for the message.
    &#34;&#34;&#34;
    self.content = new_content
    if new_tag_open is not None:
        self.tag_open = new_tag_open
    if new_tag_close is not None:
        self.tag_close = new_tag_close</code></pre>
</details>
</dd>
<dt id="glai.AIMessage.get_tags"><code class="name flex">
<span>def <span class="ident">get_tags</span></span>(<span>self) ‑> tuple</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the tags of the message.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple</code></dt>
<dd>The tags of the message.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_tags(self) -&gt; tuple:
    &#34;&#34;&#34;
    Returns the tags of the message.

    Returns:
        tuple: The tags of the message.
    &#34;&#34;&#34;
    return (self.tag_open, self.tag_close)</code></pre>
</details>
</dd>
<dt id="glai.AIMessage.text"><code class="name flex">
<span>def <span class="ident">text</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the text representation of the message.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>str</code></dt>
<dd>The text representation of the message.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def text(self):
    &#34;&#34;&#34;
    Returns the text representation of the message.

    Returns:
        str: The text representation of the message.
    &#34;&#34;&#34;
    return self.__str__()</code></pre>
</details>
</dd>
<dt id="glai.AIMessage.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the message as a dictionary.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>The message as a dictionary.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict:
    &#34;&#34;&#34;
    Returns the message as a dictionary.

    Returns:
        dict: The message as a dictionary.
    &#34;&#34;&#34;
    return {
        &#34;content&#34;: self.content,
        &#34;tag_open&#34;: self.tag_open,
        &#34;tag_close&#34;: self.tag_close,
    }</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="glai.AIMessages"><code class="flex name class">
<span>class <span class="ident">AIMessages</span></span>
<span>(</span><span>user_tags: Union[tuple[str], list[str], dict] = ('[INST]', '[/INST]'), ai_tags: Union[tuple[str], list[str], dict] = ('', ''), system_tags: Union[tuple[str], list[str], dict, ForwardRef(None)] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Represents a collection of messages in an AI system.</p>
<h2 id="properties">Properties</h2>
<p>user_tag_open (str): The opening tag for user messages.
user_tag_close (str): The closing tag for user messages.
ai_tag_open (str): The opening tag for AI messages.
ai_tag_close (str): The closing tag for AI messages.
system_tag_open (str): The opening tag for system messages.
system_tag_close (str): The closing tag for system messages.
messages (dict): The messages in the collection: {id: AIMessage}.
_message_id_generator (int): The id generator for the messages.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>messages</code></strong> :&ensp;<code>Union[<a title="glai.AIMessages" href="#glai.AIMessages">AIMessages</a>, <a title="glai.AIMessage" href="#glai.AIMessage">AIMessage</a>, str, list]</code></dt>
<dd>The messages to add to the collection.</dd>
<dt><strong><code>user_tags</code></strong> :&ensp;<code>tuple</code></dt>
<dd>The tags to use for user messages.</dd>
<dt><strong><code>ai_tags</code></strong> :&ensp;<code>tuple</code></dt>
<dd>The tags to use for AI messages.</dd>
<dt><strong><code>system_tags</code></strong> :&ensp;<code>tuple</code></dt>
<dd>The tags to use for system messages.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AIMessages:
    &#34;&#34;&#34;
    Represents a collection of messages in an AI system.
    Properties:
        user_tag_open (str): The opening tag for user messages.
        user_tag_close (str): The closing tag for user messages.
        ai_tag_open (str): The opening tag for AI messages.
        ai_tag_close (str): The closing tag for AI messages.
        system_tag_open (str): The opening tag for system messages.
        system_tag_close (str): The closing tag for system messages.
        messages (dict): The messages in the collection: {id: AIMessage}.
        _message_id_generator (int): The id generator for the messages.

    Args:
        messages (Union[AIMessages, AIMessage, str, list]): The messages to add to the collection.
        user_tags (tuple): The tags to use for user messages.
        ai_tags (tuple): The tags to use for AI messages.
        system_tags (tuple): The tags to use for system messages.
    &#34;&#34;&#34;

    def __init__(self,user_tags:Union[tuple[str], list[str], dict]=(&#34;[INST]&#34;, &#34;[/INST]&#34;), ai_tags:Union[tuple[str], list[str], dict]=(&#34;&#34;, &#34;&#34;), system_tags:Optional[Union[tuple[str], list[str], dict]]=None):
        if isinstance(user_tags, dict):
            if &#34;open&#34; in user_tags and &#34;close&#34; in user_tags:
                self.user_tag_open = user_tags[&#34;open&#34;]
                self.user_tag_close = user_tags[&#34;close&#34;]
            else:
                raise ValueError(f&#34;Invalid user tags: {user_tags}, for dict tags both &#39;open&#39; and &#39;close&#39; keys must be present.&#34;)
        elif isinstance(user_tags, set) or isinstance(user_tags, list) or isinstance(user_tags, tuple):
            self.user_tag_open = user_tags[0]
            self.user_tag_close = user_tags[1]
        else:
            raise TypeError(f&#34;Invalid type for user tags: {type(user_tags)}, must be dict, set or list.&#34;)
        
        if isinstance(ai_tags, dict):
            if &#34;open&#34; in ai_tags and &#34;close&#34; in ai_tags:
                self.ai_tag_open = ai_tags[&#34;open&#34;]
                self.ai_tag_close = ai_tags[&#34;close&#34;]
            else:
                raise ValueError(f&#34;Invalid user tags: {ai_tags}, for dict tags both &#39;open&#39; and &#39;close&#39; keys must be present.&#34;)
        elif isinstance(ai_tags, set) or isinstance(ai_tags, list) or isinstance(ai_tags, tuple):
            self.ai_tag_open = ai_tags[0]
            self.ai_tag_close = ai_tags[1]
        else:
            raise TypeError(f&#34;Invalid type for user tags: {type(ai_tags)}, must be dict, set or list.&#34;)
        
        if system_tags is not None:
            if isinstance(system_tags, dict):
                if &#34;open&#34; in system_tags and &#34;close&#34; in system_tags:
                    self.system_tag_open = system_tags[&#34;open&#34;]
                    self.system_tag_close = system_tags[&#34;close&#34;]
                else:
                    raise ValueError(f&#34;Invalid system tags: {system_tags}, for dict tags both &#39;open&#39; and &#39;close&#39; keys must be present.&#34;)
            elif isinstance(system_tags, set) or isinstance(system_tags, list) or isinstance(system_tags, tuple):
                self.system_tag_open = system_tags[0]
                self.system_tag_close = system_tags[1]
            else:
                raise TypeError(f&#34;Invalid type for system tags: {type(system_tags)}, must be dict, set or list.&#34;)
        else:
            self.system_tag_open = None
            self.system_tag_close = None
        self.messages = {}
        self._message_id_generator = 0
    
    def user_tags(self) -&gt; tuple[str]:
        &#34;&#34;&#34;
        Returns the user tags.

        Returns:
            tuple[str]: The user tags.
        &#34;&#34;&#34;
        return (self.user_tag_open, self.user_tag_close)
    
    def ai_tags(self) -&gt; tuple[str]:
        &#34;&#34;&#34;
        Returns the AI tags.

        Returns:
            tuple[str]: The AI tags.
        &#34;&#34;&#34;
        return (self.ai_tag_open, self.ai_tag_close)

    def system_tags(self) -&gt; Union[tuple[str], None]:
        &#34;&#34;&#34;
        Returns the system tags.

        Returns:
            tuple[str]: The system tags.
        &#34;&#34;&#34;
        if self.system_tag_open is not None and self.system_tag_close is not None:
            return (self.system_tag_open, self.system_tag_close)
        else:
            return None
    
    def load_messages(self, messages:Union[Any, AIMessage, str, list[Union[dict, AIMessage]]]) -&gt; None:
        if messages is not None:
            if isinstance(messages, AIMessages):
                self.messages = messages.messages
            elif isinstance(messages, AIMessage):
                self.messages = [messages]
            elif isinstance(messages, str):
                self.messages = [AIMessage(messages, self.user_tag_open, self.user_tag_close)]
            elif isinstance(messages, list):
                if all([isinstance(message, AIMessage) for message in messages]):
                    self.messages = messages
                elif all(isinstance(message, str) for message in messages):
                    self.messages = [AIMessage(message, tag_open=self.user_tag_open, tag_close=self.user_tag_close) for message in messages]
                elif all(isinstance(message, dict) for message in messages):
                    self.messages = [AIMessage.from_dict(message_dict) for message_dict in messages]
                else:
                    raise TypeError(&#34;If passing list as messages it must be a list of AIMessage or str&#34;)
            else:
                raise TypeError(&#34;messages must be a list of AIMessage or str&#34;)
    
    def to_dict(self) -&gt; dict:
        &#34;&#34;&#34;
        Returns the messages as a dictionary.

        Returns:
            dict: The messages as a dictionary.
        &#34;&#34;&#34;
        return {
            &#34;user_tag_open&#34;: self.user_tag_open,
            &#34;user_tag_close&#34;: self.user_tag_close,
            &#34;ai_tag_open&#34;: self.ai_tag_open,
            &#34;ai_tag_close&#34;: self.ai_tag_close,
            &#34;system_tag_open&#34;: self.system_tag_open,
            &#34;system_tag_close&#34;: self.system_tag_close,
            &#34;messages&#34;: [message.to_dict() for message in self.messages]
        }
    
    @staticmethod
    def from_dict(messages_dict:dict) -&gt; &#34;AIMessages&#34;:
        &#34;&#34;&#34;
        Creates a new AIMessages from a dictionary.

        Args:
            messages_dict (dict): The dictionary to create the AIMessages from.

        Returns:
            AIMessages: The created AIMessages.
        &#34;&#34;&#34;
        ai_msgs = AIMessages()
        ai_msgs.user_tag_open = messages_dict[&#34;user_tag_open&#34;]
        ai_msgs.user_tag_close = messages_dict[&#34;user_tag_close&#34;]
        ai_msgs.ai_tag_open = messages_dict[&#34;ai_tag_open&#34;]
        ai_msgs.ai_tag_close = messages_dict[&#34;ai_tag_close&#34;]
        ai_msgs.system_tag_open = messages_dict[&#34;system_tag_open&#34;] if &#34;system_tag_open&#34; in messages_dict else None
        ai_msgs.system_tag_close = messages_dict[&#34;system_tag_close&#34;] if &#34;system_tag_close&#34; in messages_dict else None
        ai_msgs.load_messages(messages_dict[&#34;messages&#34;])
        return ai_msgs
    
    def _generate_message_id(self) -&gt; int:
        &#34;&#34;&#34;
        Generates a unique message ID.
        Iters the message ID generator.

        Returns:
            int: The generated message ID.
        &#34;&#34;&#34;
        self._message_id_generator += 1
        id = self._message_id_generator
        return id
    
    def add_message(self, message:Union[str, AIMessage], tag_open:str, tag_close:str) -&gt; AIMessage:
        &#34;&#34;&#34;
        Adds a new message to the messages dictionary.
        Iters the message ID generator.

        Parameters:
        - message (str|AIMessage): The content of the message or the message object
        - tag_open (str): The opening tag for the message, set to None if message is AIMessage
        - tag_close (str): The closing tag for the message, set to None if message is AIMessage

        Returns:
        AIMessage
        &#34;&#34;&#34;
        if isinstance(message, str):
            message = AIMessage(message, tag_open, tag_close)
        self.messages[self._generate_message_id()] = message
        return self.messages[self._message_id_generator]

    def _insert_message(self, message:AIMessage, message_id:int) -&gt; AIMessage:
        &#34;&#34;&#34;
        Inserts a new message to the messages dictionary.
        Iters the message ID generator.

        Parameters:
        - message (AIMessage): The message object
        - message_id (int): The id to insert the message at

        Returns:
        AIMessage
        &#34;&#34;&#34;
        self._message_id_generator += 1 if len(self.messages) &gt; 0 else 0
        updated_messages = {}
        for id, msg in self.messages.items():
            if id &lt; message_id:
                updated_messages[id] = msg
            else:
                updated_messages[id+1] = msg
        updated_messages[message_id] = message
        self.messages = updated_messages

    def add_user_message(self, message: Union[str, AIMessage]) -&gt; AIMessage:
        &#34;&#34;&#34;
        Adds a user message to the message list.
        Uses add_message() with the user tags.
        Automatically iters the message ID generator.

        Parameters:
            message (str): The message to be added.

        Returns:
            AIMessage
        &#34;&#34;&#34;
        return self.add_message(message, self.user_tag_open, self.user_tag_close)

    def add_ai_message(self, message:Union[str, AIMessage]) -&gt; AIMessage:
        &#34;&#34;&#34;
        Adds a ai message to the message list.
        Uses add_message() with the ai tags.
        Automatically iters the message ID generator.

        Parameters:
            message (str): The message to be added.

        Returns:
            AIMessage
        &#34;&#34;&#34;
        return self.add_message(message, self.ai_tag_open, self.ai_tag_close)
    
    def set_system_message(self, message:Union[str, AIMessage]) -&gt; AIMessage:
        &#34;&#34;&#34;
        Adds a system message to the message list.
        Uses add_message() with the system tags.
        Automatically iters the message ID generator.

        Parameters:
            message (str): The message to be added.

        Returns:
            AIMessage
        &#34;&#34;&#34;
        if not self.has_system_tags():
            print(&#34;System tags are not set, this model does not support system messages.&#34;)
        else:
            if isinstance(message, str):
                message = AIMessage(message, self.system_tag_open, self.system_tag_close)    
            return self._insert_message(message, 0)

    def reset_messages(self) -&gt; None:
        self.messages = {}
        self._message_id_generator = 0

    def __str__(self) -&gt; str:
        return &#34;&#34;.join([str(message) for message in self.messages.values()])

    def __repr__(self) -&gt; str:
        return self.__str__()
    
    def text(self):
        return self.__str__()
    
    def get_last_message(self) -&gt; AIMessage:
        &#34;&#34;&#34;
        Returns the last message in the collection.

        Returns:
            AIMessage: The last message in the collection.
        &#34;&#34;&#34;
        return self.messages[self._message_id_generator]
    
    def edit_last_message(self, new_content:str, tag_open:str=None, tag_close:str=None) -&gt; None:
        &#34;&#34;&#34;
        Updates the content of the last message in the collection.

        Parameters:
            new_content (str): The new content for the message.
            tag_open (str): Optional. The new opening tag for the message.
            tag_close (str): Optional. The new closing tag for the message.

        Returns:
            None
        &#34;&#34;&#34;
        self.get_last_message().edit(new_content, tag_open, tag_close)
    
    def edit_message(self, message_id:int, new_content:str, tag_open:str=None, tag_close:str=None) -&gt; None:
        &#34;&#34;&#34;
        Updates the content of a message in the collection.

        Parameters:
            message_id (int): The id of the message to edit.
            new_content (str): The new content for the message.
            tag_open (str): The new opening tag for the message.
            tag_close (str): The new closing tag for the message.

        Returns:
            None
        &#34;&#34;&#34;
        self.messages[message_id].edit(new_content, tag_open, tag_close)

    def edit_system_message(self, new_content:str) -&gt; None:
        if self.system_tags() is None:
            raise ValueError(&#34;System tags are not set, this model does not support system messages.&#34;)
        else:
            if self.messages[0].tag_open == self.system_tag_open and self.messages[0].tag_close == self.system_tag_close:
                self.edit_message(0, new_content)
            else:
                print(&#34;Warning: System message not found, adding system message to the start of the message list.&#34;)
                self.set_system_message(new_content)

    def has_system_tags(self) -&gt; bool:
        &#34;&#34;&#34;
        Returns whether the model supports system messages.

        Returns:
            bool: Whether the model supports system messages.
        &#34;&#34;&#34;
        return self.system_tags() is not None
    
    def save_json(self, file_path:str) -&gt; None:
        &#34;&#34;&#34;
        Saves the messages as a json file.

        Parameters:
            file_path (str): The path to save the json file to.

        Returns:
            None
        &#34;&#34;&#34;
        save_json_file(self.to_dict(), file_path)
    
    @staticmethod
    def from_json(file_path:str) -&gt; &#34;AIMessages&#34;:
        &#34;&#34;&#34;
        Creates a new AIMessages from a json file.

        Args:
            file_path (str): The path to the json file.

        Returns:
            AIMessages: The created AIMessages.
        &#34;&#34;&#34;
        return AIMessages.from_dict(load_json_file(file_path))
    
    @staticmethod
    def create_single_message(message:str, tag_open:str=&#34;&#34;, tag_close:str=&#34;&#34;) -&gt;AIMessage:
        &#34;&#34;&#34;
        Creates a single AIMessage.

        Args:
            message (str): The content of the message.
            tag_open (str): The opening tag for the message.
            tag_close (str): The closing tag for the message.

        Returns:
            AIMessage: The created AIMessage.
        &#34;&#34;&#34;
        return AIMessage(message, tag_open, tag_close)</code></pre>
</details>
<h3>Static methods</h3>
<dl>
<dt id="glai.AIMessages.create_single_message"><code class="name flex">
<span>def <span class="ident">create_single_message</span></span>(<span>message: str, tag_open: str = '', tag_close: str = '') ‑> <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Creates a single AIMessage.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>message</code></strong> :&ensp;<code>str</code></dt>
<dd>The content of the message.</dd>
<dt><strong><code>tag_open</code></strong> :&ensp;<code>str</code></dt>
<dd>The opening tag for the message.</dd>
<dt><strong><code>tag_close</code></strong> :&ensp;<code>str</code></dt>
<dd>The closing tag for the message.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="glai.AIMessage" href="#glai.AIMessage">AIMessage</a></code></dt>
<dd>The created AIMessage.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def create_single_message(message:str, tag_open:str=&#34;&#34;, tag_close:str=&#34;&#34;) -&gt;AIMessage:
    &#34;&#34;&#34;
    Creates a single AIMessage.

    Args:
        message (str): The content of the message.
        tag_open (str): The opening tag for the message.
        tag_close (str): The closing tag for the message.

    Returns:
        AIMessage: The created AIMessage.
    &#34;&#34;&#34;
    return AIMessage(message, tag_open, tag_close)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.from_dict"><code class="name flex">
<span>def <span class="ident">from_dict</span></span>(<span>messages_dict: dict) ‑> <a title="glai.messages.messages.AIMessages" href="messages/messages.html#glai.messages.messages.AIMessages">AIMessages</a></span>
</code></dt>
<dd>
<div class="desc"><p>Creates a new AIMessages from a dictionary.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>messages_dict</code></strong> :&ensp;<code>dict</code></dt>
<dd>The dictionary to create the AIMessages from.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="glai.AIMessages" href="#glai.AIMessages">AIMessages</a></code></dt>
<dd>The created AIMessages.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_dict(messages_dict:dict) -&gt; &#34;AIMessages&#34;:
    &#34;&#34;&#34;
    Creates a new AIMessages from a dictionary.

    Args:
        messages_dict (dict): The dictionary to create the AIMessages from.

    Returns:
        AIMessages: The created AIMessages.
    &#34;&#34;&#34;
    ai_msgs = AIMessages()
    ai_msgs.user_tag_open = messages_dict[&#34;user_tag_open&#34;]
    ai_msgs.user_tag_close = messages_dict[&#34;user_tag_close&#34;]
    ai_msgs.ai_tag_open = messages_dict[&#34;ai_tag_open&#34;]
    ai_msgs.ai_tag_close = messages_dict[&#34;ai_tag_close&#34;]
    ai_msgs.system_tag_open = messages_dict[&#34;system_tag_open&#34;] if &#34;system_tag_open&#34; in messages_dict else None
    ai_msgs.system_tag_close = messages_dict[&#34;system_tag_close&#34;] if &#34;system_tag_close&#34; in messages_dict else None
    ai_msgs.load_messages(messages_dict[&#34;messages&#34;])
    return ai_msgs</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.from_json"><code class="name flex">
<span>def <span class="ident">from_json</span></span>(<span>file_path: str) ‑> <a title="glai.messages.messages.AIMessages" href="messages/messages.html#glai.messages.messages.AIMessages">AIMessages</a></span>
</code></dt>
<dd>
<div class="desc"><p>Creates a new AIMessages from a json file.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_path</code></strong> :&ensp;<code>str</code></dt>
<dd>The path to the json file.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="glai.AIMessages" href="#glai.AIMessages">AIMessages</a></code></dt>
<dd>The created AIMessages.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def from_json(file_path:str) -&gt; &#34;AIMessages&#34;:
    &#34;&#34;&#34;
    Creates a new AIMessages from a json file.

    Args:
        file_path (str): The path to the json file.

    Returns:
        AIMessages: The created AIMessages.
    &#34;&#34;&#34;
    return AIMessages.from_dict(load_json_file(file_path))</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="glai.AIMessages.add_ai_message"><code class="name flex">
<span>def <span class="ident">add_ai_message</span></span>(<span>self, message: Union[str, <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a>]) ‑> <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Adds a ai message to the message list.
Uses add_message() with the ai tags.
Automatically iters the message ID generator.</p>
<h2 id="parameters">Parameters</h2>
<p>message (str): The message to be added.</p>
<h2 id="returns">Returns</h2>
<p>AIMessage</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_ai_message(self, message:Union[str, AIMessage]) -&gt; AIMessage:
    &#34;&#34;&#34;
    Adds a ai message to the message list.
    Uses add_message() with the ai tags.
    Automatically iters the message ID generator.

    Parameters:
        message (str): The message to be added.

    Returns:
        AIMessage
    &#34;&#34;&#34;
    return self.add_message(message, self.ai_tag_open, self.ai_tag_close)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.add_message"><code class="name flex">
<span>def <span class="ident">add_message</span></span>(<span>self, message: Union[str, <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a>], tag_open: str, tag_close: str) ‑> <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Adds a new message to the messages dictionary.
Iters the message ID generator.</p>
<p>Parameters:
- message (str|AIMessage): The content of the message or the message object
- tag_open (str): The opening tag for the message, set to None if message is AIMessage
- tag_close (str): The closing tag for the message, set to None if message is AIMessage</p>
<p>Returns:
AIMessage</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_message(self, message:Union[str, AIMessage], tag_open:str, tag_close:str) -&gt; AIMessage:
    &#34;&#34;&#34;
    Adds a new message to the messages dictionary.
    Iters the message ID generator.

    Parameters:
    - message (str|AIMessage): The content of the message or the message object
    - tag_open (str): The opening tag for the message, set to None if message is AIMessage
    - tag_close (str): The closing tag for the message, set to None if message is AIMessage

    Returns:
    AIMessage
    &#34;&#34;&#34;
    if isinstance(message, str):
        message = AIMessage(message, tag_open, tag_close)
    self.messages[self._generate_message_id()] = message
    return self.messages[self._message_id_generator]</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.add_user_message"><code class="name flex">
<span>def <span class="ident">add_user_message</span></span>(<span>self, message: Union[str, <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a>]) ‑> <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Adds a user message to the message list.
Uses add_message() with the user tags.
Automatically iters the message ID generator.</p>
<h2 id="parameters">Parameters</h2>
<p>message (str): The message to be added.</p>
<h2 id="returns">Returns</h2>
<p>AIMessage</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def add_user_message(self, message: Union[str, AIMessage]) -&gt; AIMessage:
    &#34;&#34;&#34;
    Adds a user message to the message list.
    Uses add_message() with the user tags.
    Automatically iters the message ID generator.

    Parameters:
        message (str): The message to be added.

    Returns:
        AIMessage
    &#34;&#34;&#34;
    return self.add_message(message, self.user_tag_open, self.user_tag_close)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.ai_tags"><code class="name flex">
<span>def <span class="ident">ai_tags</span></span>(<span>self) ‑> tuple[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the AI tags.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[str]</code></dt>
<dd>The AI tags.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ai_tags(self) -&gt; tuple[str]:
    &#34;&#34;&#34;
    Returns the AI tags.

    Returns:
        tuple[str]: The AI tags.
    &#34;&#34;&#34;
    return (self.ai_tag_open, self.ai_tag_close)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.edit_last_message"><code class="name flex">
<span>def <span class="ident">edit_last_message</span></span>(<span>self, new_content: str, tag_open: str = None, tag_close: str = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Updates the content of the last message in the collection.</p>
<h2 id="parameters">Parameters</h2>
<p>new_content (str): The new content for the message.
tag_open (str): Optional. The new opening tag for the message.
tag_close (str): Optional. The new closing tag for the message.</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit_last_message(self, new_content:str, tag_open:str=None, tag_close:str=None) -&gt; None:
    &#34;&#34;&#34;
    Updates the content of the last message in the collection.

    Parameters:
        new_content (str): The new content for the message.
        tag_open (str): Optional. The new opening tag for the message.
        tag_close (str): Optional. The new closing tag for the message.

    Returns:
        None
    &#34;&#34;&#34;
    self.get_last_message().edit(new_content, tag_open, tag_close)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.edit_message"><code class="name flex">
<span>def <span class="ident">edit_message</span></span>(<span>self, message_id: int, new_content: str, tag_open: str = None, tag_close: str = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Updates the content of a message in the collection.</p>
<h2 id="parameters">Parameters</h2>
<p>message_id (int): The id of the message to edit.
new_content (str): The new content for the message.
tag_open (str): The new opening tag for the message.
tag_close (str): The new closing tag for the message.</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit_message(self, message_id:int, new_content:str, tag_open:str=None, tag_close:str=None) -&gt; None:
    &#34;&#34;&#34;
    Updates the content of a message in the collection.

    Parameters:
        message_id (int): The id of the message to edit.
        new_content (str): The new content for the message.
        tag_open (str): The new opening tag for the message.
        tag_close (str): The new closing tag for the message.

    Returns:
        None
    &#34;&#34;&#34;
    self.messages[message_id].edit(new_content, tag_open, tag_close)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.edit_system_message"><code class="name flex">
<span>def <span class="ident">edit_system_message</span></span>(<span>self, new_content: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def edit_system_message(self, new_content:str) -&gt; None:
    if self.system_tags() is None:
        raise ValueError(&#34;System tags are not set, this model does not support system messages.&#34;)
    else:
        if self.messages[0].tag_open == self.system_tag_open and self.messages[0].tag_close == self.system_tag_close:
            self.edit_message(0, new_content)
        else:
            print(&#34;Warning: System message not found, adding system message to the start of the message list.&#34;)
            self.set_system_message(new_content)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.get_last_message"><code class="name flex">
<span>def <span class="ident">get_last_message</span></span>(<span>self) ‑> <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Returns the last message in the collection.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code><a title="glai.AIMessage" href="#glai.AIMessage">AIMessage</a></code></dt>
<dd>The last message in the collection.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_last_message(self) -&gt; AIMessage:
    &#34;&#34;&#34;
    Returns the last message in the collection.

    Returns:
        AIMessage: The last message in the collection.
    &#34;&#34;&#34;
    return self.messages[self._message_id_generator]</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.has_system_tags"><code class="name flex">
<span>def <span class="ident">has_system_tags</span></span>(<span>self) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Returns whether the model supports system messages.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>bool</code></dt>
<dd>Whether the model supports system messages.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def has_system_tags(self) -&gt; bool:
    &#34;&#34;&#34;
    Returns whether the model supports system messages.

    Returns:
        bool: Whether the model supports system messages.
    &#34;&#34;&#34;
    return self.system_tags() is not None</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.load_messages"><code class="name flex">
<span>def <span class="ident">load_messages</span></span>(<span>self, messages: Union[Any, <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a>, str, list[Union[dict, <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a>]]]) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_messages(self, messages:Union[Any, AIMessage, str, list[Union[dict, AIMessage]]]) -&gt; None:
    if messages is not None:
        if isinstance(messages, AIMessages):
            self.messages = messages.messages
        elif isinstance(messages, AIMessage):
            self.messages = [messages]
        elif isinstance(messages, str):
            self.messages = [AIMessage(messages, self.user_tag_open, self.user_tag_close)]
        elif isinstance(messages, list):
            if all([isinstance(message, AIMessage) for message in messages]):
                self.messages = messages
            elif all(isinstance(message, str) for message in messages):
                self.messages = [AIMessage(message, tag_open=self.user_tag_open, tag_close=self.user_tag_close) for message in messages]
            elif all(isinstance(message, dict) for message in messages):
                self.messages = [AIMessage.from_dict(message_dict) for message_dict in messages]
            else:
                raise TypeError(&#34;If passing list as messages it must be a list of AIMessage or str&#34;)
        else:
            raise TypeError(&#34;messages must be a list of AIMessage or str&#34;)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.reset_messages"><code class="name flex">
<span>def <span class="ident">reset_messages</span></span>(<span>self) ‑> None</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def reset_messages(self) -&gt; None:
    self.messages = {}
    self._message_id_generator = 0</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.save_json"><code class="name flex">
<span>def <span class="ident">save_json</span></span>(<span>self, file_path: str) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Saves the messages as a json file.</p>
<h2 id="parameters">Parameters</h2>
<p>file_path (str): The path to save the json file to.</p>
<h2 id="returns">Returns</h2>
<p>None</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def save_json(self, file_path:str) -&gt; None:
    &#34;&#34;&#34;
    Saves the messages as a json file.

    Parameters:
        file_path (str): The path to save the json file to.

    Returns:
        None
    &#34;&#34;&#34;
    save_json_file(self.to_dict(), file_path)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.set_system_message"><code class="name flex">
<span>def <span class="ident">set_system_message</span></span>(<span>self, message: Union[str, <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a>]) ‑> <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Adds a system message to the message list.
Uses add_message() with the system tags.
Automatically iters the message ID generator.</p>
<h2 id="parameters">Parameters</h2>
<p>message (str): The message to be added.</p>
<h2 id="returns">Returns</h2>
<p>AIMessage</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def set_system_message(self, message:Union[str, AIMessage]) -&gt; AIMessage:
    &#34;&#34;&#34;
    Adds a system message to the message list.
    Uses add_message() with the system tags.
    Automatically iters the message ID generator.

    Parameters:
        message (str): The message to be added.

    Returns:
        AIMessage
    &#34;&#34;&#34;
    if not self.has_system_tags():
        print(&#34;System tags are not set, this model does not support system messages.&#34;)
    else:
        if isinstance(message, str):
            message = AIMessage(message, self.system_tag_open, self.system_tag_close)    
        return self._insert_message(message, 0)</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.system_tags"><code class="name flex">
<span>def <span class="ident">system_tags</span></span>(<span>self) ‑> Optional[tuple[str]]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the system tags.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[str]</code></dt>
<dd>The system tags.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def system_tags(self) -&gt; Union[tuple[str], None]:
    &#34;&#34;&#34;
    Returns the system tags.

    Returns:
        tuple[str]: The system tags.
    &#34;&#34;&#34;
    if self.system_tag_open is not None and self.system_tag_close is not None:
        return (self.system_tag_open, self.system_tag_close)
    else:
        return None</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.text"><code class="name flex">
<span>def <span class="ident">text</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def text(self):
    return self.__str__()</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.to_dict"><code class="name flex">
<span>def <span class="ident">to_dict</span></span>(<span>self) ‑> dict</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the messages as a dictionary.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>dict</code></dt>
<dd>The messages as a dictionary.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def to_dict(self) -&gt; dict:
    &#34;&#34;&#34;
    Returns the messages as a dictionary.

    Returns:
        dict: The messages as a dictionary.
    &#34;&#34;&#34;
    return {
        &#34;user_tag_open&#34;: self.user_tag_open,
        &#34;user_tag_close&#34;: self.user_tag_close,
        &#34;ai_tag_open&#34;: self.ai_tag_open,
        &#34;ai_tag_close&#34;: self.ai_tag_close,
        &#34;system_tag_open&#34;: self.system_tag_open,
        &#34;system_tag_close&#34;: self.system_tag_close,
        &#34;messages&#34;: [message.to_dict() for message in self.messages]
    }</code></pre>
</details>
</dd>
<dt id="glai.AIMessages.user_tags"><code class="name flex">
<span>def <span class="ident">user_tags</span></span>(<span>self) ‑> tuple[str]</span>
</code></dt>
<dd>
<div class="desc"><p>Returns the user tags.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>tuple[str]</code></dt>
<dd>The user tags.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def user_tags(self) -&gt; tuple[str]:
    &#34;&#34;&#34;
    Returns the user tags.

    Returns:
        tuple[str]: The user tags.
    &#34;&#34;&#34;
    return (self.user_tag_open, self.user_tag_close)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="glai.AutoAI"><code class="flex name class">
<span>class <span class="ident">AutoAI</span></span>
<span>(</span><span>name_search: Optional[str] = None, quantization_search: Optional[str] = None, keyword_search: Optional[str] = None, search_only_downloaded_models: bool = False, max_total_tokens: int = 1500, model_db_dir: Optional[str] = None)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize the AutoAI class for super easy LLM AI generation.</p>
<p>Searches for a model based on name/quantization/keyword.
Downloads model and sets up LlamaAI.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name_search</code></strong></dt>
<dd>Name of model to search for. Optional. Default None.</dd>
<dt><strong><code>quantization_search</code></strong></dt>
<dd>Quantization of model to search for. Optional. Default None.</dd>
<dt><strong><code>keyword_search</code></strong></dt>
<dd>Keyword of model to search for. Optional. Default None.</dd>
<dt><strong><code>new_tokens</code></strong></dt>
<dd>New token length for LlamaAI model. Default 1500.</dd>
<dt><strong><code>max_input_tokens</code></strong></dt>
<dd>Max input tokens for LlamaAI model. Default 900.</dd>
<dt><strong><code>model_db_dir</code></strong></dt>
<dd>Directory to store model data in. Defaults to global packages model directory.</dd>
</dl>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model_db</code></strong></dt>
<dd>ModelDB object. - represents the database of models, has useful functions for searching and importing models.</dd>
<dt><strong><code>model_data</code></strong></dt>
<dd>ModelData object. - represents the data of the model, has useful functions for creating, downloading and loading the model data and gguf.</dd>
<dt><strong><code>ai</code></strong></dt>
<dd>LlamaAI object. - represents the LlamaAI model, a wrapper for llama llm and tokenizer models quantized to gguf format. Has methods for adjusting generation and for generating.</dd>
<dt><strong><code>msgs</code></strong></dt>
<dd>AIMessages object. - represents the AIMessages a collection of AIMessage objects, has useful functions for adding and editing messages and can be printed to string.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AutoAI:
    &#34;&#34;&#34;
    Initialize the AutoAI class for super easy LLM AI generation.

    Searches for a model based on name/quantization/keyword. 
    Downloads model and sets up LlamaAI.

    Args:
        name_search: Name of model to search for. Optional. Default None.
        quantization_search: Quantization of model to search for. Optional. Default None.
        keyword_search: Keyword of model to search for. Optional. Default None.
        new_tokens: New token length for LlamaAI model. Default 1500.
        max_input_tokens: Max input tokens for LlamaAI model. Default 900.
        model_db_dir: Directory to store model data in. Defaults to global packages model directory.

    Attributes:
        model_db: ModelDB object. - represents the database of models, has useful functions for searching and importing models.
        model_data: ModelData object. - represents the data of the model, has useful functions for creating, downloading and loading the model data and gguf.
        ai: LlamaAI object. - represents the LlamaAI model, a wrapper for llama llm and tokenizer models quantized to gguf format. Has methods for adjusting generation and for generating.
        msgs: AIMessages object. - represents the AIMessages a collection of AIMessage objects, has useful functions for adding and editing messages and can be printed to string.
        
    &#34;&#34;&#34;
    def __init__(self, 
                 name_search: Optional[str] = None,
                 quantization_search: Optional[str] = None,
                 keyword_search: Optional[str] = None,
                 search_only_downloaded_models:bool = False,
                 max_total_tokens: int = 1500,
                 model_db_dir:Optional[str] = None,
                 ) -&gt; None:

        self.model_db = ModelDB(model_db_dir=model_db_dir, copy_verified_models=True)
        self.model_data: ModelData = self.model_db.find_model(
            name_search, quantization_search, keyword_search, search_only_downloaded_models
        )
        self.model_data.download_gguf()
        self.ai = LlamaAI(
            self.model_data.gguf_file_path, max_tokens=max_total_tokens
        )
        print(f&#34;Using model: {self.model_data}&#34;)
        self.msgs: AIMessages = AIMessages(
            self.model_data.user_tags, self.model_data.ai_tags, self.model_data.system_tags
        )
    
    def generate_from_messages(self, stop_at:str = None, include_stop_str:bool = True) -&gt; AIMessage:
        prompt = self.msgs.text()
        ai_message = self.generate_from_literal_string(prompt, stop_at=stop_at, include_stop_str=include_stop_str)
        self.msgs.add_ai_message(ai_message)
        return ai_message
    
    def generate_from_literal_string(
        self, 
        prompt: str,
        stop_at:str = None,
        include_stop_str:bool = True
    ) -&gt; AIMessage:
        &#34;&#34;&#34;
        Generate text from a prompt using the LlamaAI model.

        Args:
            prompt: Prompt text to generate from.

        Returns:
            Generated text string.
        &#34;&#34;&#34;
        return self.ai.infer(
            prompt, only_string=True, stop_at_str=stop_at, include_stop_str=include_stop_str
        )

    def generate(
        self,
        user_message: str,
        ai_message_tbc: Optional[str] = None,
        stop_at:Optional[str] = None,
        include_stop_str:bool = True,
        system_message: Optional[str] = None
    ) -&gt; AIMessage:
        &#34;&#34;&#34;
        Generate an AI response to a user message.

        Args:
            user_message: User message text.
            ai_message_tbc: Optional text to prepend.
            stop_at: Optional string to stop generation at.
            include_stop_str: Whether to include the stop string in the generated message.
            system_message: Optional system message to include at the start, not all models support this.
            If you provide system message to a model that doesn&#39;t support it, it will be ignored.
            You can check if a model supports system messages by checking the model_data.has_system_tags() method.
        Returns:
            Generated AIMessage object.
        &#34;&#34;&#34;
        generation_messages = AIMessages(user_tags=self.model_data.user_tags, ai_tags=self.model_data.ai_tags, system_tags=self.model_data.system_tags)
        generation_messages.reset_messages()
        if self.model_data.has_system_tags():
            if system_message is not None:
                generation_messages.set_system_message(system_message)
            else:
                print(&#34;WARNING: Model seeps to support system messages, but no system message provided.&#34;)
        generation_messages.add_user_message(user_message)


        if ai_message_tbc is not None:
            generation_messages.add_message(
                ai_message_tbc, 
                self.msgs.ai_tag_open, 
                &#34;&#34;
            )
        print(f&#34;Promt: {generation_messages.text()}&#34;)
        
        generated = self.generate_from_literal_string(generation_messages.text(), stop_at=stop_at, include_stop_str=include_stop_str)

        if ai_message_tbc is not None:
            generation_messages.edit_last_message(
                ai_message_tbc + generated,
                self.msgs.ai_tag_open,
                self.msgs.ai_tag_close
            )
        else:
            generation_messages.add_ai_message(generated)

        print(f&#34;Generated: {generation_messages.get_last_message().text()}&#34;)
        output = generation_messages.get_last_message()
        return output

    def count_tokens(
        self,
        user_message: str,
        ai_message_tbc: Optional[str] = None
    ) -&gt; int:
        &#34;&#34;&#34;
        Count the number of tokens in a generated message.

        Args:
            user_message: User message text.
            ai_message_tbc: Optional text to prepend.

        Returns:
            Number of tokens in generated message.
        &#34;&#34;&#34;
        generation_messages = AIMessages()
        generation_messages.reset_messages()
        generation_messages.add_user_message(user_message)

        if ai_message_tbc is not None:
            generation_messages.add_message(
                ai_message_tbc, 
                self.msgs.ai_tag_open, 
                &#34;&#34;
            )
        return self.ai.count_tokens(generation_messages.text())
    
    def is_within_input_limit(
        self,
        user_message: str,
        ai_message_tbc: Optional[str] = None
        ) -&gt; bool:
        &#34;&#34;&#34;
        Check if the generated message is within the input limit.

        Args:
            user_message: User message text.
            ai_message_tbc: Optional text to prepend.

        Returns:
            True if within input limit, False otherwise.
        &#34;&#34;&#34;
        return self.ai.is_within_input_limit(self.count_tokens(user_message, ai_message_tbc))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="glai.AutoAI.count_tokens"><code class="name flex">
<span>def <span class="ident">count_tokens</span></span>(<span>self, user_message: str, ai_message_tbc: Optional[str] = None) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Count the number of tokens in a generated message.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user_message</code></strong></dt>
<dd>User message text.</dd>
<dt><strong><code>ai_message_tbc</code></strong></dt>
<dd>Optional text to prepend.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Number of tokens in generated message.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def count_tokens(
    self,
    user_message: str,
    ai_message_tbc: Optional[str] = None
) -&gt; int:
    &#34;&#34;&#34;
    Count the number of tokens in a generated message.

    Args:
        user_message: User message text.
        ai_message_tbc: Optional text to prepend.

    Returns:
        Number of tokens in generated message.
    &#34;&#34;&#34;
    generation_messages = AIMessages()
    generation_messages.reset_messages()
    generation_messages.add_user_message(user_message)

    if ai_message_tbc is not None:
        generation_messages.add_message(
            ai_message_tbc, 
            self.msgs.ai_tag_open, 
            &#34;&#34;
        )
    return self.ai.count_tokens(generation_messages.text())</code></pre>
</details>
</dd>
<dt id="glai.AutoAI.generate"><code class="name flex">
<span>def <span class="ident">generate</span></span>(<span>self, user_message: str, ai_message_tbc: Optional[str] = None, stop_at: Optional[str] = None, include_stop_str: bool = True, system_message: Optional[str] = None) ‑> <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Generate an AI response to a user message.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user_message</code></strong></dt>
<dd>User message text.</dd>
<dt><strong><code>ai_message_tbc</code></strong></dt>
<dd>Optional text to prepend.</dd>
<dt><strong><code>stop_at</code></strong></dt>
<dd>Optional string to stop generation at.</dd>
<dt><strong><code>include_stop_str</code></strong></dt>
<dd>Whether to include the stop string in the generated message.</dd>
<dt><strong><code>system_message</code></strong></dt>
<dd>Optional system message to include at the start, not all models support this.</dd>
</dl>
<p>If you provide system message to a model that doesn't support it, it will be ignored.
You can check if a model supports system messages by checking the model_data.has_system_tags() method.</p>
<h2 id="returns">Returns</h2>
<p>Generated AIMessage object.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate(
    self,
    user_message: str,
    ai_message_tbc: Optional[str] = None,
    stop_at:Optional[str] = None,
    include_stop_str:bool = True,
    system_message: Optional[str] = None
) -&gt; AIMessage:
    &#34;&#34;&#34;
    Generate an AI response to a user message.

    Args:
        user_message: User message text.
        ai_message_tbc: Optional text to prepend.
        stop_at: Optional string to stop generation at.
        include_stop_str: Whether to include the stop string in the generated message.
        system_message: Optional system message to include at the start, not all models support this.
        If you provide system message to a model that doesn&#39;t support it, it will be ignored.
        You can check if a model supports system messages by checking the model_data.has_system_tags() method.
    Returns:
        Generated AIMessage object.
    &#34;&#34;&#34;
    generation_messages = AIMessages(user_tags=self.model_data.user_tags, ai_tags=self.model_data.ai_tags, system_tags=self.model_data.system_tags)
    generation_messages.reset_messages()
    if self.model_data.has_system_tags():
        if system_message is not None:
            generation_messages.set_system_message(system_message)
        else:
            print(&#34;WARNING: Model seeps to support system messages, but no system message provided.&#34;)
    generation_messages.add_user_message(user_message)


    if ai_message_tbc is not None:
        generation_messages.add_message(
            ai_message_tbc, 
            self.msgs.ai_tag_open, 
            &#34;&#34;
        )
    print(f&#34;Promt: {generation_messages.text()}&#34;)
    
    generated = self.generate_from_literal_string(generation_messages.text(), stop_at=stop_at, include_stop_str=include_stop_str)

    if ai_message_tbc is not None:
        generation_messages.edit_last_message(
            ai_message_tbc + generated,
            self.msgs.ai_tag_open,
            self.msgs.ai_tag_close
        )
    else:
        generation_messages.add_ai_message(generated)

    print(f&#34;Generated: {generation_messages.get_last_message().text()}&#34;)
    output = generation_messages.get_last_message()
    return output</code></pre>
</details>
</dd>
<dt id="glai.AutoAI.generate_from_literal_string"><code class="name flex">
<span>def <span class="ident">generate_from_literal_string</span></span>(<span>self, prompt: str, stop_at: str = None, include_stop_str: bool = True) ‑> <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Generate text from a prompt using the LlamaAI model.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>prompt</code></strong></dt>
<dd>Prompt text to generate from.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Generated text string.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_from_literal_string(
    self, 
    prompt: str,
    stop_at:str = None,
    include_stop_str:bool = True
) -&gt; AIMessage:
    &#34;&#34;&#34;
    Generate text from a prompt using the LlamaAI model.

    Args:
        prompt: Prompt text to generate from.

    Returns:
        Generated text string.
    &#34;&#34;&#34;
    return self.ai.infer(
        prompt, only_string=True, stop_at_str=stop_at, include_stop_str=include_stop_str
    )</code></pre>
</details>
</dd>
<dt id="glai.AutoAI.generate_from_messages"><code class="name flex">
<span>def <span class="ident">generate_from_messages</span></span>(<span>self, stop_at: str = None, include_stop_str: bool = True) ‑> <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate_from_messages(self, stop_at:str = None, include_stop_str:bool = True) -&gt; AIMessage:
    prompt = self.msgs.text()
    ai_message = self.generate_from_literal_string(prompt, stop_at=stop_at, include_stop_str=include_stop_str)
    self.msgs.add_ai_message(ai_message)
    return ai_message</code></pre>
</details>
</dd>
<dt id="glai.AutoAI.is_within_input_limit"><code class="name flex">
<span>def <span class="ident">is_within_input_limit</span></span>(<span>self, user_message: str, ai_message_tbc: Optional[str] = None) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Check if the generated message is within the input limit.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user_message</code></strong></dt>
<dd>User message text.</dd>
<dt><strong><code>ai_message_tbc</code></strong></dt>
<dd>Optional text to prepend.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>True if within input limit, False otherwise.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_within_input_limit(
    self,
    user_message: str,
    ai_message_tbc: Optional[str] = None
    ) -&gt; bool:
    &#34;&#34;&#34;
    Check if the generated message is within the input limit.

    Args:
        user_message: User message text.
        ai_message_tbc: Optional text to prepend.

    Returns:
        True if within input limit, False otherwise.
    &#34;&#34;&#34;
    return self.ai.is_within_input_limit(self.count_tokens(user_message, ai_message_tbc))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="glai.EasyAI"><code class="flex name class">
<span>class <span class="ident">EasyAI</span></span>
<span>(</span><span>**kwds)</span>
</code></dt>
<dd>
<div class="desc"><p>EasyAI provides a simple interface for LlamaAI based AI using quantized GGUF models
for inference on CPU.</p>
<p>Initialization:
Can be intialized with no arguments, followed by following configuration methods, step by step, all in one or using a dict:</p>
<p>Step by step call these with appropriate arguments:</p>
<pre><code>1. `self.load_model_db(model_db_dir: str = MODEL_EXAMPLES_DB_DIR)`
2. One of the following with necessary args: &lt;code&gt;self.model\_data\_from\_url()&lt;/code&gt; or &lt;code&gt;self.model\_data\_from\_file()&lt;/code&gt; or &lt;code&gt;self.find\_model\_data()&lt;/code&gt;
3. `self.load_ai(max_total_tokens: int = 200)`
</code></pre>
<p>All in one:</p>
<pre><code>```python
self.configure(
    model_db_dir:str = MODEL_EXAMPLES_DB_DIR, 
    model_url: Optional[str] = None, 
    model_gguf_path: Optional[str] = None, 
    name_search: Optional[str] = None, 
    quantization_search: Optional[str] = None, 
    keyword_search: Optional[str] = None, 
    max_total_tokens: int = 200, 
    )
```

Or with a dictionary with the following keys:
- model_db_dir: Directory to store model data in. Defaults to MODEL_EXAMPLES_DB_DIR.
- model_url: URL of model to configure with. Automatically downloads and builds as needed. (Optional)
- name_search: Name of model to search for in the model db dir.(Optional)
- quantization_search: Quantization of model to search for in the model db dir..(Optional)
- keyword_search: Keyword of model to search for in the model db dir..(Optional)
- model_gguf_path: Path to GGUF file of model to configure with.(Optional, not a recommended method, doesn't preserve download url)
- max_total_tokens: Max tokens to be processed by LlamaAI model. (Defaults to 200, set to around 500-1k for regular use)
</code></pre>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>model_db</code></strong></dt>
<dd>ModelDB for searching/loading models</dd>
<dt><strong><code>messages</code></strong></dt>
<dd>AIMessages for tracking conversation </dd>
<dt><strong><code>model_data</code></strong></dt>
<dd>ModelData of selected model</dd>
<dt><strong><code>lai</code></strong></dt>
<dd>LlamaAI instance for generating text</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>DB:
load_model_db: Load ModelDB from directory
ModelData:
find_model_data: Search model DB for ModelData
model_data_from_url: Get ModelData from URL
model_data_from_file: Load ModelData from file
Load to memory:
load_ai: Create LlamaAI instance from ModelData
Inference:
infer: Generate AI response to user message</p>
<p>EasyAI handles loading models, setting up messages/LLamaAI,
and generating responses. It provides a simple interface to using
LLama</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class EasyAI:
    &#34;&#34;&#34;
    EasyAI provides a simple interface for LlamaAI based AI using quantized GGUF models
    for inference on CPU.
    
    Initialization:
    Can be intialized with no arguments, followed by following configuration methods, step by step, all in one or using a dict:\n
    Step by step call these with appropriate arguments:\n
        1. `self.load_model_db(model_db_dir: str = MODEL_EXAMPLES_DB_DIR)`
        2. One of the following with necessary args: `self.model_data_from_url()` or `self.model_data_from_file()` or `self.find_model_data()`
        3. `self.load_ai(max_total_tokens: int = 200)`
    All in one:\n
        ```python
        self.configure(
            model_db_dir:str = MODEL_EXAMPLES_DB_DIR, 
            model_url: Optional[str] = None, 
            model_gguf_path: Optional[str] = None, 
            name_search: Optional[str] = None, 
            quantization_search: Optional[str] = None, 
            keyword_search: Optional[str] = None, 
            max_total_tokens: int = 200, 
            )
        ```
        
        Or with a dictionary with the following keys:
        - model_db_dir: Directory to store model data in. Defaults to MODEL_EXAMPLES_DB_DIR.
        - model_url: URL of model to configure with. Automatically downloads and builds as needed. (Optional)
        - name_search: Name of model to search for in the model db dir.(Optional)
        - quantization_search: Quantization of model to search for in the model db dir..(Optional)
        - keyword_search: Keyword of model to search for in the model db dir..(Optional)
        - model_gguf_path: Path to GGUF file of model to configure with.(Optional, not a recommended method, doesn&#39;t preserve download url)
        - max_total_tokens: Max tokens to be processed by LlamaAI model. (Defaults to 200, set to around 500-1k for regular use)

    Attributes:
        model_db: ModelDB for searching/loading models
        messages: AIMessages for tracking conversation 
        model_data: ModelData of selected model
        lai: LlamaAI instance for generating text

    Methods:
        DB:
            load_model_db: Load ModelDB from directory
        ModelData:
            find_model_data: Search model DB for ModelData
            model_data_from_url: Get ModelData from URL
            model_data_from_file: Load ModelData from file
        Load to memory:
            load_ai: Create LlamaAI instance from ModelData
        Inference:
            infer: Generate AI response to user message

    EasyAI handles loading models, setting up messages/LLamaAI,
    and generating responses. It provides a simple interface to using
    LLama
    &#34;&#34;&#34;
    def __init__(self, **kwds) -&gt; None:
        self.model_db: ModelDB = None
        self.messages: Optional[AIMessages] = None
        self.model_data: Optional[ModelData] = None
        self.ai: Optional[LlamaAI] = None
        if kwds:
            self.configure(**kwds)

    def configure(self,
                  model_db_dir: Optional[str] = None,
                  model_url: Optional[str] = None,
                  model_gguf_path: Optional[str] = None,
                  name_search: Optional[str] = None,
                  quantization_search: Optional[str] = None,
                  keyword_search: Optional[str] = None,
                  search_only_downloaded: bool = False,
                  max_total_tokens: int = 200,
                                            ) -&gt; None:
        &#34;&#34;&#34;
        Configure EasyAI with model data.

        Configures EasyAI with model data from given URL, GGUF file path, or model name/quantization/keyword.
        Sets model data attribute.

        Args:
            model_db_dir: Directory to store model data in. If none is provided global db is used.This is preferred for most use cases.
            max_total_tokens: Max tokens to be processed (input+generation) by LlamaAI model. (Defaults to 200, set to around 500-1k for regular use)
            
            Provide at least one of these args to fetch ModelData: 
            ---
            model_url: URL of model to configure with. Automatically downloads and builds as needed. (Optional)
            name_search: Name of model to search for in the model db dir.(Optional)
            quantization_search: Quantization of model to search for in the model db dir..(Optional)
            keyword_search: Keyword of model to search for in the model db dir..(Optional)
            model_gguf_path: Path to GGUF file of model to configure with.(Optional, not a recommended method, doesn&#39;t preserve download url)
            ---

        Raises:
            Exception: If no model DB loaded.
            Exception: If no model data found.

        &#34;&#34;&#34;
        if model_db_dir is None:
            print(f&#34;Using provided verified models DB, files at {VERIFIED_MODELS_DB_DIR}&#34;)
        self.load_model_db(model_db_dir)
        if model_url is not None:
            self.model_data_from_url(model_url)
        elif model_gguf_path is not None:
            self.model_data_from_file(model_gguf_path)
        elif name_search is not None or quantization_search is not None or keyword_search is not None:
            self.find_model_data(name_search, quantization_search, keyword_search, search_only_downloaded)
        else:
            raise Exception(&#34;Can&#39;t find model data. Please provide a model URL, GGUF file path, or model name/quantization/keyword.&#34;)
        
        self.load_ai(max_total_tokens)
    


            
            


    def load_model_db(self, db_dir:Optional[str] = None, copy_verified_models=True) -&gt; None:
        &#34;&#34;&#34;
        Load ModelDB from given directory.

        Args:
            db_dir: Directory to load ModelDB from.
            copy_examples: Whether to copy example GGUF files to db_dir if db_dir is empty.
        &#34;&#34;&#34;
        self.model_db = ModelDB(model_db_dir=db_dir, copy_verified_models=copy_verified_models)

    def import_verified_models_to_db(self, model_name_quantization_list:Optional[list[Union[list,set,tuple]]] = None) -&gt; None:
        &#34;&#34;&#34;
        Import verified models to new ModelDB.

        Imports verified models from global verified models DB to new ModelDB. If model_name_quantization_list is provided, only models in the list are imported.

        Args:
            model_name_quantization_list: List of tuples of model name and quantization to import. If None, all models are imported.
        &#34;&#34;&#34;
        if self.model_db is None:
            raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
        if model_name_quantization_list is not None:
            for n_q_data in model_name_quantization_list:
                name = n_q_data[0]
                quantization = n_q_data[1]
                self.model_db.import_verified_model(name, quantization, None, True)
        else:
            self.model_db.import_verified_model(None, None, None, True)
        
    def find_model_data(self,
                        model_name: Optional[str] = None,  
                        quantization: Optional[str] = None,
                        keyword: Optional[str] = None,
                        only_downloaded: bool = False) -&gt; ModelData:
        &#34;&#34;&#34;
        Find model data in database that matches given criteria.

        Searches model database for model data matching the given model name, 
        quantization, and/or keyword. Any parameters left as None are not used  
        in the search.

        Args:
            model_name: Name of model to search for.
            quantization: Quantization of model to search for.
            keyword: Keyword of model to search for.

        Returns:
            ModelData object if a match is found, else None.
        &#34;&#34;&#34;
        if self.model_db is None:
            raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
        model_data = self.model_db.find_model(model_name, quantization, keyword, only_downloaded)
        self.model_data = model_data
        return model_data

    def model_data_from_url(self,
                            url: str,
                            user_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                            ai_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                            description: Optional[str] = None,
                            keywords: Optional[str] = None,
                            save: bool = True) -&gt; None:
        &#34;&#34;&#34;
        Get model data for URL, downloading model if needed.

        Checks if model data already exists for the given URL. If not, downloads
        the model from the URL and creates new model data. Sets model data attribute.

        Args:
            url: URL of model to get data for.
            user_tags: User tags to assign if creating new model data. 
            ai_tags: AI tags to assign if creating new model data.
            description: Optional description for new model data.
            keyword: Optional keyword for new model data.
            save: Whether to save new model data JSON file.
        &#34;&#34;&#34;
        if self.model_db is None:
            raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
        print(f&#34;Trying to get model data from url: {url}&#34;)
        print(f&#34;Checking if model data already exists...&#34;)
        model_data = self.model_db.get_model_by_url(url)
        if model_data is None:
            print(f&#34;Model data not found. Creating new model data...&#34;)
            model_data = ModelData(url, self.model_db.gguf_db_dir, user_tags, ai_tags, description, keywords)
            print(f&#34;Created model data: {model_data}&#34;)
        else:
            print(f&#34;Found model data: {model_data}&#34;)
        if save:
            model_data.save_json()
        self.model_data = model_data

    def model_data_from_file(self,
                             gguf_file_path: str,
                             user_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                             ai_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                             description: Optional[str] = None,
                             keyword: Optional[str] = None,
                             save: bool = False) -&gt; None:
        &#34;&#34;&#34;
        Get model data from local GGUF file.

        Loads model data from the given local GGUF file path. Sets model data attribute.

        Args:
            gguf_file_path: Path to GGUF file.
            user_tags: User tags to assign to model data.
            ai_tags: AI tags to assign to model data.
            description: Optional description for model data.
            keyword: Optional keyword for model data.
            save: Whether to save model data JSON file.
        &#34;&#34;&#34;
        if self.model_db is None:
            raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
        model_data = ModelData.from_file(gguf_file_path, self.model_db.gguf_db_dir, user_tags, ai_tags, description, keyword)
        if save:
            model_data.save_json()
        self.model_data = model_data

    def _load_messages(self) -&gt; None:
        &#34;&#34;&#34;
        Load AIMessages using tags from model data.

        Uses user_tags and ai_tags from loaded model data to initialize AIMessages object. 
        Sets messages attribute.

        Raises:
            Exception: If no model data loaded yet.
        &#34;&#34;&#34;
        if self.model_data is None:
            raise Exception(&#34;No model data loaded. Use find_model_data(), get_model_data_from_url(), or get_model_data_from_file() first.&#34;)
        self.messages = AIMessages(user_tags=self.model_data.user_tags, ai_tags=self.model_data.ai_tags, system_tags=self.model_data.system_tags)

    def load_ai(self,
                max_total_tokens: int = 200,) -&gt; None:
        &#34;&#34;&#34;
        Load LlamaAI model from model data.

        Downloads model file from model data URL if needed. Initializes LlamaAI with model and sets lai attribute.

        Args:
            max_total_tokens: Max tokens for LlamaAI model.
        Raises:
            Exception: If no model data or messages loaded yet.
        &#34;&#34;&#34;
        self._load_messages()
        if self.messages is None:
            raise Exception(&#34;No messages loaded. Use load_messages() first.&#34;)
        if self.model_data is None:
            raise Exception(&#34;No model data loaded. Use find_model_data(), get_model_data_from_url(), or get_model_data_from_file() first.&#34;)
        self.model_data.download_gguf()
        self.ai = LlamaAI(self.model_data.model_path(), max_tokens=max_total_tokens)
        print(f&#34;Loaded: {self.model_data}&#34;)

    def generate(self,
              user_message: str,
              ai_message_tbc: Optional[str] = None,
              stop_at:Optional[str]=None,
              include_stop_str:bool=True,
              system_message: Optional[str] = None
              ) -&gt; AIMessage:
        &#34;&#34;&#34;
        Generate AI response to user message.

        Runs user message through loaded LlamaAI to generate response. Allows prepending optional 
        content to AI response. Adds messages and returns generated AIMessage.

        Args:
            user_message: User message text.
            ai_message_tbc: Optional text to prepend to AI response.
            stop_at: Optional string to stop generation at.
            include_stop_str: Whether to include stop string in generated message.
            system_message: Optional system message to include at the start, not all models support this.
            If you provide system message to a model that doesn&#39;t support it, it will be ignored.
            You can check if a model supports system messages by checking the model_data.has_system_messages()

        Returns:
            Generated AIMessage object.

        Raises:
            Exception: If no AI or messages loaded yet.
        &#34;&#34;&#34;
        if self.ai is None:
            raise Exception(&#34;No AI loaded. Use load_ai() first.&#34;)
        if self.messages is None:
            raise Exception(&#34;No messages loaded. Use load_ai() first.&#34;)
        self.messages.reset_messages()
        if self.model_data.has_system_tags():
            if system_message is not None:
                self.messages.set_system_message(system_message)
            else:
                print(&#34;WARNING: Model supports system messages, but no system message provided.&#34;)
        self.messages.add_user_message(user_message)
        print(f&#34;Input to model: \n{self.messages.text()}&#34;)
        generated: str = &#34;&#34;
        if ai_message_tbc is not None:
            generated += ai_message_tbc
            self.messages.add_message(ai_message_tbc, self.model_data.get_ai_tag_open(), &#34;&#34;)
        if stop_at is None:
            stop_at = self.messages.ai_tag_close if any([self.messages.ai_tag_close is None, self.messages.ai_tag_close == &#34;&#34;, self.messages.ai_tag_close != &#34; &#34;]) else None
            include_stop_str = False
        generated += self.ai.infer(self.messages.text(), only_string=True, stop_at_str=stop_at, include_stop_str=include_stop_str)
        if ai_message_tbc is not None:
            self.messages.edit_last_message(generated,
                                            self.model_data.get_ai_tag_open(),
                                            self.model_data.get_ai_tag_close())
        else:
            self.messages.add_ai_message(generated)
        print(f&#34;AI message: \n{self.messages.get_last_message()}&#34;)
        return self.messages.get_last_message()

    



    def count_tokens(
        self,
        user_message_text: str,
        ai_message_tbc: Optional[str] = None
    ) -&gt; int:
        &#34;&#34;&#34;
        Count the number of tokens in a generated message.

        Args:
            user_message_text: User message text.
            ai_message_tbc: Optional text to prepend.

        Returns:
            Number of tokens in generated message.
        &#34;&#34;&#34;
        generation_messages = AIMessages()
        generation_messages.reset_messages()
        generation_messages.add_user_message(user_message_text)

        if ai_message_tbc is not None:
            generation_messages.add_message(
                ai_message_tbc, 
                self.messages.ai_tag_open, 
                &#34;&#34;
            )

        return self.ai.count_tokens(generation_messages.text())
    
    def is_within_context(self,
        prompt: str,
        ) -&gt; bool:
        &#34;&#34;&#34;
        Check if the generated message is within the input limit.

        Args:
            user_message_text: User message text.
            ai_message_tbc: Optional text to prepend.

        Returns:
            True if within input limit, False otherwise.
        &#34;&#34;&#34;
        return self.ai.is_prompt_within_limit(prompt)
    
    def import_from_repo(self, hf_repo_url: str, user_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;), ai_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;), system_tags: Optional[Tuple[str, str]] = (None, None), keywords: Optional[str] = None, description: Optional[str] = None, replace_existing: bool = False) -&gt; None:
        &#34;&#34;&#34;
        Imports model data from HuggingFace model repo to current model DB. 

        Args:
            hf_repo_url: URL of model to import.
            user_tags: User tags to assign to model data.
            ai_tags: AI tags to assign to model data.
            system_tags: System tags to assign to model data.
            description: Optional description for model data.
            keyword: Optional keyword for model data.
            replace_existing: Whether to replace existing model data if found.

        Raises:
            Exception: If no model DB loaded yet.
        &#34;&#34;&#34;



        self.model_db.import_models_from_repo(
            hf_repo_url=hf_repo_url,
            user_tags=user_tags,
            ai_tags=ai_tags,
            system_tags=system_tags,
            keywords=keywords,
            description=description,
            replace_existing=replace_existing,
        )</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="glai.EasyAI.configure"><code class="name flex">
<span>def <span class="ident">configure</span></span>(<span>self, model_db_dir: Optional[str] = None, model_url: Optional[str] = None, model_gguf_path: Optional[str] = None, name_search: Optional[str] = None, quantization_search: Optional[str] = None, keyword_search: Optional[str] = None, search_only_downloaded: bool = False, max_total_tokens: int = 200) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Configure EasyAI with model data.</p>
<p>Configures EasyAI with model data from given URL, GGUF file path, or model name/quantization/keyword.
Sets model data attribute.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_db_dir</code></strong></dt>
<dd>Directory to store model data in. If none is provided global db is used.This is preferred for most use cases.</dd>
<dt><strong><code>max_total_tokens</code></strong></dt>
<dd>Max tokens to be processed (input+generation) by LlamaAI model. (Defaults to 200, set to around 500-1k for regular use)</dd>
</dl>
<h2 id="provide-at-least-one-of-these-args-to-fetch-modeldata">Provide at least one of these args to fetch ModelData:</h2>
<dl>
<dt><strong><code>model_url</code></strong></dt>
<dd>URL of model to configure with. Automatically downloads and builds as needed. (Optional)</dd>
<dt><strong><code>name_search</code></strong></dt>
<dd>Name of model to search for in the model db dir.(Optional)</dd>
<dt><strong><code>quantization_search</code></strong></dt>
<dd>Quantization of model to search for in the model db dir..(Optional)</dd>
<dt><strong><code>keyword_search</code></strong></dt>
<dd>Keyword of model to search for in the model db dir..(Optional)</dd>
<dt><strong><code>model_gguf_path</code></strong></dt>
<dd>Path to GGUF file of model to configure with.(Optional, not a recommended method, doesn't preserve download url)</dd>
</dl>
<hr>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If no model DB loaded.</dd>
<dt><code>Exception</code></dt>
<dd>If no model data found.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def configure(self,
              model_db_dir: Optional[str] = None,
              model_url: Optional[str] = None,
              model_gguf_path: Optional[str] = None,
              name_search: Optional[str] = None,
              quantization_search: Optional[str] = None,
              keyword_search: Optional[str] = None,
              search_only_downloaded: bool = False,
              max_total_tokens: int = 200,
                                        ) -&gt; None:
    &#34;&#34;&#34;
    Configure EasyAI with model data.

    Configures EasyAI with model data from given URL, GGUF file path, or model name/quantization/keyword.
    Sets model data attribute.

    Args:
        model_db_dir: Directory to store model data in. If none is provided global db is used.This is preferred for most use cases.
        max_total_tokens: Max tokens to be processed (input+generation) by LlamaAI model. (Defaults to 200, set to around 500-1k for regular use)
        
        Provide at least one of these args to fetch ModelData: 
        ---
        model_url: URL of model to configure with. Automatically downloads and builds as needed. (Optional)
        name_search: Name of model to search for in the model db dir.(Optional)
        quantization_search: Quantization of model to search for in the model db dir..(Optional)
        keyword_search: Keyword of model to search for in the model db dir..(Optional)
        model_gguf_path: Path to GGUF file of model to configure with.(Optional, not a recommended method, doesn&#39;t preserve download url)
        ---

    Raises:
        Exception: If no model DB loaded.
        Exception: If no model data found.

    &#34;&#34;&#34;
    if model_db_dir is None:
        print(f&#34;Using provided verified models DB, files at {VERIFIED_MODELS_DB_DIR}&#34;)
    self.load_model_db(model_db_dir)
    if model_url is not None:
        self.model_data_from_url(model_url)
    elif model_gguf_path is not None:
        self.model_data_from_file(model_gguf_path)
    elif name_search is not None or quantization_search is not None or keyword_search is not None:
        self.find_model_data(name_search, quantization_search, keyword_search, search_only_downloaded)
    else:
        raise Exception(&#34;Can&#39;t find model data. Please provide a model URL, GGUF file path, or model name/quantization/keyword.&#34;)
    
    self.load_ai(max_total_tokens)</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.count_tokens"><code class="name flex">
<span>def <span class="ident">count_tokens</span></span>(<span>self, user_message_text: str, ai_message_tbc: Optional[str] = None) ‑> int</span>
</code></dt>
<dd>
<div class="desc"><p>Count the number of tokens in a generated message.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user_message_text</code></strong></dt>
<dd>User message text.</dd>
<dt><strong><code>ai_message_tbc</code></strong></dt>
<dd>Optional text to prepend.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Number of tokens in generated message.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def count_tokens(
    self,
    user_message_text: str,
    ai_message_tbc: Optional[str] = None
) -&gt; int:
    &#34;&#34;&#34;
    Count the number of tokens in a generated message.

    Args:
        user_message_text: User message text.
        ai_message_tbc: Optional text to prepend.

    Returns:
        Number of tokens in generated message.
    &#34;&#34;&#34;
    generation_messages = AIMessages()
    generation_messages.reset_messages()
    generation_messages.add_user_message(user_message_text)

    if ai_message_tbc is not None:
        generation_messages.add_message(
            ai_message_tbc, 
            self.messages.ai_tag_open, 
            &#34;&#34;
        )

    return self.ai.count_tokens(generation_messages.text())</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.find_model_data"><code class="name flex">
<span>def <span class="ident">find_model_data</span></span>(<span>self, model_name: Optional[str] = None, quantization: Optional[str] = None, keyword: Optional[str] = None, only_downloaded: bool = False) ‑> gguf_modeldb.model_data.ModelData</span>
</code></dt>
<dd>
<div class="desc"><p>Find model data in database that matches given criteria.</p>
<p>Searches model database for model data matching the given model name,
quantization, and/or keyword. Any parameters left as None are not used<br>
in the search.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_name</code></strong></dt>
<dd>Name of model to search for.</dd>
<dt><strong><code>quantization</code></strong></dt>
<dd>Quantization of model to search for.</dd>
<dt><strong><code>keyword</code></strong></dt>
<dd>Keyword of model to search for.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>ModelData object if a match is found, else None.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def find_model_data(self,
                    model_name: Optional[str] = None,  
                    quantization: Optional[str] = None,
                    keyword: Optional[str] = None,
                    only_downloaded: bool = False) -&gt; ModelData:
    &#34;&#34;&#34;
    Find model data in database that matches given criteria.

    Searches model database for model data matching the given model name, 
    quantization, and/or keyword. Any parameters left as None are not used  
    in the search.

    Args:
        model_name: Name of model to search for.
        quantization: Quantization of model to search for.
        keyword: Keyword of model to search for.

    Returns:
        ModelData object if a match is found, else None.
    &#34;&#34;&#34;
    if self.model_db is None:
        raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
    model_data = self.model_db.find_model(model_name, quantization, keyword, only_downloaded)
    self.model_data = model_data
    return model_data</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.generate"><code class="name flex">
<span>def <span class="ident">generate</span></span>(<span>self, user_message: str, ai_message_tbc: Optional[str] = None, stop_at: Optional[str] = None, include_stop_str: bool = True, system_message: Optional[str] = None) ‑> <a title="glai.messages.messages.AIMessage" href="messages/messages.html#glai.messages.messages.AIMessage">AIMessage</a></span>
</code></dt>
<dd>
<div class="desc"><p>Generate AI response to user message.</p>
<p>Runs user message through loaded LlamaAI to generate response. Allows prepending optional
content to AI response. Adds messages and returns generated AIMessage.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user_message</code></strong></dt>
<dd>User message text.</dd>
<dt><strong><code>ai_message_tbc</code></strong></dt>
<dd>Optional text to prepend to AI response.</dd>
<dt><strong><code>stop_at</code></strong></dt>
<dd>Optional string to stop generation at.</dd>
<dt><strong><code>include_stop_str</code></strong></dt>
<dd>Whether to include stop string in generated message.</dd>
<dt><strong><code>system_message</code></strong></dt>
<dd>Optional system message to include at the start, not all models support this.</dd>
</dl>
<p>If you provide system message to a model that doesn't support it, it will be ignored.
You can check if a model supports system messages by checking the model_data.has_system_messages()</p>
<h2 id="returns">Returns</h2>
<p>Generated AIMessage object.</p>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If no AI or messages loaded yet.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def generate(self,
          user_message: str,
          ai_message_tbc: Optional[str] = None,
          stop_at:Optional[str]=None,
          include_stop_str:bool=True,
          system_message: Optional[str] = None
          ) -&gt; AIMessage:
    &#34;&#34;&#34;
    Generate AI response to user message.

    Runs user message through loaded LlamaAI to generate response. Allows prepending optional 
    content to AI response. Adds messages and returns generated AIMessage.

    Args:
        user_message: User message text.
        ai_message_tbc: Optional text to prepend to AI response.
        stop_at: Optional string to stop generation at.
        include_stop_str: Whether to include stop string in generated message.
        system_message: Optional system message to include at the start, not all models support this.
        If you provide system message to a model that doesn&#39;t support it, it will be ignored.
        You can check if a model supports system messages by checking the model_data.has_system_messages()

    Returns:
        Generated AIMessage object.

    Raises:
        Exception: If no AI or messages loaded yet.
    &#34;&#34;&#34;
    if self.ai is None:
        raise Exception(&#34;No AI loaded. Use load_ai() first.&#34;)
    if self.messages is None:
        raise Exception(&#34;No messages loaded. Use load_ai() first.&#34;)
    self.messages.reset_messages()
    if self.model_data.has_system_tags():
        if system_message is not None:
            self.messages.set_system_message(system_message)
        else:
            print(&#34;WARNING: Model supports system messages, but no system message provided.&#34;)
    self.messages.add_user_message(user_message)
    print(f&#34;Input to model: \n{self.messages.text()}&#34;)
    generated: str = &#34;&#34;
    if ai_message_tbc is not None:
        generated += ai_message_tbc
        self.messages.add_message(ai_message_tbc, self.model_data.get_ai_tag_open(), &#34;&#34;)
    if stop_at is None:
        stop_at = self.messages.ai_tag_close if any([self.messages.ai_tag_close is None, self.messages.ai_tag_close == &#34;&#34;, self.messages.ai_tag_close != &#34; &#34;]) else None
        include_stop_str = False
    generated += self.ai.infer(self.messages.text(), only_string=True, stop_at_str=stop_at, include_stop_str=include_stop_str)
    if ai_message_tbc is not None:
        self.messages.edit_last_message(generated,
                                        self.model_data.get_ai_tag_open(),
                                        self.model_data.get_ai_tag_close())
    else:
        self.messages.add_ai_message(generated)
    print(f&#34;AI message: \n{self.messages.get_last_message()}&#34;)
    return self.messages.get_last_message()</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.import_from_repo"><code class="name flex">
<span>def <span class="ident">import_from_repo</span></span>(<span>self, hf_repo_url: str, user_tags: Tuple[str, str] = ('', ''), ai_tags: Tuple[str, str] = ('', ''), system_tags: Optional[Tuple[str, str]] = (None, None), keywords: Optional[str] = None, description: Optional[str] = None, replace_existing: bool = False) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Imports model data from HuggingFace model repo to current model DB. </p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>hf_repo_url</code></strong></dt>
<dd>URL of model to import.</dd>
<dt><strong><code>user_tags</code></strong></dt>
<dd>User tags to assign to model data.</dd>
<dt><strong><code>ai_tags</code></strong></dt>
<dd>AI tags to assign to model data.</dd>
<dt><strong><code>system_tags</code></strong></dt>
<dd>System tags to assign to model data.</dd>
<dt><strong><code>description</code></strong></dt>
<dd>Optional description for model data.</dd>
<dt><strong><code>keyword</code></strong></dt>
<dd>Optional keyword for model data.</dd>
<dt><strong><code>replace_existing</code></strong></dt>
<dd>Whether to replace existing model data if found.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If no model DB loaded yet.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def import_from_repo(self, hf_repo_url: str, user_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;), ai_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;), system_tags: Optional[Tuple[str, str]] = (None, None), keywords: Optional[str] = None, description: Optional[str] = None, replace_existing: bool = False) -&gt; None:
    &#34;&#34;&#34;
    Imports model data from HuggingFace model repo to current model DB. 

    Args:
        hf_repo_url: URL of model to import.
        user_tags: User tags to assign to model data.
        ai_tags: AI tags to assign to model data.
        system_tags: System tags to assign to model data.
        description: Optional description for model data.
        keyword: Optional keyword for model data.
        replace_existing: Whether to replace existing model data if found.

    Raises:
        Exception: If no model DB loaded yet.
    &#34;&#34;&#34;



    self.model_db.import_models_from_repo(
        hf_repo_url=hf_repo_url,
        user_tags=user_tags,
        ai_tags=ai_tags,
        system_tags=system_tags,
        keywords=keywords,
        description=description,
        replace_existing=replace_existing,
    )</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.import_verified_models_to_db"><code class="name flex">
<span>def <span class="ident">import_verified_models_to_db</span></span>(<span>self, model_name_quantization_list: Optional[list[typing.Union[list, set, tuple]]] = None) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Import verified models to new ModelDB.</p>
<p>Imports verified models from global verified models DB to new ModelDB. If model_name_quantization_list is provided, only models in the list are imported.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>model_name_quantization_list</code></strong></dt>
<dd>List of tuples of model name and quantization to import. If None, all models are imported.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def import_verified_models_to_db(self, model_name_quantization_list:Optional[list[Union[list,set,tuple]]] = None) -&gt; None:
    &#34;&#34;&#34;
    Import verified models to new ModelDB.

    Imports verified models from global verified models DB to new ModelDB. If model_name_quantization_list is provided, only models in the list are imported.

    Args:
        model_name_quantization_list: List of tuples of model name and quantization to import. If None, all models are imported.
    &#34;&#34;&#34;
    if self.model_db is None:
        raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
    if model_name_quantization_list is not None:
        for n_q_data in model_name_quantization_list:
            name = n_q_data[0]
            quantization = n_q_data[1]
            self.model_db.import_verified_model(name, quantization, None, True)
    else:
        self.model_db.import_verified_model(None, None, None, True)</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.is_within_context"><code class="name flex">
<span>def <span class="ident">is_within_context</span></span>(<span>self, prompt: str) ‑> bool</span>
</code></dt>
<dd>
<div class="desc"><p>Check if the generated message is within the input limit.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>user_message_text</code></strong></dt>
<dd>User message text.</dd>
<dt><strong><code>ai_message_tbc</code></strong></dt>
<dd>Optional text to prepend.</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>True if within input limit, False otherwise.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def is_within_context(self,
    prompt: str,
    ) -&gt; bool:
    &#34;&#34;&#34;
    Check if the generated message is within the input limit.

    Args:
        user_message_text: User message text.
        ai_message_tbc: Optional text to prepend.

    Returns:
        True if within input limit, False otherwise.
    &#34;&#34;&#34;
    return self.ai.is_prompt_within_limit(prompt)</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.load_ai"><code class="name flex">
<span>def <span class="ident">load_ai</span></span>(<span>self, max_total_tokens: int = 200) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Load LlamaAI model from model data.</p>
<p>Downloads model file from model data URL if needed. Initializes LlamaAI with model and sets lai attribute.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>max_total_tokens</code></strong></dt>
<dd>Max tokens for LlamaAI model.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>Exception</code></dt>
<dd>If no model data or messages loaded yet.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_ai(self,
            max_total_tokens: int = 200,) -&gt; None:
    &#34;&#34;&#34;
    Load LlamaAI model from model data.

    Downloads model file from model data URL if needed. Initializes LlamaAI with model and sets lai attribute.

    Args:
        max_total_tokens: Max tokens for LlamaAI model.
    Raises:
        Exception: If no model data or messages loaded yet.
    &#34;&#34;&#34;
    self._load_messages()
    if self.messages is None:
        raise Exception(&#34;No messages loaded. Use load_messages() first.&#34;)
    if self.model_data is None:
        raise Exception(&#34;No model data loaded. Use find_model_data(), get_model_data_from_url(), or get_model_data_from_file() first.&#34;)
    self.model_data.download_gguf()
    self.ai = LlamaAI(self.model_data.model_path(), max_tokens=max_total_tokens)
    print(f&#34;Loaded: {self.model_data}&#34;)</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.load_model_db"><code class="name flex">
<span>def <span class="ident">load_model_db</span></span>(<span>self, db_dir: Optional[str] = None, copy_verified_models=True) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Load ModelDB from given directory.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>db_dir</code></strong></dt>
<dd>Directory to load ModelDB from.</dd>
<dt><strong><code>copy_examples</code></strong></dt>
<dd>Whether to copy example GGUF files to db_dir if db_dir is empty.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def load_model_db(self, db_dir:Optional[str] = None, copy_verified_models=True) -&gt; None:
    &#34;&#34;&#34;
    Load ModelDB from given directory.

    Args:
        db_dir: Directory to load ModelDB from.
        copy_examples: Whether to copy example GGUF files to db_dir if db_dir is empty.
    &#34;&#34;&#34;
    self.model_db = ModelDB(model_db_dir=db_dir, copy_verified_models=copy_verified_models)</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.model_data_from_file"><code class="name flex">
<span>def <span class="ident">model_data_from_file</span></span>(<span>self, gguf_file_path: str, user_tags: Tuple[str, str] = ('', ''), ai_tags: Tuple[str, str] = ('', ''), description: Optional[str] = None, keyword: Optional[str] = None, save: bool = False) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Get model data from local GGUF file.</p>
<p>Loads model data from the given local GGUF file path. Sets model data attribute.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gguf_file_path</code></strong></dt>
<dd>Path to GGUF file.</dd>
<dt><strong><code>user_tags</code></strong></dt>
<dd>User tags to assign to model data.</dd>
<dt><strong><code>ai_tags</code></strong></dt>
<dd>AI tags to assign to model data.</dd>
<dt><strong><code>description</code></strong></dt>
<dd>Optional description for model data.</dd>
<dt><strong><code>keyword</code></strong></dt>
<dd>Optional keyword for model data.</dd>
<dt><strong><code>save</code></strong></dt>
<dd>Whether to save model data JSON file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_data_from_file(self,
                         gguf_file_path: str,
                         user_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                         ai_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                         description: Optional[str] = None,
                         keyword: Optional[str] = None,
                         save: bool = False) -&gt; None:
    &#34;&#34;&#34;
    Get model data from local GGUF file.

    Loads model data from the given local GGUF file path. Sets model data attribute.

    Args:
        gguf_file_path: Path to GGUF file.
        user_tags: User tags to assign to model data.
        ai_tags: AI tags to assign to model data.
        description: Optional description for model data.
        keyword: Optional keyword for model data.
        save: Whether to save model data JSON file.
    &#34;&#34;&#34;
    if self.model_db is None:
        raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
    model_data = ModelData.from_file(gguf_file_path, self.model_db.gguf_db_dir, user_tags, ai_tags, description, keyword)
    if save:
        model_data.save_json()
    self.model_data = model_data</code></pre>
</details>
</dd>
<dt id="glai.EasyAI.model_data_from_url"><code class="name flex">
<span>def <span class="ident">model_data_from_url</span></span>(<span>self, url: str, user_tags: Tuple[str, str] = ('', ''), ai_tags: Tuple[str, str] = ('', ''), description: Optional[str] = None, keywords: Optional[str] = None, save: bool = True) ‑> None</span>
</code></dt>
<dd>
<div class="desc"><p>Get model data for URL, downloading model if needed.</p>
<p>Checks if model data already exists for the given URL. If not, downloads
the model from the URL and creates new model data. Sets model data attribute.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>url</code></strong></dt>
<dd>URL of model to get data for.</dd>
<dt><strong><code>user_tags</code></strong></dt>
<dd>User tags to assign if creating new model data. </dd>
<dt><strong><code>ai_tags</code></strong></dt>
<dd>AI tags to assign if creating new model data.</dd>
<dt><strong><code>description</code></strong></dt>
<dd>Optional description for new model data.</dd>
<dt><strong><code>keyword</code></strong></dt>
<dd>Optional keyword for new model data.</dd>
<dt><strong><code>save</code></strong></dt>
<dd>Whether to save new model data JSON file.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def model_data_from_url(self,
                        url: str,
                        user_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                        ai_tags: Tuple[str, str] = (&#34;&#34;, &#34;&#34;),
                        description: Optional[str] = None,
                        keywords: Optional[str] = None,
                        save: bool = True) -&gt; None:
    &#34;&#34;&#34;
    Get model data for URL, downloading model if needed.

    Checks if model data already exists for the given URL. If not, downloads
    the model from the URL and creates new model data. Sets model data attribute.

    Args:
        url: URL of model to get data for.
        user_tags: User tags to assign if creating new model data. 
        ai_tags: AI tags to assign if creating new model data.
        description: Optional description for new model data.
        keyword: Optional keyword for new model data.
        save: Whether to save new model data JSON file.
    &#34;&#34;&#34;
    if self.model_db is None:
        raise Exception(&#34;No model DB loaded. Use load_model_db() first.&#34;)
    print(f&#34;Trying to get model data from url: {url}&#34;)
    print(f&#34;Checking if model data already exists...&#34;)
    model_data = self.model_db.get_model_by_url(url)
    if model_data is None:
        print(f&#34;Model data not found. Creating new model data...&#34;)
        model_data = ModelData(url, self.model_db.gguf_db_dir, user_tags, ai_tags, description, keywords)
        print(f&#34;Created model data: {model_data}&#34;)
    else:
        print(f&#34;Found model data: {model_data}&#34;)
    if save:
        model_data.save_json()
    self.model_data = model_data</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-submodules">Sub-modules</a></h3>
<ul>
<li><code><a title="glai.ai" href="ai/index.html">glai.ai</a></code></li>
<li><code><a title="glai.messages" href="messages/index.html">glai.messages</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="glai.AIMessage" href="#glai.AIMessage">AIMessage</a></code></h4>
<ul class="">
<li><code><a title="glai.AIMessage.edit" href="#glai.AIMessage.edit">edit</a></code></li>
<li><code><a title="glai.AIMessage.from_dict" href="#glai.AIMessage.from_dict">from_dict</a></code></li>
<li><code><a title="glai.AIMessage.get_tags" href="#glai.AIMessage.get_tags">get_tags</a></code></li>
<li><code><a title="glai.AIMessage.text" href="#glai.AIMessage.text">text</a></code></li>
<li><code><a title="glai.AIMessage.to_dict" href="#glai.AIMessage.to_dict">to_dict</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="glai.AIMessages" href="#glai.AIMessages">AIMessages</a></code></h4>
<ul class="">
<li><code><a title="glai.AIMessages.add_ai_message" href="#glai.AIMessages.add_ai_message">add_ai_message</a></code></li>
<li><code><a title="glai.AIMessages.add_message" href="#glai.AIMessages.add_message">add_message</a></code></li>
<li><code><a title="glai.AIMessages.add_user_message" href="#glai.AIMessages.add_user_message">add_user_message</a></code></li>
<li><code><a title="glai.AIMessages.ai_tags" href="#glai.AIMessages.ai_tags">ai_tags</a></code></li>
<li><code><a title="glai.AIMessages.create_single_message" href="#glai.AIMessages.create_single_message">create_single_message</a></code></li>
<li><code><a title="glai.AIMessages.edit_last_message" href="#glai.AIMessages.edit_last_message">edit_last_message</a></code></li>
<li><code><a title="glai.AIMessages.edit_message" href="#glai.AIMessages.edit_message">edit_message</a></code></li>
<li><code><a title="glai.AIMessages.edit_system_message" href="#glai.AIMessages.edit_system_message">edit_system_message</a></code></li>
<li><code><a title="glai.AIMessages.from_dict" href="#glai.AIMessages.from_dict">from_dict</a></code></li>
<li><code><a title="glai.AIMessages.from_json" href="#glai.AIMessages.from_json">from_json</a></code></li>
<li><code><a title="glai.AIMessages.get_last_message" href="#glai.AIMessages.get_last_message">get_last_message</a></code></li>
<li><code><a title="glai.AIMessages.has_system_tags" href="#glai.AIMessages.has_system_tags">has_system_tags</a></code></li>
<li><code><a title="glai.AIMessages.load_messages" href="#glai.AIMessages.load_messages">load_messages</a></code></li>
<li><code><a title="glai.AIMessages.reset_messages" href="#glai.AIMessages.reset_messages">reset_messages</a></code></li>
<li><code><a title="glai.AIMessages.save_json" href="#glai.AIMessages.save_json">save_json</a></code></li>
<li><code><a title="glai.AIMessages.set_system_message" href="#glai.AIMessages.set_system_message">set_system_message</a></code></li>
<li><code><a title="glai.AIMessages.system_tags" href="#glai.AIMessages.system_tags">system_tags</a></code></li>
<li><code><a title="glai.AIMessages.text" href="#glai.AIMessages.text">text</a></code></li>
<li><code><a title="glai.AIMessages.to_dict" href="#glai.AIMessages.to_dict">to_dict</a></code></li>
<li><code><a title="glai.AIMessages.user_tags" href="#glai.AIMessages.user_tags">user_tags</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="glai.AutoAI" href="#glai.AutoAI">AutoAI</a></code></h4>
<ul class="">
<li><code><a title="glai.AutoAI.count_tokens" href="#glai.AutoAI.count_tokens">count_tokens</a></code></li>
<li><code><a title="glai.AutoAI.generate" href="#glai.AutoAI.generate">generate</a></code></li>
<li><code><a title="glai.AutoAI.generate_from_literal_string" href="#glai.AutoAI.generate_from_literal_string">generate_from_literal_string</a></code></li>
<li><code><a title="glai.AutoAI.generate_from_messages" href="#glai.AutoAI.generate_from_messages">generate_from_messages</a></code></li>
<li><code><a title="glai.AutoAI.is_within_input_limit" href="#glai.AutoAI.is_within_input_limit">is_within_input_limit</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="glai.EasyAI" href="#glai.EasyAI">EasyAI</a></code></h4>
<ul class="">
<li><code><a title="glai.EasyAI.configure" href="#glai.EasyAI.configure">configure</a></code></li>
<li><code><a title="glai.EasyAI.count_tokens" href="#glai.EasyAI.count_tokens">count_tokens</a></code></li>
<li><code><a title="glai.EasyAI.find_model_data" href="#glai.EasyAI.find_model_data">find_model_data</a></code></li>
<li><code><a title="glai.EasyAI.generate" href="#glai.EasyAI.generate">generate</a></code></li>
<li><code><a title="glai.EasyAI.import_from_repo" href="#glai.EasyAI.import_from_repo">import_from_repo</a></code></li>
<li><code><a title="glai.EasyAI.import_verified_models_to_db" href="#glai.EasyAI.import_verified_models_to_db">import_verified_models_to_db</a></code></li>
<li><code><a title="glai.EasyAI.is_within_context" href="#glai.EasyAI.is_within_context">is_within_context</a></code></li>
<li><code><a title="glai.EasyAI.load_ai" href="#glai.EasyAI.load_ai">load_ai</a></code></li>
<li><code><a title="glai.EasyAI.load_model_db" href="#glai.EasyAI.load_model_db">load_model_db</a></code></li>
<li><code><a title="glai.EasyAI.model_data_from_file" href="#glai.EasyAI.model_data_from_file">model_data_from_file</a></code></li>
<li><code><a title="glai.EasyAI.model_data_from_url" href="#glai.EasyAI.model_data_from_url">model_data_from_url</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>